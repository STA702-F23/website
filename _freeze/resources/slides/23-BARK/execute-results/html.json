{
  "hash": "b9794a7a35542551db7ab747dbe9771f",
  "result": {
    "markdown": "---\ntitle: \"Lecture 23: Bayesian Adaptive Regression Kernels\"\nauthor: \"Merlise Clyde\"\nsubtitle: \"STA702\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \n  - parse-latex\ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n## Nonparametric Regression\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Lv}{\\textsf{Lévy}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\newcommand{\\Mult}{\\textsf{MultNom}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\textsf{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\Lmea}{{\\cal L}} \n\\newcommand{\\scale}{\\bflambda} \n\\newcommand{\\Scale}{\\bfLambda} \n\\newcommand{\\bfscale}{\\bflambda} \n\\newcommand{\\mean}{\\bfchi}\n\\newcommand{\\loc}{\\bfchi}\n\\newcommand{\\bfmean}{\\bfchi}\n\\newcommand{\\bfx}{\\x}\n\\renewcommand{\\k}{g}\n\\newcommand{\\Gen}{{\\cal G}}\n\\newcommand{\\Levy}{L{\\'e}vy}\n\\def\\bfChi    {\\boldsymbol{\\Chi}}\n\\def\\bfOmega  {\\boldsymbol{\\Omega}}\n\\newcommand{\\Po}{\\mbox{\\sf P}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- Consider model $Y_1, \\dots, Y_n \\sim \\N\\left(\\mu(\\x_i), \\sigma \\right)$\n\n- Mean function represented via a Stochastic Expansion\n$$\\mu(\\x_i) = \\sum_{j \\le J} b_j(\\x_i, \\bfomega_j) \\beta_j$$\n\n- Multivariate  Gaussian Kernel $g$ with parameters $\\bfomega = (\\mean, \\bfLambda)$\n$$\nb_j(\\x, \\bfomega_j) =  g(\n\\bfLambda_j^{1/2} (\\bfx - \\bfchi_j)) = \\exp\\left\\{-\\frac{1}{2}(\\bfx - \\mean_j)^T \\bfLambda_j (\\bfx -\n  \\mean_j)\\right\\}\n$$\n\n\n\n\n\n\n- introduce a Lévy measure  $\\nu( d\\beta, d \\bfomega)$ \n\n-  Poisson distribution $J \\sim \\Poi(\\nu_+)$ where \n$\\nu_+\\equiv \\nu(\\bbR\\times\\bfOmega) = \\iint \\nu(\\beta, \\bfomega) d \\beta \\,  d\\bfomega$ \n$$\\beta_j,\\bfomega_j \\mid J \\iid \\pi(\\beta, \\bfomega)\n  \\propto \\nu(\\beta,\\bfomega)$$\n  \n## Function Spaces\n\n- Conditions on $\\nu$ \n\n     - need to have that $|\\beta_j|$ are absolutely summable\n     - finite number of large coefficients (in absolute value)\n     - allows an infinite number of small $\\beta_j \\in [-\\epsilon, \\epsilon]$\n     \n - satisfied if $$\\iint_{\\bbR \\times \\bfOmega}( 1 \\wedge |\\beta|) \\nu(\\beta, \\bfomega) d\\beta \\, d\\bfomega < \\infty$$     \n  \n- Mean function $\\E[Y_i \\mid \\tb] = \\mu(\\x_i, \\tb)$ falls in some class of nonlinear functions based on $g$ and prior on $\\bfLambda$\n\n     + Besov Space\n     + Sobolov Space\n\n \n\n\n\n\n## Inference via Reversible Jump MCMC\n\n\n- number of support points $J$ varies from iteration to iteration\n   + add a new point (birth)\n   + delete an existing point (death)\n   + combine two points (merge)\n   + split a point into two\n- update existing point(s)  \n\n- can be much faster than shrinkage or BMA with a fixed but large $J$\n\n\n\n\n\n\n\n## So far \n\n- more parsimonious than \"shrinkage\" priors or SVM with fixed $J$\n- allows for increasing number of support points as $n$ increases (adapts to smoothness)\n- no problem with non-normal data, non-negative functions or even discontinuous functions\n- credible and prediction intervals; uncertainty quantification\n- robust alternative to Gaussian Process Priors\n\n-  But - hard to scale up random scales & locations as dimension of $\\x$ increases\n- Alternative Prior Approximation II\n\n\n## Higher Dimensional $\\X$\n\nMCMC is (currently) too slow in higher dimensional space to allow \n\n-  $\\bfchi$ to be completely arbitrary; restrict support to\n  observed $\\{\\bfx_i\\}$ like in SVM  (or observed quantiles)\n  \n-  use a common diagonal $\\bfLambda$ for all kernels\n\n- Kernels take form:\n\\begin{eqnarray*}\nb_j(\\bfx, \\bfomega_j) & = & \\prod_d \\exp\\{ -\\frac{1}{2} \\lambda_d (x_d - \\chi_{dj})^2\n\\} \\\\\n\\mu(\\bfx) & =  & \\sum_j b_j(\\bfx, \\bfomega_j) \\beta_j\n\\end{eqnarray*}\n\n\n- accomodates nonlinear interactions among variables\n\n- **ensemble model** like random forests, boosting, BART, SVM\n\n## Approximate Lévy Prior II\n\n- $\\alpha$-Stable process: $\\nu(d\\beta, d \\bfomega) = \\gamma c_\\alpha |\\beta|^{-(\\alpha + 1)} d\\beta \\ \\pi(d \\bfomega)$\n\n- Continuous Approximation to an $\\alpha$-Stable process via a Student $t(\\alpha, 0, \\epsilon)$:\n$$\\nu_\\epsilon(d\\beta, d \\bfomega) = \\gamma c_\\alpha (\\beta^2 + \\alpha\n\\epsilon^2)^{-(\\alpha + 1)/2} d\\beta \\ \\pi(d \\bfomega)$$\n\n\n- Based on the following hierarchical prior\n\\begin{aligned}\n  \\beta_j \\mid \\phi_j  & \\ind  \\N(0,  \\varphi_j^{-1}) &  &\n  \\phi_j      \\ind  \\Gam\\left(\\frac{\\alpha}{2}, \\frac{\\alpha \\epsilon^2}{2}\\right) \\\\\n   J & \\sim  \\Poi(\\nu^+_\\epsilon) &  &\n\\nu^+_\\epsilon   =  \\nu_\\epsilon(\\bbR, \\bfOmega) = \\gamma \\frac{\\alpha ^{1- \\alpha/2} \\Gamma(\\alpha/2)}{\\epsilon^\\alpha  2^{1 - \\alpha} \\Gamma(1 - \\alpha/2)}\\pi(\\bfOmega)\n\\end{aligned}\n\n. . .\n\n::: {.callout-tip}\n## Key Idea:  need to have variance/scale of coefficients decrease as $J$ increases\n:::\n\n## Limiting Case \n\\begin{eqnarray*}\n    \\beta_j   \\mid  \\varphi_j & \\ind &\\N(0, 1/\\varphi_j) \\\\\n      \\varphi_j & \\iid & \\Gam(\\alpha/2, 0)\n  \\end{eqnarray*}\n\n. . .\n\nNotes:\n\n- Require $0 < \\alpha < 2$  Additional restrictions  on $\\omega$   \n \n-  Tipping's **Relevance Vector Machine** corresponds to $\\alpha =\n  0$  (improper posterior!)  \n- Provides an extension of **Generalized Ridge Priors**\n      to infinite dimensional case  \n-  Cauchy process corresponds to $\\alpha = 1$       \n- Infinite dimensional analog of Cauchy priors\n\n## Simplification with $\\alpha = 1$\n\n- Poisson number of points $J \\sim \\Poi(\\gamma/\\eps)$ \n\n- Given $J$, $[ n_1 : n_n] \\sim  \\Mult(J, 1/(n+1))$ points\n    supported at each kernel located at $\\bfx_j$   \n \n \n- Aggregating, the regression mean function can be rewritten as\n$$\\mu(\\bfx) = \\sum_{i=0}^n \\tilde{\\beta}_i b_j(\\bfx, \\bfomega_i), \\quad\n  \\tilde{\\beta}_i = \\sum_{\\{j \\, \\mid \\bfchi_j = \\bfx_i\\}} \\beta_j$$\n\n\n. . .\n\n::: {.callout-tip} \n\nif $\\alpha = 1$, not only is the Cauchy process infinitely\ndivisible, the _approximated Cauchy prior distributions_ for $\\beta_j$ are also infinitely divisible!\n\n:::\n\n$$\n  \\tilde{\\beta}_i  \\ind\n  \\N(0, n_i^2 \\tilde{\\varphi}_i^{-1}), \\qquad\n  \\tilde{\\varphi}_i \\iid \\Gam( 1/2, \\eps^2/2)\n$$\n \nAt most $n$ non-zero coefficients!\n\n## Inference for Normal Model\n\n\n- integrate out $\\tilde{\\b}$ for marginal likelihood $\\cal{L}(J, \\{n_i\\}, \\{\\tilde{\\varphi}_i\\}, \\sigma^2, \\lambdab)$\n$$\\Y \\mid \\sigma^2, \\{n_i\\}, \\{\\tilde{\\varphi}_i\\}, \\lambdab \\sim \\N\\left(\\zero_n, \\sigma^2 \\I_n + \\mathbf{b} \\, \\diag\\left(\\frac{n_i^2}{\\tilde{\\varphi}_i} \\right)\\mathbf{b}^T\\right)$$\n\n- if $n_i = 0$  then the kernel located at $\\bfx_i$ drops out so we still need birth/death steps via RJ-MCMC for $\\{n_i, \\tilde{\\varphi}_i\\}$\n\n- for $J < n$  take advantage of  the Woodbury matrix identity for matrix inversion likelihood\n$$(A + U C V)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U){-1} V A^{-1}$$\n- update $\\sigma^2, \\lambdab$ via usual MCMC \n\n- for fixed $J$ and $\\{n_i\\}$, can update $\\{\\tilde{\\varphi}_i\\}, \\sigma^2, \\lambdab)$ via usual MCMC (fixed dimension)\n\n\n## Feature Selection in Kernel\n\n-  Product structure allows interactions between variables    \n-  Many input variables may be irrelevant   \n-  Feature selection; if $\\lambda_d = 0$ variable $\\x_d$ is removed\n  from all kernels   \n-  Allow point mass on $\\lambda_d = 0$ with probability $p_\\lambda\n  \\sim \\Be(a,b)$  (in practice have used $a = b = 1$  \n- can also constrain all $\\lambda_d$ that are non-zero to be equal   across dimensions\n\n\n. . .\n\nBinary Regression\n\n- add latent Gaussian variable as in Albert & Chib\n\n##  bark package\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bark)\nset.seed(42)\nn = 500\ncircle2 = data.frame(sim_circle(n, dim = 2))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(x.1 ~ x.2, data=circle2, col=y+1)\n```\n:::\n\n\n## Circle Data Classification\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](23-BARK_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n## BARK Classification\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\ntrain = sample(1:n, size = floor(n/2), rep=FALSE)\ncircle2.bark = bark(y ~ . , data = circle2,\n                    subset = train,\n                    testdata = circle2[-train,],\n                    classification = TRUE,\n                    printevery = 10000,\n                    selection = TRUE, \n                    common_lambdas = TRUE)\n```\n:::\n\n\n- `classification = TRUE` for probit regression\n\n- `selection = TRUE` allows some of the $\\lambda_j$ to be 0\n\n- `common_lambdas = TRUE` sets all (non-zero) $\\lambda_j$ to a common $\\lambda$\n\n## Missclassification \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmisscl = (circle2.bark$yhat.test.mean > 0) != circle2[-train, \"y\"]\nplot(x.1 ~ x.2, data=circle2[-train,], pch=circle2[-train, \"y\"]+1, col=misscl + 1)\ntitle(paste(\"Missclassification Rate\", round(mean(misscl), 4)))\n```\n\n::: {.cell-output-display}\n![](23-BARK_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n## Support Vector Machines (SVM) & BART\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(e1071)\ncircle2.svm = svm(y ~ x.1 + x.2, data=circle2[train,],\n                  type=\"C\")\npred.svm = predict(circle2.svm, circle2[-train,])\nmean(pred.svm != circle2[-train, \"y\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.048\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsuppressMessages(library(BART))\ncircle.bart = pbart(x.train = circle2[train, 1:2],\n                    y.train = circle2[train, \"y\"])\npred.bart = predict(circle.bart, circle2[-train, 1:2])\nmisscl.bart = mean((pred.bart$prob.test.mean > .5) != \n              circle2[-train, \"y\"])\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.036\n```\n:::\n:::\n\n\n## Comparisons\n\n| Data Sets   | n   | p   | BARK-D  | BARK-SE  | BARK-SD  | SVM   | BART |\n|------------ |----:|---:|--------:|---------:|---------:|-------:|-------:|\n|   Circle 2  | 200  |  2  | 4.91%  | 1.88%  | 1.93%  | 5.03%  | 3.97%  |\n|   Circle 5  | 200  |  5  | 4.70%  | 1.47%  | 1.65%  | 10.99% | 6.51% |\n|   Circle 20 | 200  | 20  | 4.84%  | 2.09%  | 3.69%  | 44.10% | 15.10% |\n|   Bank      | 200  | 6   | 1.25%  | 0.55%  | 0.88%  | 1.12%  | 0.50% |\n|   BC        | 569  | 30  | 4.02%  | 2.49%  | 6.09%  | 2.70%  | 3.36% |\n|  Ionosphere | 351  | 33  | 8.59%  | 5.78%  |10.87%  | 5.17%  | 7.34% |\n\n\n- BARK-D:  different $\\lambda_d$ for each dimension\n- BARK-SE: selection and equal $\\lambda_d$ for non-zero $\\lambda_d$\n- BARK-SD: selection and different $\\lambda_d$ for non-zero $\\lambda_d$\n\n## Needs & Limitations\n\n\n- NP Bayes of many flavors often does better than frequentist methods\n(BARK, BART, Treed GP, more) \n- Hyper-parameter specification - theory & computational\n  approximation \n- asymptotic theory (rates of convergence)  \n- need faster code for BARK that is easier for users (BART & TGP are\n  great!)  \n- Can these models be added to JAGS, STAN, etc instead of\n  stand-alone R packages \n- With availability of code what are caveats for users?\n\n \n## Summary\n\nLévy Random Field Priors & LARK/BARK models:\n \n- Provide limit of finite dimensional priors (GRP & SVSS) to infinite\n  dimensional setting \n- Adaptive bandwidth for kernel regression \n- Allow flexible generating functions \n- Provide sparser representations compared to SVM & RVM, with\n    coherent Bayesian interpretation \n- Incorporation of  prior knowledge if available \n- Relax assumptions of equally spaced data and Gaussian likelihood \n- Hierarchical Extensions \n- Formulation allows one to define stochastic processes on\n    arbitrary spaces (spheres, manifolds)\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}