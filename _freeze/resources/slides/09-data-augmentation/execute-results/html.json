{
  "hash": "0e28040edd0cc323b5ba76b979d005c9",
  "result": {
    "markdown": "---\ntitle: \"Lecture 9: Gibbs and Data Augmentation\"\nsubtitle: \"STA702\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Binary Regression {.smaller}\n\n$$Y_i \\mid \\beta \\sim \\textsf{Ber}(p(x_i^T \\beta))$$\nwhere $\\Pr(Y_i  = 1 \\mid \\beta) = p(x_i^T \\beta))$ and linear predictor $x_i^T\\beta = \\lambda_i$\n\n  \n\n- link function for binary regression is any 1-1 function  $g$ that will map $(0,1) \\to \\mathbb{R}$,  i.e.  $g(p(\\lambda)) = \\lambda$\n\n  \n\n- logistic regression uses the logit link  \n$$\\log\\left(\\frac{p(\\lambda_i)}{1 - p(\\lambda_i) }\\right) = x_i^T \\beta = \\lambda_i$$\n  \n\n- probit link\n$$p(x_i^T \\beta) = \\Phi(x_i^T \\beta)$$\n\n- $\\Phi()$ is the Normal cdf\n\n \n##  Likelihood and Posterior {.smaller}\n\nLikelihood:\n$$\\cal{L}(\\beta) \\propto \\prod_{i = 1}^n \\Phi(x_i^T \\beta)^{y_i} (1 - \\Phi(x^T_i \\beta))^{1 - y_i}$$\n  \n\n- prior  $\\beta \\sim \\textsf{N}_p(b_0, \\Phi_0)$\n\n  \n\n- posterior $\\pi(\\beta) \\propto \\pi(\\beta) \\cal{L}(\\beta)$\n\n  \n\n- How to  approximate the posterior?\n\n  \n  + asymptotic Normal approximation\n  \n  \n  + MH or adaptive Metropolis\n  \n  \n  + stan (Hamiltonian Monte Carlo)\n  \n  \n  + Gibbs ?   \n  \n  \n\n- seemingly no, but there is a trick!\n  \n  \n \n##  Data Augmentation {.smaller}\n\n-  Consider an **augmented** posterior\n$$\\pi(\\beta, Z \\mid y) \\propto \\pi(\\beta) \\pi(Z \\mid \\beta) \\pi(y \\mid Z, \\theta)$$\n  \n\n- IF we choose $\\pi(Z \\mid \\beta)$ and $\\pi(y \\mid Z, \\theta)$ carefully, we can carry out Gibbs and get samples of $\\pi(\\beta \\mid y)$ !\n\n  \n\n- Conditions: we need to have \n$$\\pi(\\beta \\mid y) = \\int_{\\cal{Z}} \\pi(\\beta, z \\mid y) \\, dz$$ (it is a marginal of joint augmented posterior)\n\n  \n\n-  We have to choose\n$$p(y \\mid \\beta) = \\int_{\\cal{Z}}  \\pi(z \\mid \\beta) \\pi(y \\mid \\beta, z) \\, dz$$\n  \n\n- complete data likelihood\n\n \n##  Augmentation Strategy {.smaller}\n\nSet \n\n- $y_i = 1_{(Z_i > 0)}$ i.e. ( $y_i = 1$ if $Z_i > 0$ ) \n- $y_i = 1_{(Z_i < 0)}$ i.e.  ( $y_i = 0$ if $Z_i < 0$ )\n\n  \n\n- $Z_i = x_i^T \\beta + \\epsilon_i \\qquad \\epsilon_i \\overset{iid}{\\sim} \\textsf{N(0,1)}$\n\n  \n\n- Relationship to probit model:\n$$\\begin{align*}\\Pr(y = 1 \\mid \\beta) & = P(Z_i > 0 \\mid \\beta) \\\\\n   & = P(Z_i - x_i^T \\beta > -x^T\\beta) \\\\\n   & = P(\\epsilon_i > -x^T\\beta) \\\\\n   & =  1 - \\Phi(-x^T_i \\beta) \\\\\n   & =  \\Phi(x^T_i \\beta)\n\\end{align*}$$\n\n\n\n\n \n##  Augmented Posterior & Gibbs {.smaller}\n\n$$\\begin{align*}\\pi(& Z_1,  \\ldots, Z_n,  \\, \\beta \\mid y) \\propto \\\\\n& \\textsf{N}(\\beta; b_0, \\phi_0)  \\left\\{\\prod_{i=1}^n \\textsf{N}(Z_i; x_i^T\\beta, 1)\\right\\} \\left\\{  \\prod_{i=1}^n y_i 1_{(Z_i > 0)} + (1 - y_i)1_{(Z_i < 0)}\\right\\}\n\\end{align*}$$\n\n  \n- full conditional for $\\beta$\n$$\\beta \\mid Z_1, \\ldots, Z_n, y_1, \\ldots, y_n \\sim \\textsf{N}(b_n, \\Phi_n)$$   \n- standard Normal-Normal regression updating given $Z_i$'s  \n\n  \n- Full conditional for latent $Z_i$\n$$\\begin{align*}\n\\pi(Z_i \\mid \\beta, Z_{[-i]}, y_1, \\ldots, y_n)  & \\propto \n\\textsf{N}(Z_i; x_i^T \\beta, 1)1_{(Z_i > 0)} \\text{   if  } y_1 = 1 \\\\\n\\pi(Z_i \\mid \\beta, Z_{[-i]}, y_1, \\ldots, y_n)  & \\propto \n\\textsf{N}(Z_i; x_i^T \\beta, 1)1_{(Z_i < 0) }\\text{   if  } y_1 = 0 \\\\\n\\end{align*}$$\n\n- sample from independent truncated normal distributions !\n\n  \n\n- two block Gibbs sampler $\\theta_{[1]} = \\beta$ and $\\theta_{[2]} = (Z_1, \\ldots, Z_n)^T$\n\n \n## Truncated Normal Sampling {.smaller}\n\n- Use inverse cdf method for  cdf $F$\n\n- If $u \\sim U(0,1)$ set $z = F^{-1}(u)$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-data-augmentation_files/figure-revealjs/inverseCDF-1.png){fig-align='center' width=60% height=55%}\n:::\n:::\n\n- if $Z \\in (a, b)$, Draw $u \\sim U(F(a),F(b))$\n  and set $z = F^{-1}(u)$\n\n \n## Data Augmentation in General {.smaller}\n\nDA is a broader than a computational trick allowing Gibbs sampling\n\n  \n\n- missing data\n\n  \n\n- random effects or latent variable modeling i.e we introduce latent variables to simplify dependence structure modelling\n\n\n  \n\n- Modeling heavy tailed distributions such as $t$ errors in regression \n\n\n\n\n\n\n\n\n\n\n\n \n## Comments {.smaller}\n\n\n\n- Why don't we treat each individual $\\theta_j$ as a separate block?\n\n  \n\n-  Gibbs always accepts, but can mix slowly if parameters in different blocks are highly correlated!\n\n  \n\n- Use block sizes in Gibbs that are as big as possible to improve  mixing (proven faster convergence)\n\n  \n\n- Collapse the sampler by integrating out as many parameters as possible  (as long as resulting sampler has good mixing)\n\n  \n\n- can use Gibbs steps and (adaptive) Metropolis Hastings steps together\n\n  \n\n\n- latent variables to allow Gibbs steps  but not always better!  \n\n\n\n\n\n",
    "supporting": [
      "09-data-augmentation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}