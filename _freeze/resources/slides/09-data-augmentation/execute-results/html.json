{
  "hash": "c48cb8b931cfcd2f1a989b67e024d599",
  "result": {
    "markdown": "---\ntitle: \"Lecture 9: Gibbs Sampling and Data Augmentation\"\nsubtitle: \"STA702\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n## Normal Linear Regression Example {.smaller}\n\n- Model\n$$\\begin{align*}\nY_i \\mid \\beta, \\phi & \\overset{  ind}{\\sim} \\textsf{N}(x_i^T\\beta, 1/\\phi) \\\\\nY \\mid \\beta, \\phi & \\sim \\textsf{N}(X \\beta, \\phi^{-1} I_n) \\\\\n\\beta & \\sim \\textsf{N}(b_0, \\Phi_0^{-1}) \\\\\n\\phi & \\sim \\textsf{Gamma}(v_0/2, s_0/2)\n\\end{align*}$$\n\n- $x_i$ is a $p \\times 1$ vector of predictors and $X$ is $n \\times p$ matrix\n\n  \n\n- $\\beta$ is a $p \\times 1$ vector of coefficients\n\n  \n\n- $\\Phi_0$ is a $p \\times p$ prior precision matrix \n\n  \n\n- Multivariate Normal density for $\\beta$\n$$\\pi(\\beta \\mid b_0, \\Phi_0) = \\frac{|\\Phi_0|^{1/2}}{(2 \\pi)^{p/2}}\\exp\\left\\{- \\frac{1}{2}(\\beta - b_0)^T \\Phi_0 (\\beta - b_0)  \\right\\}$$\n \n## Full Conditional for $\\beta$ {.smaller}\n\n$$\\begin{align*}\n\\beta & \\mid \\phi, y_1, \\ldots, y_n \\sim \\textsf{N}(b_n, \\Phi_n^{-1}) \\\\\nb_n & =  (\\Phi_0 + \\phi X^TX)^{-1}(\\Phi_0 b_0  +  \\phi X^TX \\hat{\\beta})\\\\\n\\Phi_n & = \\Phi_0 + \\phi X^TX\n\\end{align*}$$\n\n \n## Derivation continued {.smaller}\n\n\n \n## Full Conditional for $\\phi$ {.smaller}\n\n$$\\phi \\mid \\beta, y_1, \\ldots, y_n \\sim \\textsf{Gamma}\\left(\\frac{v_0 + n}{2}, \\frac{s_0 + \\sum_i(y_i - x^T_i \\beta)^2}{2}\\right)$$\n\n\n\n \n##  Choice of Prior Precision {.smaller}\n\n- Non-Informative $\\Phi_0 \\to 0$ \n\n  \n\n- Formal Posterior given $\\phi$\n  $$\\beta \\mid \\phi, y_1, \\ldots, y_n \\sim \\textsf{N}(\\hat{\\beta}, \\phi^{-1} (X^TX)^{-1})$$\n  \n  \n\n- needs $X^TX$ to be full rank for distribution to be unique!\n\n \n\n## Binary Regression {.smaller}\n\n$$Y_i \\mid \\beta \\sim \\textsf{Ber}(p(x_i^T \\beta))$$\nwhere $\\Pr(Y_i  = 1 \\mid \\beta) = p(x_i^T \\beta))$ and linear predictor $x_i^T\\beta = \\lambda_i$\n\n  \n\n- link function for binary regression is any 1-1 function  $g$ that will map $(0,1) \\to \\mathbb{R}$,  i.e.  $g(p(\\lambda)) = \\lambda$\n\n  \n\n- logistic regression uses the logit link  \n$$\\log\\left(\\frac{p(\\lambda_i)}{1 - p(\\lambda_i) }\\right) = x_i^T \\beta = \\lambda_i$$\n  \n\n- probit link\n$$p(x_i^T \\beta) = \\Phi(x_i^T \\beta)$$\n\n- $\\Phi()$ is the Normal cdf\n\n \n##  Likelihood and Posterior {.smaller}\n\nLikelihood:\n$$\\cal{L}(\\beta) \\propto \\prod_{i = 1}^n \\Phi(x_i^T \\beta)^{y_i} (1 - \\Phi(x^T_i \\beta))^{1 - y_i}$$\n  \n\n- prior  $\\beta \\sim \\textsf{N}_p(b_0, \\Phi_0)$\n\n  \n\n- posterior $\\pi(\\beta) \\propto \\pi(\\beta) \\cal{L}(\\beta)$\n\n  \n\n- How to  approximate the posterior?\n\n  \n  + asymptotic Normal approximation\n  \n  \n  + MH with Independence chain or adaptive Metropolis\n  \n  \n  + stan (Hamiltonian Monte Carlo)\n  \n  \n  + Gibbs ?   \n  \n  \n\n- seemingly no, but there is a trick!\n  \n  \n \n##  Data Augmentation {.smaller}\n\n-  Consider an **augmented** posterior\n$$\\pi(\\beta, Z \\mid y) \\propto \\pi(\\beta) \\pi(Z \\mid \\beta) \\pi(y \\mid Z, \\theta)$$\n  \n\n- IF we choose $\\pi(Z \\mid \\beta)$ and $\\pi(y \\mid Z, \\theta)$ carefully, we can carry out Gibbs and get samples of $\\pi(\\beta \\mid y)$ !\n\n  \n\n- desired marginal of joint augmented posterior\n$$\\pi(\\beta \\mid y) = \\int_{\\cal{Z}} \\pi(\\beta, z \\mid y) \\, dz$$ \n\n  \n\n-  We have to choose latent prior and sampling model such that\n$$p(y \\mid \\beta) = \\int_{\\cal{Z}}  \\pi(z \\mid \\beta) \\pi(y \\mid \\beta, z) \\, dz$$\n  \n\n- complete data likelihood $\\pi(z \\mid \\beta) \\pi(y \\mid \\beta, z)$\n\n \n##  Augmentation Strategy {.smaller}\n\nSet \n\n- $y_i = 1_{(Z_i > 0)}$ i.e. ( $y_i = 1$ if $Z_i > 0$ ) \n- $y_i = 1_{(Z_i < 0)}$ i.e.  ( $y_i = 0$ if $Z_i < 0$ )\n\n  \n\n- $Z_i = x_i^T \\beta + \\epsilon_i \\qquad \\epsilon_i \\overset{iid}{\\sim} \\textsf{N(0,1)}$\n\n  \n\n- Relationship to probit model:\n$$\\begin{align*}\\Pr(y = 1 \\mid \\beta) & = P(Z_i > 0 \\mid \\beta) \\\\\n   & = P(Z_i - x_i^T \\beta > -x^T\\beta) \\\\\n   & = P(\\epsilon_i > -x^T\\beta) \\\\\n   & =  1 - \\Phi(-x^T_i \\beta) \\\\\n   & =  \\Phi(x^T_i \\beta)\n\\end{align*}$$\n\n\n\n\n \n##  Augmented Posterior & Gibbs {.smaller}\n\n- two block Gibbs sampler $\\theta_{[1]} = \\beta$ and $\\theta_{[2]} = (Z_1, \\ldots, Z_n)^T$\n$$\\begin{align*}\\pi(& Z_1,  \\ldots, Z_n,  \\, \\beta \\mid y) \\propto \\\\\n& \\textsf{N}(\\beta; b_0, \\phi_0)  \\left\\{\\prod_{i=1}^n \\textsf{N}(Z_i; x_i^T\\beta, 1)\\right\\} \\left\\{  \\prod_{i=1}^n y_i 1_{(Z_i > 0)} + (1 - y_i)1_{(Z_i < 0)}\\right\\}\n\\end{align*}$$\n\n  \n- full conditional for $\\beta$  given $Z_i$'s based on Normal-Normal regression\n$$\\beta \\mid Z_1, \\ldots, Z_n, y_1, \\ldots, y_n \\sim \\textsf{N}(b_n, \\Phi_n)$$   \n\n\n  \n- Full conditional for latent $Z_i$ (product of independent dist's)\n$$\\begin{align*}\n\\pi(Z_i \\mid \\beta, Z_{[-i]}, y_1, \\ldots, y_n)  & \\propto \n\\textsf{N}(Z_i; x_i^T \\beta, 1)1_{(Z_i > 0)} \\text{   if  } y_1 = 1 \\\\\n\\pi(Z_i \\mid \\beta, Z_{[-i]}, y_1, \\ldots, y_n)  & \\propto \n\\textsf{N}(Z_i; x_i^T \\beta, 1)1_{(Z_i < 0) }\\text{   if  } y_1 = 0 \\\\\n\\end{align*}$$\n\n  \n\n \n## Truncated Sampling {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n- Use inverse cdf method for  cdf $F$\n\n- If $U\\sim U(0,1)$ set $X = F^{-1}(U)$\n\n- if $X \\in (a, b)$, Draw $X \\sim U(F(a),F(b))$\n  and set $X = F^{-1}(u)$\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-data-augmentation_files/figure-revealjs/inverseCDF-1.png){fig-align='center' width=1200}\n:::\n:::\n\n\n:::\n\n::::\n\n## Truncated Normal Sampling {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- sample from independent truncated normal distributions  for full conditional for $Z_i$\n\n- if $Y_i = 1$ then $Z_i \\sim \\textsf{Normal}(x_i^T\\beta, 1) I(0, \\infty)$ \n\n- standard truncated normal $\\tilde{Z} = Z_i - x_i^T \\beta \\in (-x_i^T \\beta, \\infty)$\n\n1) Generate $U \\sim \\textsf{Uniform}(\\Phi(-x_i^T\\beta), \\Phi(\\infty))$\n\n2) Set $\\tilde{z} =  \\Phi^{-1}(U)$ (Standard truncated normal)\n\n3) Shift $Z_i = x_i^T \\beta + \\tilde{z}$\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-data-augmentation_files/figure-revealjs/inverseNCDF-1.png){fig-align='center' width=1200}\n:::\n:::\n\n\n\n-  U =  0.69, $Z_i = x_i^T \\beta + \\Phi^{-1}(U)$ = 0.99 \n\n:::\n\n::::\n\n## Comments  on Gibbs {.smaller}\n\n\n\n- Why don't we treat each individual $\\theta_j$ as a separate block?\n\n  \n\n-  Gibbs always accepts, but can mix slowly if parameters in different blocks are highly correlated!\n\n  \n\n- Use block sizes in Gibbs that are as big as possible to improve  mixing (proven faster convergence)\n\n  \n\n- Collapse the sampler by integrating out as many parameters as possible  (as long as resulting sampler has good mixing)\n\n  \n\n- can use Gibbs steps and (adaptive) Metropolis Hastings steps together\n\n  \n\n\n- latent variables may allow Gibbs steps, but not always better compared to MH!  \n\n\n \n## Data Augmentation in General {.smaller}\n\nDA is a broader than a computational trick allowing Gibbs sampling\n\n  \n\n\n\n  \n\n- random effects or latent variable modeling i.e we introduce latent variables to simplify dependence structure modelling\n\n\n  \n\n- Modeling heavy tailed distributions for priors or errors in robust regression as mixtures of normals\n\n- outliesr\n\n- variable selection\n\n\n- missing data\n\n- Next class: \n\n    - Multivariate Normal data  \n    - Wishart and inverse-Wishart distributions\n    - missing data \n\n\n\n\n\n\n\n",
    "supporting": [
      "09-data-augmentation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}