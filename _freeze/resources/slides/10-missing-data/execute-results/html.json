{
  "hash": "15f9a9679468717e796d638d800a06d3",
  "result": {
    "markdown": "---\ntitle: \"Multivariate Normal Models, Missing Data and Imputation\"\nsubtitle: \"STA702\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Introduction to Missing Data {.smaller}\n\n- Missing data/nonresponse is fairly common in real data.\n  + Failure to respond to survey question\n  + Subject misses some clinic visits out of all possible\n  + Only subset of subjects asked certain questions\n  \n  \n\n- posterior computation usually depends on the data through $\\mathcal{p}(Y \\mid  X, \\theta)$, which can be difficult to compute (at least directly) when some of the $y_i$ (multivariate $Y$) or $x^T_i$ values are missing.\n\n  \n\n- Most software packages often throw away all subjects with incomplete data (can lead to bias and precision loss).\n\n  \n\n- Some individuals impute missing values with a mean or some other fixed value (ignores uncertainty).\n\n  \n\n- Imputing missing data is actually quite natural in the Bayesian context.\n\n\n\n\n \n## Missing data mechanisms {.smaller}\n\n- Data are said to be [missing completely at random (MCAR)]{style=\"color:red\"} if the reason for missingness does not depend on the values of the observed data or missing data.\n\n  \n\n- For example, suppose\n  - you handed out a double-sided survey questionnaire of 20 questions to a sample of participants;\n  - questions 1-15 were on the first page but questions 16-20 were at the back; and\n  - some of the participants did not respond to questions 16-20.\n \n  \n \n- Then, the values for questions 16-20 for those people who did not respond would be [MCAR]{style=\"color:red\"}  if they simply did not realize the pages were double-sided; they had no reason to ignore those questions.\n \n  \n \n- **This is rarely plausible in practice!**\n\n\n \n## Missing Data Mechanisms {.smaller}\n\n- Data are said to be    [missing at random (MAR)]{style=\"color:red\"} if, conditional on the values of the observed data, the reason for missingness does not depend on the missing data.\n\n  \n\n- Using our previous example, suppose\n  - questions 1-15 include demographic information such as age and education;\n  - questions 16-20 include income related questions; and\n  - once again, some participants did not respond to questions 16-20.\n\n  \n  \n- Then, the values for questions 16-20 for those people who did not respond would be    [MAR]{style=\"color:red\"}  if younger people are more likely not to respond to those income related questions than old people, where age is observed for all participants. (missingness reason must be independent of income)\n  \n  \n\n- **This is the most commonly assumed mechanism in practice!**\n\n\n \n## Missing data mechanisms {.smaller}\n\n- Data are said to be    [missing not at random (MNAR or NMAR)]{style=\"color:red\"} if the reason for missingness depends on the actual values of the missing (unobserved) data.\n\n  \n\n-  suppose again that\n  - questions 1-15 include demographic information such as age and education;\n  - questions 16-20 include income related questions; and\n  - once again, some of the participants did not respond to questions 16-20.\n\n  \n  \n- Then, the values for questions 16-20 for those people who did not respond would be    [MNAR]{style=\"color:red\"} if people who earn more money are less likely to respond to those income related questions than those with lower  incomes.\n\n  \n  \n- **This is usually the case in real data, but analysis can be complex!**\n\n\n \n## Multivariate Formulation {.smaller}\n\n- Consider the multivariate data scenario with $\\boldsymbol{Y}_i = (\\boldsymbol{Y}_1,\\ldots,\\boldsymbol{Y}_n)^T$, where $\\boldsymbol{Y}_i = (Y_{i1},\\ldots,Y_{ip})^T$, for $i = 1,\\ldots, n$.\n\n  \n\n- For now, we will assume the multivariate normal model as the sampling model, so that each $p$ dimensional $\\boldsymbol{Y}_i = (Y_{i1},\\ldots,Y_{ip})^T \\sim \\mathcal{N}_p(\\boldsymbol{\\theta}, \\Sigma)$.\n$$p(\\boldsymbol{Y}_i \\mid \\boldsymbol{\\theta}, \\Sigma) = \\frac{|\\Sigma|^{-1/2}}{(2\\pi)^{p/2}} \\exp\\left\\{ -\\frac{1}{2} (\\boldsymbol{Y} - \\boldsymbol{\\theta})^T \\Sigma^{-1} (\\boldsymbol{Y} - \\boldsymbol{\\theta}) \\right\\}$$\n\n  \n\t\n- Suppose now that $\\boldsymbol{Y}$ contains missing values.\n\n  \n\n- We can separate $\\boldsymbol{Y}$ into the observed and missing parts so that for for each individual, $$\\boldsymbol{Y}_i = (\\boldsymbol{Y}_{i,obs},\\boldsymbol{Y}_{i,mis})$$\n\n  \n\n\n\n \n## Mathematical Formulation {.smaller}\n\n- Let\n  + $j$ index variables (where $i$ already indexes individuals),\n  + $r_{ij} = 1$ when $y_{ij}$ is missing,\n  + $r_{ij} = 0$ when $y_{ij}$ is observed.\n\n  \n\n- Here, $r_{ij}$ is known as the missingness indicator of variable $j$ for person $i$. \n\n  \n\n- Also, let \n  + $\\boldsymbol{R}_i = (r_{i1},\\ldots,r_{ip})^T$ be the vector of missing indicators for person $i$.\n  + $\\boldsymbol{R} = (\\boldsymbol{R}_1,\\ldots,\\boldsymbol{R}_n)$ be the matrix of missing indicators for everyone.\n  + $\\boldsymbol{\\psi}$ be the set of parameters associated with $\\boldsymbol{R}$.\n\n  \n\n- Assume $\\boldsymbol{\\psi}$ and $(\\boldsymbol{\\theta}, \\Sigma)$ are distinct.\n\n\n \n## Mathematical Formulation {.smaller}\n\n- MCAR:\n$$p(\\boldsymbol{R} | \\boldsymbol{Y},\\boldsymbol{\\theta}, \\Sigma, \\boldsymbol{\\psi}) = p(\\boldsymbol{R} | \\boldsymbol{\\psi})$$\n\n\n  \n\n- MAR:\n$$p(\\boldsymbol{R} | \\boldsymbol{Y},\\boldsymbol{\\theta}, \\Sigma, \\boldsymbol{\\psi}) = p(\\boldsymbol{R} | \\boldsymbol{Y}_{obs},\\boldsymbol{\\psi})$$\n\n\n  \n\n- MNAR:\n$$p(\\boldsymbol{R} | \\boldsymbol{Y},\\boldsymbol{\\theta}, \\Sigma, \\boldsymbol{\\psi}) = p(\\boldsymbol{R} | \\boldsymbol{Y}_{obs},\\boldsymbol{Y}_{mis},\\boldsymbol{\\psi})$$\n\n\n\n\n \n## Implications for Likelihood Function {.smaller}\n\n- Each type of mechanism has a different implication on the likelihood of the observed data $\\boldsymbol{Y}_{obs}$, and the missing data indicator $\\boldsymbol{R}$.\n\n  \n\n- Without missingness in $\\boldsymbol{Y}$, the likelihood of the observed data is\n$$p(\\boldsymbol{Y}_{obs} | \\boldsymbol{\\theta}, \\Sigma)$$\n\n\n  \n\n- With missingness in $\\boldsymbol{Y}$, the likelihood of the observed data is instead\n$$\n\\begin{split}\np(\\boldsymbol{Y}_{obs}, \\boldsymbol{R} |\\boldsymbol{\\theta}, \\Sigma, \\boldsymbol{\\psi}) & = \\int p(\\boldsymbol{R} | \\boldsymbol{Y}_{obs},\\boldsymbol{Y}_{mis},\\boldsymbol{\\psi}) \\cdot p(\\boldsymbol{Y}_{obs},\\boldsymbol{Y}_{mis} | \\boldsymbol{\\theta}, \\Sigma) \\textrm{d}\\boldsymbol{Y}_{mis} \n\\end{split}\n$$\n\n\n \n  \n\n- Since we do not actually observe $\\boldsymbol{Y}_{mis}$, we would like to be able to integrate it out so we don't have to deal with it and infer $(\\boldsymbol{\\theta}, \\Sigma)$ using only the observed data.\n\n\n\n\n \n## Likelihood function: MAR {.smaller}\n\n- Focus on MAR\n$$\n\\begin{split}\np(\\boldsymbol{Y}_{obs}, \\boldsymbol{R} |\\boldsymbol{\\theta}, \\Sigma, \\boldsymbol{\\psi}) & = \\int p(\\boldsymbol{R} | \\boldsymbol{Y}_{obs},\\boldsymbol{Y}_{mis},\\boldsymbol{\\psi}) \\cdot p(\\boldsymbol{Y}_{obs},\\boldsymbol{Y}_{mis} | \\boldsymbol{\\theta}, \\Sigma) \\textrm{d}\\boldsymbol{Y}_{mis} \\\\\n& = \\int p(\\boldsymbol{R} | \\boldsymbol{Y}_{obs}, \\boldsymbol{\\psi}) \\cdot p(\\boldsymbol{Y}_{obs},\\boldsymbol{Y}_{mis} | \\boldsymbol{\\theta}, \\Sigma) \\textrm{d}\\boldsymbol{Y}_{mis} \\\\\n& = p(\\boldsymbol{R} | \\boldsymbol{Y}_{obs},\\boldsymbol{\\psi}) \\cdot \\int p(\\boldsymbol{Y}_{obs},\\boldsymbol{Y}_{mis} | \\boldsymbol{\\theta}, \\Sigma) \\textrm{d}\\boldsymbol{Y}_{mis} \\\\\n& = p(\\boldsymbol{R} | \\boldsymbol{Y}_{obs},\\boldsymbol{\\psi}) \\cdot p(\\boldsymbol{Y}_{obs} | \\boldsymbol{\\theta}, \\Sigma). \\\\\n\\end{split}\n$$\n\n\n  \n\n- For inference on $(\\boldsymbol{\\theta}, \\Sigma)$, we only need $p(\\boldsymbol{Y}_{obs} | \\boldsymbol{\\theta}, \\Sigma)$ in the likelihood function for inference $(\\boldsymbol{\\theta}, \\Sigma)$.\n\n  \n\n- Still is hard, as we need marginal model!\n\n \n## Bayesian Inference with Missing Data {.smaller}\n\n\n- For posterior sampling for most models (especially multivariate models), sampling is easier with complete data  $\\boldsymbol{Y}$'s to update the parameters.\n\n\n  \n\n- Think of the missing data as [latent variables]{style=\"color:red\"} and sample from the [posterior predictive distribution ]{style=\"color:red\"} of the missing data conditional on the observed data and parameters:\n$$\n\\begin{split}\np(\\boldsymbol{Y}_{mis} | \\boldsymbol{Y}_{obs},\\boldsymbol{\\theta}, \\Sigma) \\propto \\prod^n_{i=1} p(\\boldsymbol{Y}_{i,mis} | \\boldsymbol{Y}_{i,obs},\\boldsymbol{\\theta}, \\Sigma).\n\\end{split}\n$$\n  \n\n- In the case of the multivariate  normal model, each $p(\\boldsymbol{Y}_{i,mis} | \\boldsymbol{Y}_{i,obs},\\boldsymbol{\\theta}, \\Sigma)$ is just a normal distribution, and we can leverage results on conditional distributions for normal models.\n\n \n## Model for Missing Data {.smaller}\n\n- Rewrite  as $\\boldsymbol{Y}_i$ in block form\n\\begin{eqnarray*}\n\\boldsymbol{Y}_i =\n\\begin{pmatrix}\\boldsymbol{Y}_{i,mis}\\\\\n\\boldsymbol{Y}_{i,obs}\n\\end{pmatrix} & \\sim & \\mathcal{N}_p\\left[\\left(\\begin{array}{c}\n\\boldsymbol{\\theta}_1\\\\\n\\boldsymbol{\\theta}_2\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\Sigma_{11} & \\Sigma_{12} \\\\\n\\Sigma_{21} & \\Sigma_{22}\n\\end{array}\\right)\\right],\\\\\n\\end{eqnarray*}\n\n\n   \n  \n  \n\n- Missing data has a conditional \n$$\\boldsymbol{Y}_{i,mis} | \\boldsymbol{Y}_{i,obs} = \\boldsymbol{y}_{i,obs} \\sim \\mathcal{N}\\left(\\boldsymbol{\\theta}_1 + \\Sigma_{12}\\Sigma_{22}^{-1}  (\\boldsymbol{y}_{i,obs}-\\boldsymbol{\\theta}_2), \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\\right).$$\n\n\n- multivariate normal distribution (or univariate normal distribution if $\\boldsymbol{Y}_i$ only has one missing entry) \n  \n  \n\n- This sampling technique actually encodes MAR since the imputations for $\\boldsymbol{Y}_{mis}$ depend on the $\\boldsymbol{Y}_{obs}$.\n  \n\n \n\n## Semi-Conjugate Prior {.smaller}\n\n- We need prior distributions for  $\\boldsymbol{\\theta}$ and $\\Sigma$\n\n  \n\n- Multivariate Normal Prior for $\\boldsymbol{\\theta} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}_0, \\Lambda_0^{-1})$\n\n  \n\n- Analogous to the univariate case, the [inverse-Wishart distribution]{style=\"color:red\"} is the corresponding conditionally conjugate prior for $\\Sigma$ (multivariate generalization of the inverse-gamma).\n\n  \n\n\n- A random variable $\\Sigma \\sim \\textrm{IW}_p(\\eta_0, \\boldsymbol{S}_0^{-1})$, where $\\Sigma$ is positive definite and $p \\times p$, has pdf\n$$p(\\Sigma) \\propto  \\left|\\Sigma\\right|^{\\frac{-(\\eta_0 + p + 1)}{2}} \\textrm{exp} \\left\\{-\\frac{1}{2} \\textsf{tr}(\\boldsymbol{S}_0\\Sigma^{-1}) \\right\\}$$\n  \n    + $\\eta_0 > p - 1$ is the \"degrees of freedom\", and \n    + $\\boldsymbol{S}_0$ is a $p \\times p$ positive definite matrix.\n  \n \n## Mean {.smaller}\n\n- For this distribution, $E[\\Sigma] = \\frac{1}{\\eta_0 - p - 1} \\boldsymbol{S}_0$, for $\\eta_0 > p + 1$.\n\n\n  \n\n- If we are very confident in a prior guess $\\Sigma_0$, for $\\Sigma$, then we might set\n  + $\\eta_0$, the degrees of freedom to be very large, and\n  + $\\boldsymbol{S}_0 = (\\eta_0 - p - 1)\\Sigma_0$. \n  +  $E[\\Sigma] = \\frac{1}{\\eta_0 - p - 1} \\boldsymbol{S}_0 = \\frac{1}{\\eta_0 - p - 1}(\\eta_0 - p - 1)\\Sigma_0 = \\Sigma_0$, and $\\Sigma$ is tightly (depending on the value of $\\eta_0$) centered around $\\Sigma_0$.\n  \n  \n\n- If we are not at all confident but we still have a prior guess $\\Sigma_0$, we might set\n  + $\\eta_0 = p + 2$, so that the $E[\\Sigma] = \\frac{1}{\\eta_0 - p - 1} \\boldsymbol{S}_0$ is finite.\n  + $\\boldsymbol{S}_0 = \\Sigma_0$\n  \n## Alternatives\n  \n\n- Jeffreys prior (improper limiting case)\n\n- unit-information (data dependent)\n  \n-  [Sun, D. and Berger, J.O (2006)](https://www2.stat.duke.edu/~berger/papers/mult-normal.pdf) Objective Bayesian Analysis for the Multivariate Normal Model\n\n- [Mulder, J. Pericchi, L.R. (2018)](https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-4/The-Matrix-F-Prior-for-Estimating-and-Testing-Covariance-Matrices/10.1214/17-BA1092.full) The Matrix-F Prior for Estimating and Testing Covariance Matrices.\n \n## Wishart distribution {.smaller}\n\n- Just as we had with the gamma and inverse-gamma relationship in the univariate case, we can also work in terms of the [Wishart distribution]{style=\"color:red\"} (multivariate generalization of the gamma) instead.\n\n  \n\n- The **Wishart distribution** provides a conditionally-conjugate prior for the precision matrix $\\Sigma^{-1}$ in a multivariate normal model.\n\n  \n\n- if $\\Sigma \\sim \\textrm{IW}_p(\\eta_0, \\boldsymbol{S}_0)$, then $\\Phi = \\Sigma^{-1} \\sim \\textrm{W}_p(\\eta_0, \\boldsymbol{S}_0^{-1})$.\n\n  \n\n- A random variable $\\Phi \\sim \\textrm{W}_p(\\eta_0, \\boldsymbol{S}_0^{-1})$, where $\\Phi$ has dimension $(p \\times p)$, has pdf\n$$\\begin{align*}\nf(\\Phi) \\ \\propto \\ \\left|\\Phi\\right|^{\\frac{\\eta_0 - p - 1}{2}} \\textrm{exp} \\left\\{-\\frac{1}{2} \\text{tr}(\\boldsymbol{S}_0\\Phi) \\right\\}.\n\\end{align*}$$\n\n\n  \n\n- Here, $E[\\Phi] = \\eta_0 \\boldsymbol{S}_0$.\n\n\n  \n \n## Conditional posterior for $\\Sigma$ {.smaller}\n\n$$\\begin{align}Y_i  \\mid  \\boldsymbol{\\theta}, \\Sigma & \\overset{ind}{\\sim} N(\\boldsymbol{\\theta}, \\Sigma)\\\\\n\\Sigma  & \\sim  \\textrm{IW}_p(\\eta_0, \\boldsymbol{S}_0^{-1}) \\\\\n\\boldsymbol{\\theta} & \\sim N(\\mu_0, \\Psi_0^{-1}) \n\\end{align}$$\n\n- The conditional posterior (full conditional) $\\Sigma \\mid \\boldsymbol{\\theta}, \\boldsymbol{Y}$, is then\n$$\\Sigma \\mid \\boldsymbol{\\theta}, \\boldsymbol{Y} \\sim \\textrm{IW}_p\\left(\\eta_0 + n, \\left(\\boldsymbol{S}_0+ \\sum_{i=1}^n (\\boldsymbol{Y}_i - \\boldsymbol{\\theta})(\\boldsymbol{Y}_i - \\boldsymbol{\\theta})^T\\right)^{-1} \\right)$$ \n\n- posterior sample size $\\eta_0 + n$\n\n  \n\n- posterior sum of squares $\\boldsymbol{S}_0+ \\sum_{i=1}^n (\\boldsymbol{Y}_i - \\boldsymbol{\\theta})(\\boldsymbol{Y}_i - \\boldsymbol{\\theta})^T$\n\n\n\n \n## Posterior Derivation {.smaller}\n- The conditional posterior (full conditional) $\\Sigma \\mid \\boldsymbol{\\theta}, \\boldsymbol{Y}$, is \n$$\\begin{align*}\n\\pi(\\Sigma & \\mid \\boldsymbol{\\theta}, \\boldsymbol{Y})\\propto p(\\Sigma) \\cdot p( \\boldsymbol{Y}  \\mid \\boldsymbol{\\theta}, \\Sigma)\\\\\n& \\propto \\left|\\Sigma\\right|^{\\frac{-(\\eta_0 + p + 1)}{2}} \\textrm{exp} \\left\\{-\\frac{1}{2} \\text{tr}(\\boldsymbol{S}_0\\Sigma^{-1}) \\right\\} \\cdot \\prod_{i = 1}^{n}\\left|\\Sigma\\right|^{-\\frac{1}{2}} \\ \\textrm{exp} \\left\\{-\\frac{1}{2}\\left[(\\boldsymbol{Y}_i - \\boldsymbol{\\theta})^T \\Sigma^{-1} (\\boldsymbol{Y}_i - \\boldsymbol{\\theta})\\right] \\right\\} \\\\\n & \\\\\n & \\\\\n & \\\\\n & \n\\end{align*}\n$$\n\n. . .\n\n$$\\Sigma \\mid \\boldsymbol{\\theta}, \\boldsymbol{Y} \\sim \\textrm{IW}_p\\left(\\eta_0 + n, \\left(\\boldsymbol{S}_0+ \\sum_{i=1}^n (\\boldsymbol{Y}_i - \\boldsymbol{\\theta})(\\boldsymbol{Y}_i - \\boldsymbol{\\theta})^T\\right)^{-1} \\right)$$ \n\n  \n\n\n\n \n## Gibbs sampler with missing data {.smaller}\n\nAt iteration $s+1$, do the following\n\n1. Sample $\\boldsymbol{\\theta}^{(s+1)}$ from its multivariate normal full conditional\n$p(\\boldsymbol{\\theta}^{(s+1)} | \\boldsymbol{Y}_{obs}, \\boldsymbol{Y}_{mis}^{(s)}, \\Sigma^{(s)})$\n\n  \n  \n\n2. Sample $\\Sigma^{(s+1)}$ from its inverse-Wishart full conditional\n$p(\\Sigma^{(s+1)} | \\boldsymbol{Y}_{obs}, \\boldsymbol{Y}_{mis}^{(s)}, \\boldsymbol{\\theta}^{(s+1)})$\n\n\n  \n\n3. For each $i = 1, \\ldots, n$, with at least one \"1\" value in the missingness indicator vector $\\boldsymbol{R}_i$, sample $\\boldsymbol{Y}_{i,mis}^{(s+1)}$ from the full conditional\n$$\\begin{align}\n\\boldsymbol{Y}_{i,mis}^{(s+1)}| \\boldsymbol{Y}_{i,obs},  \\boldsymbol{\\theta}^{(s+1)},  \\Sigma^{(s+1)}  \\sim \\mathcal{N}(& \\boldsymbol{\\theta}_1^{(s+1)} + \\Sigma_{12}^{(s+1)}{\\Sigma_{22}^{(s+1)}}^{-1}  (\\boldsymbol{Y}_{i,obs}-\\boldsymbol{\\theta}_2^{(s+1)}),  \\\\\n &  \\Sigma_{11}^{(s+1)} - \\Sigma_{12}^{(s+1)}{\\Sigma_{22}^{(s+1)}}^{-1}\\Sigma_{21}^{(s+1)})\n\\end{align}$$\n\n . . .\n \n- derived from the original sampling model but with the updated parameters,  $\\boldsymbol{Y}_i^{(s+1)} = (\\boldsymbol{Y}_{i,obs},\\boldsymbol{Y}_{i,mis}^{(s+1)})^T \\sim \\mathcal{N}_p(\\boldsymbol{\\theta}^{(s+1)}, \\Sigma^{(s+1)})$.\n\n\n\n \n \n## Reading example from Hoff with missing data {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n      pretest posttest\n [1,]      59       77\n [2,]      43       39\n [3,]      34       46\n [4,]      32       NA\n [5,]      NA       38\n [6,]      38       NA\n [7,]      55       NA\n [8,]      67       86\n [9,]      64       77\n[10,]      45       60\n[11,]      49       50\n[12,]      72       59\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n  pretest  posttest \n0.1363636 0.2272727 \n```\n:::\n:::\n\n\n## MCMC Summary for $\\Sigma$ {.smaller}\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\" hash='10-missing-data_cache/revealjs/unnamed-chunk-5_8e7456be4d9e1492ad979fd55412c157'}\n\n:::\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\nIterations = 1:20000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 20000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean    SD Naive SE Time-series SE\nsigma_11 194.0 63.08   0.4460         0.4947\nsigma_12 152.1 60.75   0.4295         0.4665\nsigma_21 152.1 60.75   0.4295         0.4665\nsigma_22 248.7 83.70   0.5918         0.6884\n\n2. Quantiles for each variable:\n\n           2.5%   25%   50%   75% 97.5%\nsigma_11 106.45 149.8 182.4 224.1 349.8\nsigma_12  64.04 109.8 142.3 182.8 299.2\nsigma_21  64.04 109.8 142.3 182.8 299.2\nsigma_22 132.50 190.3 233.4 289.5 456.1\n```\n:::\n:::\n\n\n\n \n## Compare to inference from full data {.smaller}\n\n- With missing data:\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n         theta_1  theta_2\nMin.    30.45459 38.29322\n1st Qu. 43.65988 51.96991\nMedian  45.60829 54.19592\nMean    45.63192 54.20408\n3rd Qu. 47.61896 56.48918\nMax.    58.81206 70.49105\n```\n:::\n:::\n\n\n- Based on true data:\n\n. . .\n\n\n::: {.cell layout-align=\"center\" hash='10-missing-data_cache/revealjs/unnamed-chunk-10_acf6d453e9bde5ad83d6db7ea5fdd3c9'}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n         theta_1  theta_2\nMin.    34.88365 37.80999\n1st Qu. 45.29473 51.47834\nMedian  47.28229 53.65172\nMean    47.26301 53.64100\n3rd Qu. 49.21423 55.81819\nMax.    60.94924 69.92354\n```\n:::\n:::\n\n\n- Very similar for the most part.\n\n \n## Compare to inference from full data {.smaller}\n\n- With missing data:\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n        sigma_11  sigma_12  sigma_21  sigma_22\nMin.     64.0883 -20.39204 -20.39204  82.55346\n1st Qu. 149.8338 109.84218 109.84218 190.25962\nMedian  182.4496 142.34686 142.34686 233.43312\nMean    193.9803 152.12898 152.12898 248.67527\n3rd Qu. 224.0994 182.75082 182.75082 289.47663\nMax.    734.8704 668.77332 668.77332 981.99916\n```\n:::\n:::\n\n\n\n\n- Based on true data:\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n        sigma_11  sigma_12  sigma_21  sigma_22\nMin.     76.4661 -38.75561 -38.75561  93.65776\n1st Qu. 157.5870 113.32529 113.32529 203.69192\nMedian  190.6578 145.08962 145.08962 246.08696\nMean    201.9547 155.20374 155.20374 260.11361\n3rd Qu. 233.5809 186.36991 186.36991 300.70840\nMax.    664.8241 577.99100 577.99100 947.39333\n```\n:::\n:::\n\n\n- Also very similar. A bit more uncertainty in dimension of $Y_{i2}$ because we have more missing data there.\n\n\n\n \n## Posterior distribution of the mean {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-missing-data_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\n \n## Missing data vs predictions for new observations {.smaller}\n\n- How about predictions for completely new observations?\n\n  \n\n- That is, suppose your original dataset plus sampling model is $\\boldsymbol{y_i} = (y_{i,1},y_{i,2})^T \\sim \\mathcal{N}_2(\\boldsymbol{\\theta}, \\Sigma)$, $i = 1, \\ldots, n$.\n\n  \n\n- Suppose now you have $n^\\star$ new observations with $y_{2}^\\star$ values but no $y_{1}^\\star$.\n\n  \n\n- How can we predict $y_{i,1}^\\star$ given $y_{i,2}^\\star$, for $i = 1, \\ldots, n^\\star$?\n\n  \n\n- Well, we can view this as a \"train $\\rightarrow$ test\" prediction problem rather than a missing data problem on an original data.\n\n\n \n## Missing data vs predictions for new observations {.smaller}\n\n- That is, given the posterior samples of the parameters, and the test values for $y_{i2}^\\star$, draw from the posterior predictive distribution of $(y_{i,1}^\\star | y_{i,2}^\\star, \\{(y_{1,1},y_{1,2}), \\ldots, (y_{n,1},y_{n,2})\\})$. \n\n  \n\n- To sample from this predictive distribution, think of compositional sampling.\n\n  \n\n- for each posterior sample of $(\\boldsymbol{\\theta}, \\Sigma)$, sample from $(y_{i,1} | y_{i,2}, \\boldsymbol{\\theta}, \\Sigma)$, which is just from the form of the sampling distribution.\n\n  \n\n- In this case, $(y_{i,1} | y_{i,2}, \\boldsymbol{\\theta}, \\Sigma)$ is just a normal distribution derived from $(y_{i,1}, y_{i,2} | \\boldsymbol{\\theta}, \\Sigma)$, based on the conditional normal formula.\n\n  \n\n- No need to incorporate the prediction problem into your original Gibbs sampler!\n\n\n\n\n \n## MNAR Likelihood function:  {.smaller}\n\n- For MNAR, we have:\n$$\n\\begin{split}\np(\\boldsymbol{Y}_{obs}, \\boldsymbol{R} |\\boldsymbol{\\theta}, \\Sigma, \\boldsymbol{\\psi}) & = \\int p(\\boldsymbol{R} | \\boldsymbol{Y}_{obs},\\boldsymbol{Y}_{mis},\\boldsymbol{\\psi}) \\cdot p(\\boldsymbol{Y}_{obs},\\boldsymbol{Y}_{mis} | \\boldsymbol{\\theta}, \\Sigma) \\textrm{d}\\boldsymbol{Y}_{mis} \\\\\n\\end{split}\n$$\n\n\n  \n\n- The likelihood under MNAR cannot simplify any further.\n  \n  \n\n- In this case, we cannot ignore the missing data when making inferences about $(\\boldsymbol{\\theta}, \\Sigma)$.\n  \n  \n\n- We must include the model for $\\boldsymbol{R}$ and also infer the missing data $\\boldsymbol{Y}_{mis}$.\n \n  \n\n- So how can we tell the type of mechanism we are dealing with? \n\n  \n\n- In general, we don't know!!!\n\n  \n\n- Rare that data are MCAR (unless planned beforehand); more likely that data are MNAR or MNAR.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}