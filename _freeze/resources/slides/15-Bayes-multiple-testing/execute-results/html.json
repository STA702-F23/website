{
  "hash": "a7d4a10be0891dfa007349b75d248988",
  "result": {
    "markdown": "---\ntitle: \"Lecture 15: Bayesian Multiple Testing\"\nauthor: \"Merlise Clyde\"\nsubtitle: \"STA702\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Normal Means Model\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\text{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\nSuppose we have normal data with $Y_i \\mid \\mu_i, \\sigma^2 \\overset{iid}{\\sim} \\N(\\mu_i, \\sigma^2)$\n\n\n- **Multiple Testing** $H_{0i}: \\mu_i = 0$ versus $H_{1i}: \\mu_i \\neq 0$ \n\n- $n$ hypotheses that may potentially be closely related,  e.g. $H_{01}$ no difference in expression of gene $i$ between cases and controls, for $n$ genes\n\n\n - Means Model  based on a \"Spike & Slab\" Prior:\n$$\\mu_i  \\mid  \\tau \\overset{iid}{\\sim} \\pi_0 \\delta_0 + (1 - \\pi_0)g(\\mu_i \\mid 0,  \\tau)$$ \n\n  \n\n-  need to specify \n  - $\\pi_0$ Probability of $H_{0i}$ or that $\\mu_{i} = 0$ (spike)\n  - $g$ \"slab distribution\"\n  \n  \n- concern: is that # errors blows up with $n$ \n($n$ = # tests = dimension of $\\{\\mu_i\\}$ )\n\n  \n\n\n \n## Approach 1: Prespecify $\\pi_0$\n\n\n  \n\n- seemingly non-informative choice?  \n$\\pi_0 = 0.5$\n\n  \n\n\n- Let \n$$\\gamma_i = \\left\\{ \\begin{array}{c} 1 \\text{ if }  H_{1i} \\text{ is true } \\\\\n0 \\text{ if }  H_{0i} \\text{ is true } \\end{array} \\right.$$\n\n\n . . .\n \n\n$$\\gamma^{(n)} = (\\gamma_1, \\gamma_2, \\ldots, \\gamma_n)^2  \\qquad \\text{ e.g.   } \\gamma^{(n)} = (0,1,0,0, \\ldots, 1)^T$$\n\n  \n\n- model size $p_\\gamma  = \\sum_{i = 1}^n \\gamma_i$ is the number of non-zero values.  What does $\\pi_0 = 0.5$ imply about the number of times $H_{1i}$ is true _a priori_?\n\n  \n\n \n##  Induced Distribution\n\nif $p_\\gamma  = \\sum_{i = 1}^n \\gamma_i$ with $p(\\gamma_i = 1) = 0.5$ then\n$p_\\gamma \\sim \\textsf{Binomial(n, 1/2)}$\n  \n\n- Expect 1/2 of the hypotheses to be true _a priori_\n\n. . .\n  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-Bayes-multiple-testing_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\n \n## Probabilities of no features or at least 1 feature?\n\n$$p_\\gamma \\sim \\textsf{Binomial(n, 1/2)}$$\n\n- probability of no features $\\gamma^{(n)} = (0,0,0,\\ldots, 0)^T$ or $p_\\gamma = 0$\n$$\\Pr(p_\\gamma = 0) = \\pi_0^n = 0.5^n$$\n  \n\n- approximately $0$ for large $n$\n\n  \n\n-  Similarily, the probability of at least one feature is  $1 - 0.5^n \\approx 1$\n\n \n## Control  Type I Errors\n\n- Suppose we want to fix $\\pi_0$ to protect against Type I errors blowing up as $n$ increases\n$$\\Pr( p_\\gamma = \\mathbf{0}_n) =  \\frac{1}{2} = \\pi_0^n$$\n  \n\n- \"Bayesian Bonferroni Prior\" \n\n  \n\n- leads to $\\pi_0 = 0.5^{1/n}$  very close to 1  for large $n$!   We would need overwhelming evidence in the data for $\\Pr(H_{1i} \\mid y^{(n)})$ to not be $\\approx 0$!\n\n  \n\n- not a great idea to prespecify $\\pi_0$!\n\n \n## Approach 2: Empirical Bayes\n\n- Estimate $\\pi_0$ from the data by maximizing the marginal likelihood  \n$$\\begin{align}\nY_i \\mid \\mu_i, \\sigma^2 \\mid \\overset{ind}{\\sim} \\textsf{N}(\\mu_i, \\sigma^2) \\\\\n\\mu_i \\mid \\tau, \\pi_0 & \\overset{iid}{\\sim} \\pi_0 \\delta_0 + (1 - \\pi_0)\\textsf{N}(\\mu_i; 0,  \\tau)\n\\end{align}$$ \n \n\n  \n- marginal likelihood\n$$\\begin{align*} \\cal{L}(\\pi_0, \\tau) & =  \\int_{\\mathbb{R}^n} \\prod_{i = 1}^n  \\textsf{N}(y_i ; \\mu_i, \\sigma^2) \\left\\{\\pi_0 \\delta_0(\\mu_i) + (1 - \\pi_0)\\textsf{N}(\\mu_i; 0,  \\tau) \\right\\} d\\mu_1 \\ldots d\\mu_n\n\\\\\n& =  \\prod_{i = 1}^n  \\int_\\mathbb{R}  \\textsf{N}(y_i ; \\mu_i, \\sigma^2) \\left\\{\\pi_0 \\delta_0(\\mu_i) + (1 - \\pi_0)\\textsf{N}(\\mu_i; 0,  \\tau) \\right\\} d\\mu_i\n\\end{align*}$$\n\n  \n\n- Conjugate or nice setups we can integrate out $\\mu_i$ and then maximize marginal likelihood for $\\pi_0$ and $\\tau$\n\n  \n\n- Numerical integration (lab) or EM algorithms to get $\\hat{\\pi}_0^{\\textsf{EB}}$ and $\\hat{\\tau}^{\\textsf{EB}}$\n\n\n \n## Expectation-Maximization   ($\\sigma = 1$)\n\n\nIntroduce latent variables so that \"complete\" data likelihood is nice!  (no integrals!) \n$$\\begin{align} y_i \\mid \\gamma_i, \\tau & \\overset{iid}{\\sim} \\textsf{N}(0, 1)^{1 - \\gamma_i} \\textsf{N}(0, 1 + \\tau)^{\\gamma_i} \\\\\n\\gamma_i \\mid \\pi_0 & \\overset{iid}{\\sim} \\textsf{Ber}(1 - \\pi_0)\n\\end{align}$$\n\n  \n\n- Iterate: For $t = 1, \\ldots$\n\n  \n -  M-step:  Solve for $(\\hat{\\pi}_0^{(t)}, \\hat{\\tau}^{(t)}) = \\arg \\max\\cal{L}(\\pi_0, \\tau \\mid \\hat{\\gamma}^{(t-1)})$\n  \n  \n -  E-step:  find the expected values of the latent sufficient statistics given the data, $\\hat{\\pi}_0^{(t)}$ , $\\hat{\\tau}^{(t)}$  (i.e. posterior expectation)\n$$\\hat{\\gamma}^{(t)} = \\E [\\gamma_i \\mid y, \\hat{\\pi}^{(t)}_0, \\hat{\\tau}^{(t)}]$$\n  \n \n - Clyde & George (2000)  Silverman & Johnstone (2004) for orthogonal regression\n\n\n## M-Step\n\n- log-likelihood\n$$\\begin{align} \\cal{L}(\\pi_0, \\tau) = &  \\sum_i (1 -\\gamma_i) \\log(\\pi_0) + \\gamma_i \\log(1 - \\pi_0) + \\\\\n& \\sum_i(1 - \\gamma_i) N(y_i; 0, 1) + \\gamma_i N(y_i; 0, 1 + \\tau)\n\\end{align}$$\n\n- plug in $\\hat{\\gamma}_i^{(t)}$ above and maximize wrt $\\pi_0$ and $\\tau$\n\n- $\\hat{\\pi}_0^{(t)} = 1 - \\frac{\\sum_i\\hat{\\gamma}_i^{(t)}}{n}$\n\n- $\\hat{\\tau}^{(t)} = \\max\\{0, \\frac{\\sum_i \\hat{\\gamma}_i^{(t)} y_i^2}{\\sum_i \\hat{\\gamma}_i^{(t)}} - 1\\}$\n\n\n## E-Step\n\nPosterior distribution for $\\gamma_i \\mid y_i, \\hat{\\tau}, \\hat{\\pi}$\n$$\\gamma_i \\mid y_i, \\hat{\\tau}, \\hat{\\pi}_0 \\iid  \\Ber(\\omega_{i})$$\n\n \n - $\\omega_i = \\frac{\\cal{O}_i}{1 + \\cal{O}_i}$\n with posterior odds $\\cal{O}_i$\n $$\\begin{align}\n \\cal{O}_i & = \\frac{1 - \\hat{\\pi}^{(t)}_0}{\\hat{\\pi}^{(t)}_0} \\times \\BF_{10} \\\\\n \\BF_{10} & = \\frac{p(y \\mid \\gamma_i = 1, \\hat{\\tau}^{(t)})}{p(y \\mid \\gamma_i = 0)} = \n \\frac{1}{(1 + \\hat{\\tau}^{(t)})^{1/2}} e^{ \\frac{1}{2} y_i^2 \\frac{\\hat{\\tau}^{(t)}} {1 + \\hat{\\tau}^{(t)}}}\n \\end{align}$$\n \n \n \n## Adding Noise\n\n\n\n\n\n\n\nWhat happens to $\\hat{\\pi}_0^{\\textsf{EB}}$ if have all noise?\n\n- $\\hat{\\tau} \\to 0$  as $n \\to \\infty$ (here $n = 10000$) ($\\hat{\\tau}^{(t)} = \\max\\{0, \\frac{\\sum_i \\hat{\\gamma}_i^{(t)} y_i^2}{\\sum_i \\hat{\\gamma}_i^{(t)}} - 1\\}$ ) so distribution collapses to the same as the noise model\n\n\n- $\\BF_{10} \\to 1$ so $\\hat{\\pi}_0^{(t)} = \\hat{\\pi}_0^{(0)}$\n\n- $\\hat{\\pi}_0^{\\textsf{EB}}$ gets stuck at initial value of $\\pi_0$!\n\n- posterior probability of $H_{1i}$ not consistent as well as $\\pi_0$\n\n- similar problems with convergence to a local mode with even with more features \n\n\n\n\n\n\n\n\n\n\n  \n\n\n \n## Approach 3:  Fully Bayes\n\nChoose a prior for $\\pi_0$ (and $\\tau$), simplest case\n$\\pi_0 \\sim \\textsf{Beta}(a,b)$\n\n  \n\n- Consider the thought experiment where we don't know the first hypothesis but we know that the others  are all null  $\\gamma_j = 0$ for $j = 2, \\ldots, n$\n$$\\gamma^{(n)} = (?, 0,,\\ldots, 0)^T$$\n\n\n\n  \n\n- $\\gamma_i \\sim \\textsf{Bernoulli}(1 - \\pi_0)$\n\n  \n\n- Update the prior for $\\pi_0$ to include the info  $\\gamma_j = 0$ for $j = 2, \\ldots, n$\n$$\\begin{align}\n\\pi(\\pi_0 \\mid \\gamma_2, \\ldots, \\gamma_n) & \\propto \\pi_0^{a - 1} (1 - \\pi_0)^{b -1} \\prod_{j = 2}^n \\pi_0^{1 - \\gamma_j} (1 - \\pi_0)^{\\gamma_j}\\\\\n\\pi(\\pi_0  \\mid \\gamma_2, \\ldots, \\gamma_n) & \\propto \\pi_0^{a + n -1  -1} (1 - \\pi_0)^{b - 1}\n\\end{align}$$\n \n## Beta Posterior\n\nPosterior $\\pi_0 \\mid \\gamma_2, \\ldots, \\gamma_n \\sim \\textsf{Beta(a + n - 1, b)}$\nwith mean \n$$\\textsf{E}[\\pi_0 \\mid \\gamma_2, \\ldots, \\gamma_n] = \\frac{a + n - 1}{a + n - 1 + b}$$\n  \n\n- suppose $a = b = 1$   (Uniform prior)\n$$\\textsf{E}[\\pi_0 \\mid \\gamma_2, \\ldots, \\gamma_n] = \\frac{n}{n + 1}$$\n \n  \n \n- implies probability of $H_{01} \\to 1$ and  $H_{11} \\to 0$ as $n \\to \\infty$  borrowing strength from other nulls\n\n  \n\n- Multiplicity adjustment as in the EB case\n\n  \n\n- Scott & Berger (2006 JSPI, 2010 AoS) show that above framework protects against increasing Type I errors with $n$; We also get FDR control automatically\n\n \n## Induced Prior on $p_{\\gamma}$\n\n::: {.callout-tip}\n## Exercise for the Energetic Student:\n\nIf $p_{\\gamma} \\mid \\pi_0 \\sim \\textsf{Binomial}(n, 1 - \\pi_0)$ and $\\pi_0 \\sim \\textsf{Beta}(1,1)$\n\n- What is the probability that $p_\\gamma = 0$\n\n- What is the probability  that $p_\\gamma = n$\n\n- What is the distribution of $p_\\gamma$ ?    \n::: \n \n \n-  This is a Beta-Binomial  distribution!\n-  special case  $a = b = 1$ this is a discrete uniform on model size!\n \n \n \n  \n \n. . .\n\n**Bottomline:**  We need to  \"learn\" key parameters  in our hierarchical prior or the magic doesn't work!   Borrowing comes through using all the data to inform about \"global\" parameters in the prior, in this case $\\pi_0$ (and $\\tau$)!\n\n \n \n##  Posteriors, Inference and Decisions\n\n\n\n  \n\n - Posterior distribution of $\\mu_i$ is a spike at 0 and continous distribution\n \n- Joint posterior distribution of $\\mu_1, \\ldots, \\mu_n$ averaged over hypotheses  \"Model averaging\"\n  \n\n- select a hypothesis\n\n  \n\n- Report posterior (summaries)  conditional on a hypothesis  \n\n  \n\n- Issue is the **winner's curse** !  \n\n- Need to have coherent conditional inference given that you selected a hypothesis.   \n \n\n- Don't report selected hypotheses but report results under model averaging! \n\n \n## Choice of Slab \n\n\n$$\\mu_i \\overset{iid}{\\sim} \\pi_0 \\delta_0 + (1 - \\pi_0)g(\\mu_i \\mid 0,  \\tau, H_{i1})$$ \n  \n\n- growing literature on posterior contraction in high dimensional settings as $n \\to \\infty$ with \"sparse signals\"\n\n  \n\n- posterior $\\pi(\\mu^{(n)}) \\mid y^{(n)})$\n\n  \n\n- Want \n$$\\Pr(\\mu^{(n)} \\in \\cal{N}_{\\epsilon_n}(\\mu_0^{(n)}) \\mid y^{(n)})\\to 1$$\n\n  \n\n- assume that there are $s$ features (fixed or growing slowly)\n\n  \n\n- feature values are bounded away from zero\n\n  \n\n- Want the posterior under the Spike and Slab prior to concentrate on this neighborhood (ie. probability 1 )\n\n  \n\n- active area of research!  \n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}