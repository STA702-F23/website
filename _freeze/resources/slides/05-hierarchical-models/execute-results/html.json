{
  "hash": "e995b23aa0ec13cc8d17fbda8927e6d5",
  "result": {
    "markdown": "---\ntitle: \"Lecture 5: Introduction to Hierarchical Modelling, Empirical Bayes, and MCMC\"\nauthor: \"Merlise Clyde\"\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Normal Means Model\nSuppose we have normal data with \n$$Y_i \\overset{iid}{\\sim} \\textsf(\\mu_i, \\sigma^2)$$\n \n\n- separate mean for each observation!\n\n \n\n**Question**: How can we possibly hope to estimate all these $\\mu_i$?  One $y_i$ per $\\mu_i$ and $n$ observations!\n\n \n\n**Naive estimator**:  just consider only using $y_i$ in estimating and not the other observations.\n\n \n\n- MLE $\\hat{\\mu}_i = y_i$\n\n \n\n**Hierarchical Viewpoint**:  Let's borrow information from other observations!\n\n -\n## Motivation\n\n- Example $y_i$ is difference in gene expression for the $i^{\\text{th}}$ gene between cancer and control lines\n\n \n\n- may be natural to think that the $\\mu_i$ arise from some common distribution,\n$\\mu_i \\overset{iid}{\\sim} g$\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](05-hierarchical-models_files/figure-html/randomeffects-1.png){fig-align='center' width=50%}\n:::\n:::\n\n \n\n- unbiased but high variance estimators of $\\mu_i$ based on one observation!\n\n \n\n\n---\n##  Low Variability\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](05-hierarchical-models_files/figure-html/means-1.png){fig-align='center' width=50%}\n:::\n:::\n \n\n- little variation in $\\mu_i$s so a better estimate might be $\\bar{y}$\n\n \n\n- Not forced to choose either - what about some weighted average between $y_i$ and $\\bar{y}$?\n\n \n## Simple Example\n\nData Model\n\n$$Y_i \\mid \\mu_i, \\sigma^2 \\overset{iid}{\\sim} \\textsf(\\mu_i, \\sigma^2)$$\n\n \n\nMeans Model \n$$\\mu_i \\mid \\mu, \\tau \\overset{iid}{\\sim} \\textsf(\\mu, \\sigma^2_{\\mu})$$ \n \n\n- not necessarily a prior!\n\n \n\n- Now estimate $\\mu_i$   (let $\\phi = 1/\\sigma^2$ and $\\phi_{\\mu} = 1/\\sigma^2_\\mu$)\n\n \n\n- Calculate the \"posterior\"  $\\mu_i \\mid y_i, \\mu, \\phi, \\phi_\\mu$\n\n \n## Hiearchical Estimates\n\n- Posterior: $\\mu_i \\mid y_i, \\mu, \\phi, \\phi_\\mu \\overset{ind}{\\sim} \\textsf{N}(\\tilde{\\mu}_i, 1/\\tilde{\\phi}_\\mu)$\n \n\n- estimator of $\\mu_i$ weighted average of data and population parameter $\\mu$\n\n$$\\tilde{\\mu}_i = \\frac{\\phi_\\mu \\mu + \\phi y_i}\n                          {\\phi_\\mu + \\phi} \\qquad \\qquad \\tilde{\\phi}_\\mu = \\phi + \\phi_\\mu$$\n\n\n \n\n- if $\\phi_\\mu$ is large relative to $\\phi$ all of the $\\mu_i$ are close together and benefit by borrowing  information\n\n \n\n- in limit as $\\sigma^2_\\mu \\to 0$ or $\\phi_\\mu \\to \\infty$ we have $\\tilde{\\mu}_i = \\mu$  (all means are the same)\n\n \n\n- if $\\phi_\\mu$ is small relative to $\\phi$ little borrowing of information\n\n \n\n- in the limit as $\\phi_\\mu \\to 0$ we have $\\tilde{\\mu}_i = y_i$\n\n\n\n\n\n \n## Bayes Estimators and Bias\n\n\nNote: you often benefit from a hierarchical model, even if its not obvious that the $\\mu_i$s are related!\n\n \n\n- The MLE for the $\\mu_i$ is just the sample $y_i$.\n\n \n\n- $y_i$ is unbiased for $\\mu_i$ but can have high variability!\n\n \n\n- the posterior mean is actually biased.\n\n \n\n- Usually through the weighting of the sample data and prior, Bayes procedures have the tendency to pull the estimate of $\\mu_i$ toward the prior  or **shrinkage**\nmean.\n\n\n \n\n- <div class=\"question\">\nWhy would we ever want to do this? Why not just stick with the MLE?\n</div>\n\n\n\n \n\n\n\n-  MSE or Bias-Variance Tradeoff\n\n\n\n\n\n\n \n## Modern relevance\n\n- The fact that a biased estimator would do a better job in many estimation/prediction problems can be proven rigorously, and is referred to as **Stein's paradox**.\n\n \n\n- Stein's result implies, in particular, that the sample mean is an *inadmissible* estimator of the mean of a multivariate normal distribution in more than two dimensions   i.e. there are other estimators that will come closer to the true value in expectation.\n\n \n\n- In fact, these are Bayes point estimators (the posterior expectation of the parameter $\\mu_i$).\n\n \n\n- Most of what we do now in high-dimensional statistics is develop biased estimators that perform better than unbiased ones.\n\n \n\n- Examples: lasso regression, ridge regression, various kinds of hierarchical Bayesian models, etc.\n\n \n## Population Parameters\n\n\n- we don't know $\\mu$ (or $\\sigma^2$ and $\\sigma^2_\\mu$ for that matter)\n\n \n\n- Find marginal likelihood $\\cal{L}(\\mu, \\sigma^2, \\sigma^2_\\mu)$ by integrating out $\\mu_i$ with respect to $g$\n\n\n \n\n$$\\cal{L}(\\mu, \\sigma^2, \\sigma^2_\\mu) \\propto \\prod_{i = 1}^n \n\\int \\textsf{N}(y_i; \\mu_i, \\sigma^2)  \\textsf{N}(\\mu_i; \\mu, \\sigma^2_\\mu) \\, d \\mu_i$$\n\n \n\n- Product of predictive distributions for $Y_i \\mid \\mu, \\sigma^2, \\sigma^2_\\mu \\overset{iid}{\\sim} \\textsf{N}(\\mu, \\sigma^2 + \\sigma^2_\\mu)$  \n\n \n\n$$\\cal{L}(\\mu, \\sigma^2, \\sigma^2_\\mu) \\propto \\prod_{i = 1}^n (\\sigma^2 + \\sigma^2_\\mu)^{-1/2} \\exp \\left\\{ - \\frac{1}{2} \\frac{\\left(y_i - \\mu \\right)^2}{\\sigma^2 + \\sigma^2_\\mu }\\right\\}$$\n\n \n\n- Find MLE's\n\n \n## MLEs\n\n$$\\cal{L}(\\mu, \\sigma^2, \\sigma^2_\\mu) \\propto  (\\sigma^2 + \\sigma^2_\\mu)^{-n/2} \\exp\\left\\{ - \\frac{1}{2} \\sum_{i=1}^n\\frac{\\left(y_i - \\mu \\right)^2}{\\sigma^2 + \\sigma^2_\\mu }\\right\\}$$\n\n \n\n\n- MLE of $\\mu$: $\\hat{\\mu} = \\bar{y}$\n\n\n \n\n- Can we say anything about $\\sigma^2_\\mu$?  or $\\sigma^2$ individually?\n\n \n\n\n- MLE of $\\sigma^2 + \\sigma^2_\\mu$ is \n\n$$\\widehat{\\sigma^2 + \\sigma^2_\\mu} = \\frac{\\sum(y_i - \\bar{y})^2}{n}$$\n\n\n \n\n- Assume $\\sigma^2$ is known (say 1)\n\n$$\\hat{\\sigma}^2_\\mu = \\frac{\\sum(y_i - \\bar{y})^2}{n} - 1$$\n\n\n \n## Empirical Bayes Estimates\n\n- plug in estimates of hyperparameters into the prior and pretend they are known \n\n \n- resulting estimates are known as Empirical Bayes\n\n\n\n \n\n-  underestimates uncertainty\n\n \n\n- Estimates of variances may be negative -  constrain to 0 on the boundary)\n\n\n \n\n- Fully Bayes would put a prior on the unknowns\n\n \n##  Bayes and Hierarchical Models\n\n\n\n- We know the conditional posterior distribution of $\\mu_i$ given the other parameters, lets work with the marginal likelihood $\\cal{L}(\\theta)$\n\n \n\n- need a prior $\\pi(\\theta)$ for unknown parameters are  $\\theta = (\\mu, \\sigma^2, \\sigma^2_\\mu)$  (details later)\n\n\n \n\nPosterior\n\n$$\\pi(\\theta \\mid y) = \\frac{\\pi(\\theta) \\cal{L}(\\theta)}\n{\\int_\\Theta \\pi(\\theta) \\cal{L}(\\theta) \\, d\\theta} =\n\\frac{\\pi(\\theta) \\cal{L}(\\theta)}\n{m(y)}$$\n\n \n\nProblems:\n\n- Except for simple cases (conjugate models)  $m(y)$ is not available analytically\n\n \n## Large Sample Approximations\n\n- Appeal to BvM (Bayesian Central Limit Theorem) and approximate $\\pi(\\theta \\mid y)$ with a Gaussian distribution centered at the posterior mode $\\hat{\\theta}$  and asymptotic covariance matrix \n\n$$V_\\theta = \\left[- \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T} \\left\\{\\log(\\pi(\\theta)) + \\log(\\cal{L}(\\theta)) \\right\\} \\right]^{-1}$$\n\n \n\n- we can try to approximate $m(y)$  but this may involve a high dimensional integral\n\n \n\n- Laplace approximation to integral (also large sample)\n\n \n\nStochastic methods\n \n## Stochastic Integration\n\n\n\n$$\\textsf{E}[h(\\theta) \\mid y] =  \\int_\\Theta h(\\theta) \\pi(\\theta \\mid y) \\, d\\theta \\approx \\frac{1}{T}\\sum_{t=1}^{T} h(\\theta^{(t)}) \\qquad \\theta^{(t)} \\sim \\pi(\\theta \\mid y)$$\n \n\nwhat if we can't sample from the posterior but can sample from some distribution $q()$\n \n \n$$\\textsf{E}[h(\\theta) \\mid y] =  \\int_\\Theta h(\\theta) \\frac{\\pi(\\theta \\mid y)}{q(\\theta)} q(\\theta)\\, d\\theta \\approx \\frac{1}{T}\\sum_{t=1}^{T} h(\\theta^{(t)}) \\frac{\\pi(\\theta^{(t)} \\mid y)} {q(\\theta^{(t)})} \\qquad$$\nwhere $\\theta^{(t)} \\sim q(\\theta)$\n\n \n\nWithout the denominator  in $\\pi(\\theta \\mid y)$ we just have $\\pi(\\theta \\mid y) \\propto  \\pi(\\theta) \\cal{L}(\\theta)$\n\n\n \n\n-  use twice for numerator and denominator\n\n\n \n## Important Sampling Estimate\n\nEstimate of $m(y)$\n\n$$m(y) \\approx \\frac{1}{T} \\sum_{t=1}^{T}  \\frac{\\pi(\\theta^{(t)}) \\cal{L}(\\theta^{(t)})}{q(\\theta^{(t)})} \\qquad \\theta^{(t)} \\sim q(\\theta)$$\n \n\n$$\\textsf{E}[h(\\theta) \\mid y] \\approx \\frac{\\sum_{t=1}^{T} h(\\theta^{(t)}) \\frac{\\pi(\\theta^{(t)}) \\cal{L}(\\theta^{(t)})}{q(\\theta^{(t)})}}\n{ \\sum_{t=1}^{T}  \\frac{\\pi(\\theta^{(t)}) \\cal{L}(\\theta^{(t)})}{q(\\theta^{(t)})}}\n\\qquad \\theta^{(t)} \\sim q(\\theta)$$\n\n \n\n$$\\textsf{E}[h(\\theta) \\mid y] \\approx \\sum_{t=1}^{T} h(\\theta^{(t)}) w(\\theta^{(t)})  \\qquad \\theta^{(t)} \\sim q(\\theta)$$\n\nwith un-normalized weights  $w(\\theta^{(t)}) \\propto \\frac{\\pi(\\theta^{(t)}) \\cal{L}(\\theta^{(t)})}{q(\\theta^{(t)})}$\n\n(normalize to sum to 1)\n\n\n \n## Markov Chain Monte Carlo (MCMC)\n\n\n- Typically $\\pi(\\theta)$ and $\\cal{L}(\\theta)$ are easy to evaluate\n\n \n\n\n\n<div class=\"question\">How do we draw samples  only using evaluations of the prior and likelihood in higher dimensional settings?\n</div>\n \n\n- construct a Markov chain $\\theta^{(t)}$ in such a way the the stationary distribution of the Markov chain is the posterior distribution $\\pi(\\theta \\mid y)$!\n\n\n$$\\theta^{(0)} \\overset{k}{\\longrightarrow} \\theta^{(1)} \\overset{k}{\\longrightarrow} \\theta^{(2)} \\cdots$$\n \n\n- $k_t(\\theta^{(t-1)} ; \\theta^{(t)})$  transition kernel\n\n \n\n- initial state $\\theta^{(0)}$\n\n \n\n- choose some nice $k_t$ such that $\\theta^{(t)} \\to \\pi(\\theta \\mid y)$ as $t \\to \\infty$\n\n \n\n- biased samples initially but get closer to the target\n\n \n##  Metropolis Algorithm  (1950's)\n\n-  Markov chain $\\theta^{(t)}$\n\n \n\n- propose $\\theta^* \\sim g(\\theta^{(t-1)})$ where $g()$ is a symmetric distribution centered at $\\theta^{(t-1)}$\n\n \n\n- set $\\theta^{(t)} = \\theta^*$ with some probability\n\n \n\n- otherwise set $\\theta^{(t)} = \\theta^{(t-1)}$\n\n \n\nAcceptance probability is \n\n$$\\alpha = \\min \\left\\{ 1, \\frac{\\pi(\\theta^*) \\cal{L}(\\theta^*)}\n                           {\\pi(\\theta^{(t-1)}) \\cal{L}(\\theta^{(t-1)})}\\right\\}$$\n                            \n \n\n- ratio of posterior densities where normalizing constant cancels!\n\n \n## Example\n\n- Let's use a prior for $p(\\mu) \\propto 1$\n\n \n\n- Posterior for $\\mu \\mid \\sigma^2, \\sigma^2_\\mu$  is \n$\\textsf{N}\\left(\\bar{y}, \\frac{\\sigma^2 + \\sigma^2_\\mu}{n}\\right)$\n\n \n\n$$\\cal{L}(\\sigma^2, \\sigma^2_\\tau) \\propto\n(\\sigma^2 + \\sigma^2_\\mu)^{-\\frac{n - 1}{2}} \\exp \\left\\{ - \\frac 1 2 \\sum_i \\frac{(y_i - \\bar{y})^2}{\\sigma^2 + \\sigma^2_\\mu)} \\right\\}$$\n\n \n\n- Take $\\sigma^2 = 1$  \n\n \n\n- Use a $\\textsf{Cauchy}(0,1)$ prior on $\\sigma_\\mu$\n\n \n\n- Symmetric proposal for $\\sigma_\\tau$?    Try  a normal with variance $\\frac{2.4^2}{d} \\textsf{var}(\\sigma_\\mu)$ where $d$ is the dimension of $\\theta$  (d = 1)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n \n## Joint Posterior\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](05-hierarchical-models_files/figure-html/joint-1.png){fig-align='center' width=75%}\n:::\n:::\n\n \n## Marginal Posterior\n\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](05-hierarchical-models_files/figure-html/marg-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\nMLE of $\\sigma_\\mu$ is 0.11\n\n\n \n## Trace Plots\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](05-hierarchical-models_files/figure-html/traceplot-1.png){fig-align='center' width=1500}\n:::\n:::\n\n- Acceptance probability is 0.57   \n\n \n\n- Goal is around 0.44 in 1 dimension to 0.23 in higher dimensions\n\n \n## AutoCorrelation Function\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](05-hierarchical-models_files/figure-html/acf-1.png){fig-align='center' width=1500}\n:::\n:::",
    "supporting": [
      "05-hierarchical-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}