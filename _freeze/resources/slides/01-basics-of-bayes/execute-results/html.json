{
  "hash": "77a36eeabdd19648559780b401bb07c1",
  "result": {
    "markdown": "---\nsubtitle: \"STA 702: Lecture 1\"\ntitle: \"Basics of Bayesian Statistics\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\n---\n\n\n## Ingredients\n\n1.  [Prior Distribution]{style=\"color:red\"} $\\pi(\\theta)$ for\n    unknown $\\theta$\n\n2.  [Likelihood Function]{style=\"color:red\"}\n    ${\\cal{L}}(\\theta \\mid y ) \\propto p(y \\mid \\theta)$ (sampling\n    model)\n\n3.  [Posterior Distribution]{style=\"color:red\"}\n    $$\\pi(\\theta | y) = \\frac{\\pi(\\theta)p(y \\mid \\theta)}{\\int_{\\Theta}\\pi({\\theta})p(y\\mid {\\theta}) \\textrm{d}{\\theta}} = \\frac{\\pi(\\theta)p(y\\mid\\theta)}{p(y)}$$\n\n4.  [Loss Function]{style=\"color:red\"} Depends on what you want to\n    report; estimate of $\\theta$, predict future $Y_{n+1}$, etc\n\n## Posterior Depends on Likelihoods {.smaller}\n\n-   Likelihood function is defined up to a consant\n    $$c \\, {\\cal{L}}(\\theta \\mid Y) =  p(y \\mid \\theta) $$\n\n-   Bayes' Rule\n    $$\\pi(\\theta | y) = \\frac{\\pi(\\theta)p(y \\mid \\theta)}{\\int_{\\Theta}\\pi({\\theta})p(y\\mid {\\theta}) \\textrm{d}{\\theta}} = \n    \\frac{\\pi(\\theta)c {\\cal{L}}(\\theta \\mid y)}{\\int_{\\Theta}\\pi({\\theta})c{\\cal{L}}(\\theta \\mid y) \\textrm{d}{\\theta}}  = \n    \\frac{\\pi(\\theta){\\cal{L}}(\\theta \\mid y)}{m(y)}$$\n\n-   $m(y)$ is proportional to the marginal distribution of data\\\n    $$m(y) = \\int_{\\Theta}\\pi({\\theta}){\\cal{L}}(\\theta \\mid y) \\textrm{d}{\\theta}$$\n\n-   [marginal likelihood ]{style=\"color:red\"} of this model or \"evidence\"\n\n. . .\n\n**Note:** the marginal likelihood and maximized likelihood are\n    *very* different!\n\n## Binomial Example\n\n- Binomial sampling  $Y \\mid n, \\theta \\sim \\textsf{Binomial}(n, \\theta)$\n\n\n- Probability Mass Function \n$$p(y \\mid \\theta) = {n \\choose y} \\theta^y(1-\\theta)^{n-y}$$\n\n\n\n- Likelihood  $\\cal{L}(\\theta ) =  \\theta^y(1-\\theta)^{n-y}$\n\n\n-  MLE $\\hat{\\theta}$ of Binomial is $\\bar{y} = y/n$ proportion of\nsuccesses\n\n- Recall Derivation!\n\n## Marginal Likelihood\n\n\n$$m(y) = \\int_\\Theta \\cal{L}(\\theta)  \\pi(\\theta) \\textrm{d}\\theta=  \\int_\\Theta \\theta^y(1-\\theta)^{n-y} \\pi(\\theta) \\textrm{d}\\theta$$\n\n\"Averaging\" of likelihood over prior $\\pi(\\theta) = 1$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01-basics-of-bayes_files/figure-revealjs/marg-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Binomial Example {.smaller}\n\n-   [Prior]{style=\"color:red\"}  $\\theta \\sim \\textsf{U}(0,1)$ or\n    $\\pi(\\theta) = 1, \\quad \\textrm{for } \\theta \\in (0,1)$\n\n-   [Marginal]{style=\"color:red\"}  $m(y)   =  \\int_0^1 \\theta^y(1-\\theta)^{n-y}\\, 1 \\,\\textrm{d}\\theta$\n\n\n\n-   Special function known as the [beta function]{style=\"color:red\"} - see Rudin\n$${B}(a, b) =  \\int_0^1 \\theta^{a - 1}(1-\\theta)^{b - 1} \\,\\textrm{d}\\theta $$\n\n-  [Posterior Distribution]{style=\"color:red\"} \n$$\\pi(\\theta \\mid y ) = \\frac{ \\theta^{(y+1)-1} (1-\\theta)^{(n - y +1) -1}}{B(y + 1,n - y + 1)}$$\n\n. . .\n\n$$\\theta \\mid y \\sim \\textsf{Beta}(y + 1, n - y + 1) $$\n\n## Beta Prior Distributions {.smaller}\n\n$\\textsf{Beta}(a, b)$ is a probability density function (pdf) on (0,1),\n\n. . .\n\n$$\\pi(\\theta) = \\frac{1}{B(a,b)} \\theta^{a-1} (1-\\theta)^{b -1}$$\n \n-  Use the [kernel]{style=\"color:red\"}  trick to find the posterior\n$$\\pi(\\theta \\mid y) \\propto \\cal{L}(\\theta \\mid y) \\pi(\\theta)$$\n\n- Write down likelihood and prior  (ignore constants wrt $\\theta$)\n- Recognize kernel of density\n- Figure out normalizing constant/distribution\n\n## Try it!\n\n## Prior to Posterior Updating Binomial Data {.smaller}\n\n-   [Prior]{style=\"color:red\"}  $\\textsf{Beta}(a, b)$\n\n-   [Posterior]{style=\"color:red\"}  $\\textsf{Beta}(a + y, b + n - y)$\n\n-   [Conjugate]{style=\"color:red\"}  prior & posterior distribution are in the same family\n    of distributions, (Beta)\n\n-   Simple updating of information from the prior to posterior\n\n    -   $a + b$ \"prior sample size\" (number of trials in a hypothetical\n        experiment)\n\n    -   $a$ \"number of successes\"\n\n    -   $b$ \"number of failures\"\n\n-   [prior elicitation]{style=\"color:red\"}  (process of choosing the\n    prior hyperparamters) based on historic or imaginary data\n\n## Summaries & Properties {.smaller}\n\n- For  $\\theta \\sim \\textsf{Beta}(a,b)$  let $a + b = n_0$ \"prior sample size\"\n\n- Prior mean $$\\textsf{E}[\\theta] = \\frac{a}{a+b}  \\equiv \\theta_0 $$\n\n- Posterior mean\n$$\\textsf{E}[\\theta \\mid y ] = \\frac{a + y }{a+b +n}  \\equiv \\tilde{\\theta} $$\n\n- Rewrite with MLE $\\hat{\\theta} = \\bar{y} = \\frac{y}{n}$ and prior mean\n$$\\textsf{E}[\\theta \\mid y ] = \\frac{a + y }{a+b +n}  \n= \\frac{n_0}{n_0 + n} \\theta_0  + \\frac{n}{n_0 + n} \\hat{\\theta}$$\n\n- Weighted average of prior mean and MLE where weight for\n$\\theta_0 \\propto n_0$ and weight for $\\hat{\\theta} \\propto n$\n\n## Properties {.smaller}\n\n\n- Posterior mean \n$$\\tilde{\\theta} = \\frac{n_0}{n_0 + n} \\theta_0  + \\frac{n}{n_0 + n} \\hat{\\theta}$$\n\n-   in finite samples we get **shrinkage**: posterior mean pulls the MLE\n    toward the prior mean; amount depends on prior sample size $n_0$ and\n    data sample size $n$\n\n-   **regularization** effect to reduce Mean Squared Error for\n    estimation with small sample sizes and noisy data\n\n    -   introduces some bias (in the frequentist sense) due to prior\n        mean $\\theta_0$\n\n    -   reduces variance (bias-variance trade-off)\n\n-   helpful in the Binomial case, when sample size is small or\n    $\\theta_{\\text{true}} \\approx 0$ (rare events) and\n    $\\hat{\\theta} = 0$ (inbalanced categorical data)\n\n-   as we get more information from the data $n \\to \\infty$ we have\n    $\\tilde{\\theta} \\to \\hat{\\theta}$ and **consistency** ! As\n    $n \\to \\infty, \\textsf{E}[\\tilde{\\theta}] \\to \\theta_{\\text{true}}$\n\n## Some possible prior densities\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-basics-of-bayes_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n## Prior Choice {.smaller}\n\n-   Is the uniform prior $\\textsf{Beta}(1,1)$ non-informative?\n\n    -   No- if $y = 0$ (or $n$) sparse/rare events saying that we have a\n        prior \"historical\" sample with 1 success and 1 failure ( $a = 1$\n        and $b = 1$ ) can be very informative\n\n-   What about a uniform prior on the log odds?\n    $\\eta \\equiv \\log\\left( \\frac{\\theta} {1 - \\theta} \\right)$?\n$$\\pi(\\eta) \\propto 1, \\qquad \\eta \\in \\mathbb{R}$$\n\n    -   Is this a **proper** prior distribution?\n\n    -   what would be induced measure for $\\theta$?\n\n    -   Find Jacobian  (exercise!)\n$$\\pi(\\theta) \\propto \\theta^{-1} (1 - \\theta)^{-1}, \\qquad \\theta \\in (0,1)$$\n\n    -   limiting case of a Beta $a \\to 0$ and $b \\to 0$ (Haldane's prior)\n\n## Formal Bayes\n\n-   use of improper prior and turn the Bayesian crank\n\n-   calculate $m(y)$ and renormalize likelihood times \"improper prior\"\n    if $m(y)$ is finite\n\n-   formal posterior is $\\textsf{Beta}(y, n-y)$ and reasonable only if\n    $y \\neq 0$ or $y \\neq n$ as $B(0, -)$ and $B(-, 0)$ (normalizing\n    constant) are undefined!\n\n-   no shrinkage\n    $\\textsf{E}[\\theta \\mid y] = \\frac{y}{n} = \\tilde{\\theta} = \\hat{\\theta}$\n\n## Invariance {.smaller}\n\n- Jeffreys argues that priors should be invariant to transformations to be\nnon-informative.  . . . i.e. if we reparameterize with $\\theta = h(\\rho)$ then the rule should be that\n$$\\pi_\\theta(\\theta) = \\left|\\frac{ d \\rho} {d \\theta}\\right| \\pi_\\rho(h^{-1}(\\theta))$$\n\n\n- Jefferys' rule is to pick $\\pi(\\rho) \\propto (I(\\rho))^{1/2}$\n\n- Expected Fisher Information for $\\rho$\n$$ I(\\rho) = - \\textsf{E} \\left[ \\frac {d^2 \\log ({\\cal{L}}(\\rho))} {d^2 \\rho} \\right]$$\n\n- For the Binomial example\n$\\pi(\\theta) \\propto \\theta^{-1/2} (1 - \\theta)^{-1/2}$\n\n- Thus Jefferys' prior is a $\\textsf{Beta}(1/2, 1/2)$\n\n## Why ?\n\nChain Rule!\n\n- Find Jefferys' prior for $\\theta$ where $Y \\sim \\textsf{Ber}(\\theta)$\n\n- Find information matrix   $I(\\rho)$ for $\\rho = \\rho(\\theta)$ from $I(\\theta)$\n\n- Show that the prior satisfies the invariance property!\n\n- Find Jeffreys' prior for $\\rho = \\log(\\frac{\\theta}{1 - \\theta})$\n",
    "supporting": [
      "01-basics-of-bayes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}