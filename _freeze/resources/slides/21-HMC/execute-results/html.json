{
  "hash": "d8e4231002e48092f51f65af06287f93",
  "result": {
    "markdown": "---\ntitle: \"Lecture 21: Hamiltonian Monte Carlo\"\nauthor: \"Merlise Clyde\"\nsubtitle: \"STA702\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n\n\n\n\n## Gibbs sampling\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\newcommand{\\Mb}{\\mathbf{M}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\text{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\Psib}{\\boldsymbol{\\Psi}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n\n\n\n- Consider model\n$$\n\\begin{aligned}\n\\boldsymbol{Y}_1, \\dots, \\boldsymbol{Y}_n &\\sim \\N_2 \\left(\\boldsymbol{\\theta}, \\Sigmab \\right); \\\\\n\\theta_j &\\sim \\N(0, 1)~~~~~~j=1,2.\n\\end{aligned}\n$$\n\n\n- Suppose that the covariance matrix $\\Sigmab$ is known and has the form\n$$\\Sigmab = \n\\left[\\begin{array}{cc}\n1 & \\rho \\\\\n\\rho & 1\n\\end{array}\\right]$$\n\n   \n-  What happens when $\\rho = 0.995$ for sampling from full conditionsals for $\\theta_1$ and  $\\theta_2$?\n\n   \n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n  \n## Gibbs vs Stan samples\n\n\n::: {.cell layout-align=\"center\" hash='21-HMC_cache/revealjs/rho_4d1ee9b9fda279c79440cee83a7091e7'}\n\n:::\n\n::: {.cell layout-align=\"center\" fig='true' hash='21-HMC_cache/revealjs/output_Gibbs_311b5651701efd59c3b0a12c301f9af4'}\n::: {.cell-output-display}\n![](21-HMC_files/figure-revealjs/output_Gibbs-1.png){fig-align='center' width=3000}\n:::\n\n::: {.cell-output-display}\n![](21-HMC_files/figure-revealjs/output_Gibbs-2.png){fig-align='center' width=3000}\n:::\n:::\n\n\n  \n## ACF\n\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](21-HMC_files/figure-revealjs/acf-1.png){fig-align='center' width=3000}\n:::\n\n::: {.cell-output-display}\n![](21-HMC_files/figure-revealjs/acf-2.png){fig-align='center' width=3000}\n:::\n:::\n\n\n  \n## Hamiltonian Monte Carlo (HMC)\n\n-  HMC creates transitions that *efficiently explore the parameter space* by using concepts from Hamiltonian mechanics.\n   \n\n- In Hamiltonian mechanics, a physical system is  specified by positions $\\mathbf{q}$ and momenta $\\mathbf{p}$. \n   \n\n- A space defined by these coordinates is called a **phase space**\n   \n\n- If the parameters of interest in a typical MCMC method are denoted as $q_1, \\dots, q_K$, then HMC introduces auxiliary **momentum** parameters $p_1, \\dots, p_K$ such that the algorithm produces draws from the joint density:\n$$\n\\pi( \\mathbf{q}, \\mathbf{p}) = \\pi (\\mathbf{p} | \\mathbf{q}) \\pi(\\mathbf{q})\n$$\n   \n\n- marginalizing over the $p_k$'s, we recover the marginal distribution of the $q_k$'s Therefore, if we create a Markov Chain that converges to $\\pi(\\mathbf{q}, \\mathbf{p})$, we have immediate access to samples from $\\pi(\\mathbf{q})$, which is our target distribution.\n\n  \n## Hamiltonian\n\n- Hamilton's equations describe the time evolution of the system in terms of the **Hamiltonian**, $\\mathcal{H}$, which  corresponds to the total energy of the system:\n$$\\mathcal{H}(\\mathbf{p},\\mathbf{q}) = K(\\mathbf{q}, \\mathbf{p}) + U(\\mathbf{q})$$\n   \n\n- $K(\\mathbf{q}, \\mathbf{p})$ represents the **kinetic energy** of the system and is equal to the negative logarithm of the momentum distribution, e.g.\n$$K(\\mathbf{p}) = \\frac{\\mathbf{p}^T \\Mb^{-1} \\mathbf{p}}{2} = \\sum_ i \\frac{p_i^2}{2 m_i}$$\n- $\\Mb$ is the Mass matrix\n   \n\n- $U(\\mathbf{q})$  the **potential energy** of the system; equal to the negative logarithm of the distribution of $\\mathbf{q}$.\n \n- Joint $\\pi(\\mathbf{q}, \\mathbf{p}) \\propto e^{- \\cal{H}(\\mathbf{q}, \\mathbf{p})} = e^{- K(\\mathbf{p})} e^{- U(\\mathbf{q})}$\n \n\n \n  \n## Evolution \n- At each iteration of the sampling algorithm, HMC implementations make draws from some distribution $\\pi(\\mathbf{p} | \\mathbf{q})$  and then *evolves the system* $(\\mathbf{q}, \\mathbf{p})$ to obtain the next sample of $\\mathbf{q}$. \n \n   \n- To \"evolve the system\" is to move $(\\mathbf{q}, \\mathbf{p})$ forward in \"time,\" i.e. to change the values of $(\\mathbf{q}, \\mathbf{p})$ according to Hamilton's differential equations: \n$$\\begin{align}\n\\frac{d \\mathbf{p}}{dt} &= - \\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{q}} = -\\frac{\\partial K}{\\partial \\mathbf{q}} - \\frac{\\partial U}{\\partial \\mathbf{q}} \\\\\n\\frac{d \\mathbf{q}}{dt} &= +\\frac{\\partial \\mathcal{H}}{\\partial \\mathbf{p}} = +\\frac{\\partial K}{\\partial \\mathbf{p}}\n\\end{align}$$\n   \n\n- Defines a mapping $T_s$ from the state at any time $t$ to the state at $t+s$\n   \n. . .\n\n> \"The differential change in momentum parameters $\\mathbf{p}$ over time is governed in part by the differential information of the density over the target parameters.\"\n\n\n  \n## Key Properties\n\n1)  **Reversibility** The mapping  of the state at time $t$ $(\\mathbf{p}(t), \\mathbf{q}(t))$ to the state at $t+s$ $(\\mathbf{p}(t+s), \\mathbf{q}(t+s))$ is one-to-one and we have an inverse $T_{-s}$ - obtained by negating the derivatives; $K(\\mathbf{p}) = K(-\\mathbf{p})$  _MCMC updates using the dymamics don't modify invariant distribution!_\n\n \n   \n\n2) **Invariance/Conservation** the dymamics keep the Hamiltonian invariant - if we use the dynamics to generate proposals, the acceptance probability of MH is equal to one if $\\cal{H}$ is kept invariant!\n\n   \n\n3)  **Volume Preservation/Symplectiness**  the mapping $T_s$ of a region $R$  to $T_s(R)$ preserves volume      means that we do not need to compute Jacobians \n\n. . .\n   \n\n> in practice we need to use approximations to solve the PDE's so won't have exact invariance etc so acceptance probability is not 1!\n\n\n  \n##  Approximate Solutions to Differential Eqs\n\n- Discretize time into steps $\\epsilon$ \n   \n\n- Euler's Method for $i$th coordinate\n$$\\begin{align}\np_i(t + \\epsilon) & = p_i(t) + \\epsilon \\frac{d p_i}{t}(t) =  p_i(t) - \\epsilon \\frac{\\partial U(q_i(t))} {\\partial q_i} \\\\\nq_i(t + \\epsilon) & = q_i(t) + \\epsilon \\frac{d q_i}{t}(t) =  q_i(t) + \\epsilon \\frac{\\partial K(p_i(t))} {\\partial p_i} =  q_i(t) + \\epsilon \\frac{p_i(t)}{m_i}\\\\\n\\end{align}$$\n\n   \n\n-  Modified Euler method \n$$\\begin{align}\np_i(t + \\epsilon) & =  p_i(t) - \\epsilon \\frac{\\partial U(q_i(t))} {\\partial q_i} \\\\\nq_i(t + \\epsilon) & =   q_i(t) + \\epsilon \\frac{p_i(t + \\epsilon)}{m_i}\\\\\n\\end{align}$$\n\n\n\n  \n## Leapfrog\n\n- Divide into half steps\n\n   \n\n- apply Modified Euler\n$$\\begin{align}\np_i(t + \\epsilon/2) & =  p_i(t) - \\frac{\\epsilon}{2} \\frac{\\partial U(q_i(t))} {\\partial q_i} \\\\\nq_i(t + \\epsilon) & = q_i(t) + \\epsilon \\frac{p_i(t + \\epsilon/2)}{m_i}\\\\\np_i(t + \\epsilon) & =  p_i(t) - \\frac{\\epsilon}{2} \\frac{\\partial U(q_i(t + \\epsilon))} {\\partial q_i} \n\\end{align}$$\n   \n\n- Preserves volume exactly\n   \n\n- Reversible\n   \n\n- We don't get exact invariance (so probability of acceptance is not 1)\n   \n\n- Step size and number of steps is still important!\n\n\n  \n## MCMC with HMC \n\nSteps:  replace $\\mathbf{q}$ with $\\boldsymbol{\\theta}$\n\n   \n\n1) sample a new value for the momentum $\\mathbf{p}^{(t)} \\sim \\N(\\zero_K, \\Mb)$\n\n   \n\n2) Metropolis: from current state $(\\mathbf{q}^{(t-1)}, \\mathbf{p}^{(t)})$ simulate proposal $(\\mathbf{q}^*, \\mathbf{p}^*)$ using Hamiltonian dynamics by applying Leapfrog with step size $\\epsilon$ for $L$ steps (tuning parameters) (start with $\\epsilon*L = 1$)\n\n   \n\n3) Accept or reject acceptance probability is \n$$\\min \\{1, \\exp( - \\cal{H}(\\mathbf{q}^*, \\mathbf{p}^*) + \\cal{H}(\\mathbf{q}^{(t-1)}, \\mathbf{p}^{(t)}) \\}$$\n. . .\n\ntheory suggests optimal acceptance rate is around 65%\n  \n## Tuning\n\n\n- in addition to tuning $\\epsilon$ and $L$, we can tune $\\Mb$\n   \n\n- $\\text{Cov}(\\mathbf{q}) = \\V$ can be highly variable\n\n   \n- Consider reparameterization $\\A \\mathbf{q} = \\mathbf{q}^\\prime$ so that  $\\text{Cov}(\\A \\mathbf{q}) =  \\A \\V \\A^T = \\I_d$; $\\A = \\V^{-1/2}$\n   \n\n- eliminates posterior correlation!\n\n   \n- general trick of reparameterizing to reduce posterior correlation is  often called **pre-conditioning** - improves efficiency! \n   \n\n- use $\\Mb = \\I_d$\n   \n\n-  Automatic tuning is achieved by the No-U-Turn-Sampler (NUTS)\n   (bit complicated, but used by STAN)\n   \n\n- other variations Metropolis-Adjusted Langevin Algorithm (MALA) \n\n  \n## Hybrid Approaches\n\n- Recall mixed effects model\n$$Y_{ij} = \\x_{ij}^T \\b + \\z_{ij}^T \\g_j + \\epsilon_{ij} \\qquad \\epsilon_{ij}  \\sim \\N(0, \\sigma^2)$$\n   \n\n- random effects $\\g_j \\sim \\N_d(\\zero_d, \\Psib)$  (diagonal $\\Psib$)\n   \n\n- marginalize over the random effects\n$$\\Y_{j} = \\N(\\X_j \\b, \\Z_j \\Psib \\Z_j^T + \\sigma^2 \\I_{n_j})$$\n\n   \n- we could use Gibbs on the conditional model, but we may get slow mixing (i.e. due to updating variance components)\n\n   \n\n- run HMC within Gibbs to update the variance components $\\Psib$  and $\\sigma^2$ using the marginal model given $\\b$\n   \n\n- HMC in its basic form doesn't like constraints so reparameterize to use log transformations\n\n\n\n\n  \n## Advantages & Disadvantages\n\n- HMC can produce samples with low correlation and high acceptance ratio!\n\n- can be slow with long or short tailed distributions (use local curvature in $\\Mb$)\n\n   \n\n- driven by step size  (larger time steps mean values are farther away but may lead to lower acceptance- \n  error is $O(\\epsilon^2)$ for the leapfrog method)\n\n   \n\n- number of steps (more steps reduces correlation; to avoid U turns stan uses NUTS)\n\n   \n\n- most implementations limited to continuous  variables (need gradients of log densities) \n\n   \n\n- need to calculate gradients (analytic or automatic differentiation methods)\n\n   \n\n- can mix Gibbs (for discrete) and HMC (for continuous)\n\n   \n\n- Nishimura et al (2020 Biometrika) for  HMC with discrete targets\n\n- rates of convergence and other theory\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}