{
  "hash": "4a8b9e98545109f4d2f4ea50d7675ac9",
  "result": {
    "markdown": "---\nsubtitle: \"STA 702: Lecture 4\"\ntitle: \"Prior/Posterior Checks\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n \n\n## Uses of Posterior Predictive {.smaller}\n\n- Plot the entire density or summarize\n\n  \n \n- Available analytically for conjugate families\n\n  \n\n- Monte Carlo Approximation\n$$p(y_{n+1} \\mid y_1, \\ldots y_n) \\approx \\frac 1 T \\sum_{t = 1}^T  p(y_{n+1} \\mid \\theta^{(t)})$$ \nwhere $\\theta^{(t)} \\sim \\pi(\\theta \\mid y_1, \\ldots y_n)$ for $t = 1, \\ldots, T$ \n\n  \n\n- T samples from the posterior distribution\n\n  \n\n- Empirical Estimates & Quantiles from Monte Carlo Samples\n\n\n \n## Models\n\n- So far this all assumes we have a correct sampling model and a \"reasonable\"  prior distrbution\n\n\n- George Box:  _All models are wrong but some are useful_\n\n  \n\n- \"Useful\" $\\rightarrow$ model provides a good approximation; there aren't clear aspects of the data that are ignored or misspecified\n\n-  how can we decide if a model is misspecified and needs to change?\n \n## Example {.smaller}\n\n- Poisson model\n$$Y_i  \\mid \\theta \\stackrel{iid}{\\sim }\\textsf{Poisson}(\\theta) \\qquad i = 1, \\ldots, n$$\n- How might our model be misspecified?\n\n    - Poisson assumes that $\\textsf{E}(Y_i) = \\textsf{Var}(Y_i) = \\theta$\n\n    - it's _very_ common for data to be **over-dispersed** $\\textsf{E}(Y_i) <  \\textsf{Var}(Y_i)$\n    \n    - ignored additional structure in the data, i.e. data are not _iid_\n    \n    - **zero-inflation**  many more zero values than consistent with the poisson model \n\n\n\n## Posterior Predictive Checks {.smaller}\n\n- Guttman (1967), Rubin (1984) proposed the use of [Posterior Predictive Checks (PPC)]{style=\"color:red}] for model criticism;  further developed by Gelman et al (1996)\n\n-  the spirit of posterior predictive checks is that \"_If my model is good, then its posterior predictive distribution will generate data that look like my oberved data_\"\n\n\n\n- $y^{\\text{obs}}$ is the observed  data\n\n\n\n- $y^{\\text{rep}}$ is a new dataset sampled from the posterior predictive $p(y^{\\text{rep}} \\mid y^{\\text{obs}})$ of size $n$  (same  size as the observed)\n\n\n- Use a [diagnostic statistic]{style=\"color:red\"}  $d(y)$ to capture some feature of the data that the model may fail to capture, say variance\n\n- compare $d(y^{\\text{obs}})$ to the reference distribution of $d(y^{\\text{rep}})$ \n\n- Use Posterior Predictive P-value as a summary\n$$ p_{PPC} = P(d(y^{\\text{obs}}) > d(y^{\\text{rep}}) \\mid y^{\\text{obs}})\n$$\n\n \n##  Formally {.smaller}\n\n- choose a \"diagnostic statistic\" $d(\\cdot)$  that captures some summary of the data, e.g. $\\textsf{Var}(y)$ for over-dispersion, where large values of the statistic would be surprising if the model were correct.\n\n  \n\n- $d(y^{\\text{obs}}) \\equiv d_{\\textrm{obs}}$ value of statistic in observed data\n\n  \n\n- $d(y^{\\text{rep}}_t)  \\equiv d_{\\textrm{pred}}$ value of  statistic for the $t$th random dataset drawn from the posterior predictive distribution\n  1) Generate $\\theta_t  \\stackrel{iid}{\\sim}p(\\theta \\mid y^{\\textrm{obs}})$\n  2)  Generate $y^{\\textrm{rep}_t} \\mid \\theta_t \\stackrel{iid}{\\sim} p(y \\mid \\theta_t)$\n  3) Calculate $d(y^{\\text{rep}}_t)$\n  \n\n- plot posterior predictive distribution of $d(y^{\\text{rep}}_t)$ and add $d_{\\textrm{obs}}$ \n\n  \n\n- How _extreme_ is $t_{\\textrm{obs}}$ compared to the distribution of $d(y^{\\text{rep}})$?\n\n- compute p-value $p_{PPC} = \\frac 1 T \\sum_t I(d(y^{\\text{obs}}) > d(y^{\\text{rep}}_t))$\n\n \n##  Example with Over Dispersion {.smaller}\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {.cell layout-align=\"center\" fig='false'}\n\n```{.r .cell-code}\nn = 100; phi = 1; mu = 5\ntheta.t = rgamma(n,phi,phi/mu)\ny = rpois(n, theta.t)\na = 1; b = 1;\nt.obs = var(y)\n\nnT = 10000\nt.pred = rep(NA, nT)\nfor (t in 1:nT) {\n  theta.post = rgamma(1, a + sum(y),\n                         b + n)\n  y.pred = rpois(n, theta.post)\n  t.pred[t] = var(y.pred)\n}\n\nhist(t.pred, \n     xlim = range(c(t.pred, t.obs)),\n     xlab=\"var\", \n     main=\"Posterior Predictive Distribution\")\n\nabline(v = t.obs, col=\"red\")\n```\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](04-predictive-checks_files/figure-revealjs/overdispersion-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n:::\n::::\n\n \n## Zero Inflated Distribution {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](04-predictive-checks_files/figure-revealjs/zero-1.png){fig-align='center' width=1500}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nR Code to generate zero inflated \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn = 1000\nmu = 5; phi = 1\ntheta.t = rgamma(n,phi,phi/mu)\nz = rbinom(n, 1, .90)\ny = rpois(n, theta.t)*z\n```\n:::\n\n\n- Let the $d()$ be the proportion of zeros in the sample\n$$\\begin{aligned}\nd(y) & = \\frac{\\sum_{i = 1}^{n}1(y_i = 0)}{n} \\\\\n     & = 0.24\n\\end{aligned}$$   \n:::\n::::\n\n \n##  Posterior Predictive Distribution\n\n\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](04-predictive-checks_files/figure-revealjs/zeroPP-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n \n## Posterior Predictive p-values (PPPs) {.smaller}\n\n- p-value is probability of seeing something  as extreme or more so under a hypothetical  \"null\" model\n\n- from a frequentist perspect, one appealing property of p-values is that they should be uniformally distributed under the \"null\" model\n\n  \n\n- PPPs advocated by Gelman & Rubin in papers and BDA are not **valid** p-values generally.  They are do not have a  uniform  distribution under the hypothesis that the model is correctly specified\n\n  \n\n- the PPPs tend to be concentrated around 0.5, tend not to reject  (conservative)\n\n  \n\n- theoretical reason for the incorrect distribution is due to double use of the data\n\n  \n\n- **DO NOT USE as a formal test!**\nuse as a diagnostic plot to see how model might fall flat, but be cautious!\n\n## Example: Bivariate Normal {.smaller}\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](04-predictive-checks_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=1500}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](04-predictive-checks_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=1500}\n:::\n:::\n\n\n- PPP = 0.52\n\n- What's happening?\n:::\n::::\n\n## Problems with PPC {.smaller}\n\n- Bayarri & Berger (2000) provides more discussion about why PPP are not always calibrated\n\n- Double use of the data;  $Y^{\\text{rep}}$ depends on the observed diagnostic in last case\n\n- Bayarri & Berger propose partial predictive p-values  and conditional predictive p-values that avoid double use of the data by \"removing\" the contribution of $d_{\\text{obs}}$ to the posterior for $\\theta$ or conditioning on a statistic, such as the MLE of $\\theta$\n\n- heuristically, need the diagnostic to be independent of posterior for $\\theta$ (asymptoptically) under the assumed model \n\n- not always easy to find!\n\n- Moran  et al (2022) propose a workaround to avoid double use of the data  by spliting the data $y_{\\text{obs}}, y_{\\text{new}}$, use  $y_{\\text{obs}}$, to learn $\\theta$ and the other to calculate $d_{\\textrm{new}}$  \n\n- can be calculated via simulation easily\n\n\n\n## POP-PC of Moran et al\n\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](04-predictive-checks_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=1500}\n:::\n:::\n\n\n\n- POP-PPC = 0.35\n \n## Modeling Over-Dispersion {.smaller}\n\n- Original Model  $Y_i \\mid \\theta \\sim \\textsf{Poisson}(\\theta)$\n\n  \n\n- cause of overdispersion is variation in the rate\n$$ Y_i \\mid \\theta_i \\sim \\textsf{Poisson}(\\theta_i)$$\n\n  \n- model variation via prior\n$$\\theta_i \\sim \\pi_\\theta()$$\n\n  \n\n- $\\pi_\\theta()$ characterizes variation in the rate parameter across inviduals\n\n  \n\n- Simple Two Stage Hierarchical Model\n\n \n\n\n## Modeling Perspectives {.smaller}\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n1) start with a simple model \n  - ask if there are surprises through Posterior Checks\n  - need calibrated  diagnostic(s) with good power\n  - need these to work even if starting model is relatively complex\n  - other informal diagnostics (residuals)\n  - remodel if needed based on departures\n  - Bayesian meaning?\n  \n:::\n\n::: {.column width=\"50%\"}\n\n2) start with a fairly complex model or models\n - shrinkage to prevent overfitting\n - formal tests for simplifying models\n - methods to combine multiple models to express uncertaity\n - properties\n \n \n:::\n::::\n  \n\n## Example\n\n$$\\theta_i \\sim \\textsf{Gamma}(\\phi \\mu, \\phi)$$\n  \n\n- Find pmf for $Y_i \\mid \\mu, \\phi$\n\n  \n\n- Find $\\textsf{E}[Y_i \\mid \\mu, \\phi]$ and $\\textsf{Var}[Y_i \\mid \\mu, \\phi]$ \n\n  \n\n- Homework: \n$$\\theta_i \\sim \\textsf{Gamma}(\\phi, \\phi/\\mu)$$\n           \n  \n\n- Can either of these model zero-inflation?\n\n\n\n",
    "supporting": [
      "04-predictive-checks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}