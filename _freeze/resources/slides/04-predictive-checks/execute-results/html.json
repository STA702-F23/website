{
  "hash": "ef1af763a5d6f4ae96d6f98f412649d6",
  "result": {
    "markdown": "---\nsubtitle: \"STA 702: Lecture 4\"\ntitle: \"Comparing Estimators & Prior/Posterior Checks\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n \n## Normal Model Setup from Last Class\n\n-  independent observations \n$\\mathbf{y} = (y_1,y_2,\\ldots,y_n)^T$\nwhere each $Y_i \\mid \\theta\\stackrel{iid}\\sim \\textsf{N}(\\theta, 1/\\tau)$  (iid)\n\n  \n\n- The likelihood for $\\theta$ is proportional to the sampling model\n\n\n$$\\begin{split}\n\\cal{L}(\\theta) & \\propto \\tau^{\\frac{n}{2}} \\ \\exp\\left\\{-\\frac{1}{2} \\tau \\sum_{i=1}^n (y_i-\\theta)^2\\right\\}\\\\\n& \\propto \\tau^{\\frac{n}{2}} \\ \\exp\\left\\{-\\frac{1}{2} \\tau \\sum_{i=1}^n \\left[ (y_i-\\bar{y}) - (\\theta - \\bar{y}) \\right]^2 \\right\\}\\\\\n& \\propto \\tau^{\\frac{n}{2}} \\ \\exp\\left\\{-\\frac{1}{2} \\tau \\left[ \\sum_{i=1}^n (y_i-\\bar{y})^2 + \\sum_{i=1}^n(\\theta - \\bar{y})^2 \\right] \\right\\}\\\\\n& \\propto \\tau^{\\frac{n}{2}} \\ \\exp\\left\\{-\\frac{1}{2} \\tau \\left[ \\sum_{i=1}^n (y_i-\\bar{y})^2 + n(\\theta - \\bar{y})^2 \\right] \\right\\}\\\\\n& \\propto \\exp\\left\\{-\\frac{1}{2} \\tau n(\\theta - \\bar{y})^2\\right\\}\n\\end{split}$$\n\n\n\n \n## Exercises for Practice\n\n<div class=\"question\">\nTry this\n\n</div>\n \n\n1)  Use $\\cal{L}(\\theta)$  based on $n$ observations to find  $\\pi(\\theta \\mid y_1, \\ldots, y_n)$ based on the sufficient statistics and prior  $\\theta \\sim \\textsf{N}(\\theta_0, 1/\\tau_0)$\n\n  \n\n2)\nUse $\\pi(\\theta \\mid y_1, \\ldots, y_n)$ to find the posterior predictive distribution for $Y_{n+1}$\n\n \n## After $n$ observations \nPosterior for $\\theta$ \n\n$$\\theta \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\frac{\\tau_0 \\theta_0 + n \\tau \\bar{y}}\n{\\tau_0 + n \\tau}, \\frac{1}{ \\tau_0 + n \\tau} \\right)$$\n\n  \n\nPosterior Predictive Distribution for $Y_{n+1}$\n\n$$Y_{n+1} \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\frac{\\tau_0 \\theta_0 + n \\tau \\bar{y}}\n{\\tau_0 + n \\tau}, \\frac{1}{\\tau} + \\frac{1}{ \\tau_0 + n \\tau} \\right)$$\n\n  \n\n- Shrinkage of the MLE to the prior mean\n\n\n \n## Results with Jeffreys' Prior\n - What if $\\tau_0 \\to 0$? (or $\\sigma^2_0 \\to \\infty$)\n \n  \n\n- Prior predictive $\\textsf{N}(\\theta_0, \\sigma^2_0 + \\sigma^2 )$  (not  proper in the limit)\n\n  \n\n- Posterior for $\\theta$ (formal posterior)\n\n$$\\theta \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\frac{\\tau_0 \\theta_0 + n \\tau \\bar{y}}\n{\\tau_0 + n \\tau}, \\frac{1}{ \\tau_0 + n \\tau} \\right)$$\n\n  \n\n$$\\to  \\qquad \\theta \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\bar{y}, \n \\frac{1}{n \\tau} \\right)$$\n\n\n\n  \n\nPosterior Predictive\n$Y_{n+1} \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\bar{y}, \\sigma^2 (1 + \\frac{1}{n} )\\right)$\n\n\n \n## Comparing Estimators\n\nExpected loss (from frequentist perspective) of using Bayes Estimator\n\n  \n\n- Posterior mean is optimal under squared error loss (min Bayes Risk)  [also absolute error loss]\n\n  \n\nCompute Mean Square Error (or Expected Average Loss)\n$$\\textsf{E}_{\\bar{y} \\mid \\theta}\\left[\\left(\\hat{\\theta} - \\theta \\right)^2 \\mid \\theta \\right]$$\n $$ = \\textsf{Bias}(\\hat{\\theta})^2 + \\textsf{Var}(\\hat{\\theta})$$\n  \n\n- For the MLE $\\bar{Y}$ this is just the variance of $\\bar{Y}$ or $\\sigma^2/n$\n\n \n## MSE for Bayes\n\n$$\\textsf{E}_{\\bar{y} \\mid \\theta}\\left[\\left(\\hat{\\theta} - \\theta \\right)^2 \\mid \\theta \\right] = \\textsf{MSE} =  \\textsf{Bias}(\\hat{\\theta})^2 + \\textsf{Var}(\\hat{\\theta})$$\n\n- Bias of Bayes Estimate\n\n$$\\textsf{E}_{\\bar{Y} \\mid \\theta}\\left[ \\frac{\\tau_0 \\theta_0 + \\tau n \\bar{Y}}\n{\\tau_0  + \\tau n}\\right]  - \\theta = \n\\frac{\\tau_0(\\theta_0 - \\theta)}{\\tau_0 + \\tau n}$$\n\n  \n\n- Variance\n\n$$\\textsf{Var}\\left(\\frac{\\tau_0 \\theta_0 + \\tau n \\bar{Y}}{\\tau_0 + \\tau n} - \\theta  \\mid \\theta \\right)  = \\frac{\\tau n}{(\\tau_0 + \\tau n)^2}$$\n  \n\n(Frequentist) expected Loss  when truth is $\\theta$\n\n$$\\textsf{MSE} = \\frac{\\tau_0^2(\\theta - \\theta_0)^2 + \\tau n}{(\\tau_0 + \\tau n)^2}$$\n  \n\nBehavior ?\n\n \n## Plot\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-predictive-checks_files/figure-revealjs/MSE-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n\n\n \n## Exercise\n\n\n<div class=\"question\">\nRepeat this for estimating a future Y under squared error loss using a proper prior and Jeffreys' prior\n\n</div>\n\n$$\\textsf{E}_{Y_{n+1} \\mid \\theta}\\left[(Y_{n+1} - \\textsf{E}[Y_{n+1} \\mid y_1, \\ldots, n])^2]\\right]$$\n \n## Uses of Posterior Predictive\n\n- Plot the entire density or summarize\n\n  \n \n- Available analytically for conjugate families\n\n  \n\n- Monte Carlo Approximation\n\n$$p(y_{n+1} \\mid y_1, \\ldots y_n) \\approx \\frac 1 T \\sum_{t = 1}^T  p(y_{n+t} \\mid \\theta^{(t)})$$\n\nwhere $\\theta^{(t)} \\sim \\pi(\\theta \\mid y_1, \\ldots y_n)$ for $t = 1, \\ldots, T$ \n\n  \n\n- T samples from the posterior distribution\n\n  \n\n- Empirical Estimates & Quantiles from Monte Carlo Samples\n\n\n \n## Model Diagnostics\n\n- Need an accurate specification of likelihood function  (and reasonable prior)\n\n  \n\n- George Box:  _All models are wrong but some are useful_\n\n  \n\n- \"Useful\" $\\rightarrow$ model provides a good approximation; there aren't clear aspects of the data that are ignored or misspecified\n\n \n## Example\n\n$$Y_i \\sim \\textsf{Poisson}(\\theta) \\qquad i = 1, \\ldots, n$$\nHow might our model be misspecified?\n\n  \n\n- Poisson assumes that $\\textsf{E}(Y_i) = \\textsf{Var}(Y_i) = \\theta$\n\n  \n\n- it's _very_ common for data to be **over-dispersed** $\\textsf{E}(Y_i) <  \\textsf{Var}(Y_i)$\n\n  \n\n- **zero-inflation**  many more zero values than consistent with the poisson model \n\n  \n\n-  Can we use the Posterior Predictive to diagnose whether these are issues with our observed data?\n\n \n## Posterior Predictive  (PP) Checks\n\n- $y^{(n)}$ is observed & fixed training data\n\n  \n\n- $p(y_{n+1} \\mid y^{(n)})$ is PP distributoin\n\n  \n\n- $\\tilde{y}^{(n)}_t$ is $t^{\\text{th}}$ new dataset sampled from the PP of size $n$  (same as training)\n\n  \n\n- $p(\\tilde{y}^{(n)}_t \\mid y^{(n)})$ is PP of new data sets\n\n\n  \n\n- compare some feature of the observed data to the datasets simulated from the PP\n\n \n##  Formally\n\n- choose a \"test statistic\" $t(\\cdot)$  that captures some summary of the data, e.g. $\\textsf{Var}(y^{(n)})$ for over-dispersion\n\n  \n\n- $t(y^{(n)}) \\equiv t_{\\textrm{obs}}$ value of test statistic in observed data\n\n  \n\n- $t(\\tilde{y}^{(n)}) \\equiv t_{\\textrm{pred}}$ value of test statistic for a random dataset drawn from the posterior predictive \n\n  \n\n- plot posterior predictive distribution of $t(\\tilde{y}^{(n)})$\n\n  \n\n- add $t_{\\textrm{obs}}$ to plot\n\n  \n\n- How _extreme_ is $t_{\\textrm{obs}}$ compared to the distribution of $t(\\tilde{y}^{(n)})$\n\n \n##  Example Over Dispersion\n\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](04-predictive-checks_files/figure-revealjs/overdispersion-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n \n## Posterior Predictive p-values (PPPs)\n\n- p-value is probability of seeing something  as extreme or more so under a hypothetical  \"null\" model & are uniformally distributed under the \"null\" model\n\n  \n\n- PPPs advocated by Gelman & Rubin in papers and BDA are not **valid** p-values.  They are do not have a  uniform  distribution under the hypothesis that the model is correctly specified\n\n  \n\n- the PPPs tend to be concentrated around 0.5, tends not to reject  (conservative)\n\n  \n\n- theoretical reason for the incorrect distribution is due to double use of the data\n\n  \n\n**DO NOT USE as a formal test!**\nuse as a diagnostic plot to see how model might fall flat\n\n  \n\nBetter approach is to split the data use one piece to learn $\\theta$ and the other to calculate $t_{\\textrm{obs}}$  \n\n\n \n## Zero Inflated Distribution\n\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](04-predictive-checks_files/figure-revealjs/zero-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\n- Let the $t()$ be the proportion of zeros\n\n$$t(y) = \\frac{\\sum_{i = 1}^{n}1(y_i = 0)}{n}$$\n\n \n##  Posterior Predictive Distribution\n\n\n\n::: {.cell layout-align=\"center\" fig='true'}\n::: {.cell-output-display}\n![](04-predictive-checks_files/figure-revealjs/zeroPP-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n \n## Modeling Over-Dispersion\n\n- Original Model  $Y_i \\mid \\theta \\sim \\textsf{Poisson}(\\theta)$\n\n  \n\n- cause of overdispersion is variation in the rate\n\n  \n\n\n$$ Y_i \\mid \\theta \\sim \\textsf{Poisson}(\\theta_i)$$\n\n  \n\n$$\\theta_i \\sim \\pi_\\theta()$$\n\n  \n\n- $\\pi_\\theta()$ characterizes variation in the rate parameter across inviduals\n\n  \n\n- Simple Two Stage Hierarchical Model\n\n \n## Example\n\n$$\\theta_i \\sim \\textsf{Gamma}(\\phi \\mu, \\phi)$$\n  \n\n- Find pmf for $Y_i \\mid \\mu, \\phi$\n\n  \n\n- Find $\\textsf{E}[Y_i \\mid \\mu, \\phi]$ and $\\textsf{Var}[Y_i \\mid \\mu, \\phi]$ \n\n  \n\n- Homework: \n$$\\theta_i \\sim \\textsf{Gamma}(\\phi, \\phi/\\mu)$$\n           \n  \n\n- Can either of these model zero-inflation?\n\n\n\n\n  \n\n- See Bayarri & Berger (2000) for more discussion about why PPP should not be used as a test\n",
    "supporting": [
      "04-predictive-checks_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}