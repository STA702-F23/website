{
  "hash": "50a94044c147fa805092c3f3ac4eec06",
  "result": {
    "markdown": "---\nsubtitle: \"STA 702: Lecture 2\"\ntitle: \"Loss Functions, Bayes Risk and Posterior Summaries\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\n---\n\n\n## Last Time ...\n\n-   Introduction to \"ingredients\" of Bayesian analysis\n\n-   Illustrated a simple Beta-Binomial conjugate example\n\n-   Posterior $\\pi(\\theta \\mid y)$ is a\n    $\\textsf{Beta}(a + y, b + n - y )$\n\n. . .\n\nToday ...\n\n-   an introduction to loss functions\n\n-   Bayes Risk\n\n-   optimal decisions and estimators\n\n## Bayes estimate {.smaller}\n\n-   As we've seen by now, having posterior distributions instead of\n    one-number summaries is great for capturing uncertainty.\n\n-   That said, it is still very appealing to have simple summaries,\n    especially when dealing with clients or collaborators from other\n    fields, who desire one.\n\n    -  What if we want to produce a single \"best\" estimate of $\\theta$?\n\n    - What if we want to produce an interval estimate\n    $(\\theta_L, \\theta_U )$?\n\n. . .\n\nThese would provide alternatives to the frequentist MLEs and confidence\nintervals\n\n## Heuristically {.smaller}\n\n::: {.columns}\n\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-loss-functions_files/figure-revealjs/post-1.png){width=480}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"40%\"} \n\n- \"best\" estimate of $\\theta$ is the maximum [a posteriori estimate]{style=\"color:red\"}\n    (**MAP**) or posterior mode\n\n    -   _what do we really mean by \"best\"?_\n\n-  find an interval such that\n    $P(\\theta \\in ( \\theta_L, \\theta_U ) \\mid y) = 1- \\alpha$\n\n    -   _lots of intervals that satisfy this! which one is \"best\"?_\n\n:::\n::::\n\n## Loss Functions for Estimators {.smaller}\n\nIntroduce loss functions for decision making about what to report!\n\n\n-  a loss function provides a summary for how bad an estimator $\\hat{\\theta}$ is relative to the \"true\" value of $\\theta$\n\n- Squared error loss $(L2)$  \n$$l(\\theta, \\hat{\\theta}) = (\\hat{\\theta} - \\theta)^2$$\n\n- Absolute error loss $(L1)$\n$$l(\\theta, \\hat{\\theta}) = |\\hat{\\theta} - \\theta|$$\n\n. . .\n\nBut how do we deal with the fact that we do not know $\\theta$?\n\n## Bayes Risk {.smaller}\n\n\n- [**Bayes risk**]{style=\"color:red\"} is defined as the expected loss of using $\\hat{\\theta}$ averaging over the posterior distribution.\n$$ R(\\hat{\\theta}) = \\textsf{E}_{\\pi(\\theta \\mid y)} [l(\\theta, \\hat{\\theta}) ]$$\n\n\n- the [**Bayes optimal estimate**]{style=\"color:red\"} $\\hat{\\theta}$ is the estimator that has the lowest posterior expected loss or Bayes Risk\n\n\n\n\n- Depends on choice of loss function\n\n\n\n- [**Frequentist risk**]{style=\"color:red\"} also exists for evaluating a given estimator under true value of $\\theta$\n$$\\textsf{E}_{p(y \\mid \\theta_{\\textrm{true}})} [l(\\theta_{\\textrm{true}} , \\hat{\\theta}) )]$$\n\n## Squared Error Loss\n\nA common choice for point estimation is [squared error loss]{style=\"color:red\"}:\n \n $$R(\\hat{\\theta}) = \\textsf{E}_{\\pi(\\theta \\mid y)} [l(\\theta, \\hat{\\theta}) ] = \\int_\\Theta (\\hat{\\theta} - \\theta)^2 \\pi(\\theta \\mid y) \\, d\\theta$$\n\n. . .\n\n\n::::{.callout-note}\n### Let's work it out!\n\nExpand, take expectations of $R(\\hat{\\theta})$ with respect to $\\theta$ and factor as a quadratic to find the minimizer (or take derivatives)\n\n::::\n\n  \n\n## Steps  {.smaller}\n\n$$R(\\hat{\\theta}) = \\int_\\Theta (\\hat{\\theta}^2 - 2 \\hat{\\theta} \\theta + \\theta^2) \\pi(\\theta \\mid y) \\, d \\theta$$\n\n. . .\n\n$$R(\\hat{\\theta}) = \\hat{\\theta}^2 - 2 \\hat{\\theta} \\int_\\Theta \\theta \\pi(\\theta \\mid y) \\, d\\theta +  \\int_\\Theta \\theta^2  \\pi(\\theta \\mid y) \\, d\\theta$$\n\n. . .\n\n$$R(\\hat{\\theta}) = \\hat{\\theta}^2 - 2 \\hat{\\theta} \\textsf{E}[\\theta \\mid y] + \\textsf{E}[\\theta^2 \\mid y]$$\n\n. . .\n\n$$R(\\hat{\\theta}) = \\hat{\\theta}^2 - 2 \\hat{\\theta} \\textsf{E}[\\theta \\mid y] +  \\textsf{E}[\\theta \\mid y]^2 - \\textsf{E}[\\theta \\mid y]^2 +  \\textsf{E}[\\theta^2 \\mid y]$$\n\n. . . \n\n\nQuadratic in $\\hat{\\theta}$ minimized when $\\hat{\\theta} = \\textsf{E}[\\theta \\mid y]$  \n  $\\Rightarrow$ [posterior mean]{style=\"color:red\"} is the [**Bayes optimal estimator**]{style=\"color:red\"} for $\\theta$ under squared error loss\n\n\n\n- In the beta-binomial case for example, the optimal Bayes estimate under squared error loss is  $\\hat{\\theta} = \\frac{a+y}{a+b+n}$\n\n\n\n## What about other loss functions? {.smaller}\n\n- Clearly, squared error is only one possible loss function. An alternative is [**absolute loss**]{style=\"color:red\"}, which has\n$$l(\\theta, \\hat{\\theta})  = |\\theta - \\hat{\\theta}|$$\n\n\n\n\n- Absolute loss places less of a penalty on large deviations & the resulting Bayes estimate is the \n[**posterior median**]{style=\"color:red\"}.\n\n- Median is actually relatively easy to estimate.\n\n- Recall that for a continuous random variable $Y$ with cdf $F$, the median of the distribution is the value $z$, which satisfies\n$$F(z) = \\Pr(Y\\leq z) = \\dfrac{1}{2}= \\Pr(Y\\geq z) = 1-F(z)$$\n\n- As long as we know how to evaluate the CDF of the distribution we have, we can solve for $z$. \n\n\n\n\n## Beta-Binomial {.smaller}\n\n- For the beta-binomial model, the CDF of the beta posterior can be written as\n$$F(z) = \\Pr(\\theta\\leq z | y) = \\int^z_0 \\textrm{Beta}(\\theta| a+y, b+n-y) d\\theta.$$\n\n\n\n- Then, if $\\hat{\\theta}$ is the median, we have that $F(\\hat{\\theta}) = 0.5$\n\n\n- To solve for $\\hat{\\theta}$, apply the inverse CDF $$\\hat{\\theta} = F^{-1}(0.5)$$\n\n\n\n- In R, that's simply\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqbeta(0.5,a+y,b+n-y)\n```\n:::\n\n\n\n\n- For other distributions, switch out the beta.\n\n\n## Loss Functions in General \n\n- A [**loss function** ]{style=\"color:red\"} $l(\\theta, \\delta(y) )$ is a function of the  parameter $\\theta$ and  $\\delta(y)$ based on just the data $y$ \n\n\n\n- For example, $\\delta(y) = \\bar{y}$ can be the decision to use the sample mean to estimate $\\theta$, the true population mean.\n\n\n\n- $l(\\theta, \\delta(y) )$ determines the penalty for making the decision $\\delta(y)$, if $\\theta$ is the true parameter or state of nature; the loss function characterizes the price paid for errors.\n\n\n\n\n- Bayes optimal estimator or action is the estimator/action that minimizes the expected posterior loss marginalizing out any unknowns over posterior/predictive distribution.\n\n\n\n## MAP Estimator\n\n- What about the MAP estimator?  Is it an optimal Bayes estimator & under what  choice of loss function?\n\n\n\n- $L_\\infty$ loss: \n$$R_{\\infty}(\\hat{\\theta}) = \\lim_{p \\to \\infty} \\int_\\Theta (\\theta - \\hat{\\theta})^p \\pi(\\theta \\mid y) \\, d \\theta$$ \n\n\n\n\n\n- Essentially saying that we need the estimator to be right on the truth or the error blows up!\n\n\n\n- Is this a reasonable loss function?  \n\n\n## Interval Estimates  {.smaller}\n\nRecall that a frequentist confidence interval  $[l(y), \\ u(y)]$ has 95% frequentist coverage for a population parameter $\\theta$ if, before we collect the data,\n$$\\Pr[l(y) < \\theta < u(y) | \\theta] = 0.95.$$\n\n\n\n\n\n- This means that 95% of the time, our constructed interval will cover the true parameter, and 5% of the time it won't.\n\n\n\n- There is NOT a 95% chance your interval covers the true parameter once you have collected the data.\n\n\n\n\n- In any given sample, you don't know whether you're in the lucky 95% or the unlucky 5%.\nYou just know that either the interval covers the parameter, or it doesn't (useful, but not too helpful clearly). \n\n\n\n\n- Often based on aysmptotics i.e use a Wald or other type of frequentist asymptotic  interval  $\\hat{\\theta} \\pm 1.96 \\,\\text{se}(\\hat{\\theta})$\n\n\n## Bayesian Intervals  {.smaller}\n\n- We want a Bayesian alternative to confidence intervals\nfor some pre-specified value of $\\alpha$\n\n- An interval $[l(y), \\ u(y)]$ has $1 - \\alpha$ 100% Bayesian coverage for $\\theta$ if\n$$\\Pr(\\theta \\in [l(y), \\ u(y)] \\mid y) = 1 - \\alpha$$\n\n\n\n\n- This describes our information about where $\\theta$ lies _after_ we observe the data.\n\n\n\n- Fantastic!  This is actually the interpretation people want to give to the frequentist confidence interval.\n\n\n\n- Bayesian interval estimates are often generally called [**credible intervals** or **credible sets**]{style=\"color:red\"}.\n\n. . .\n\nHow to choose $[l(y), \\ u(y)]$?\n\n\n## Bayesian Equal Tail Interval {.smaller}\n\n- The easiest way to obtain a Bayesian interval estimate is to use posterior quantiles with [**equal tail areas**]{style=\"color:red\"}.   Often when researchers refer to a credible interval, this is what they mean.\n\n\n\n\n- To make a $100 \\times (1-\\alpha)%$ equi-tail quantile-based credible interval, find numbers (quantiles) $\\theta_{\\alpha/2} < \\theta_{1-\\alpha/2}$ such that\n\n  1. $\\Pr(\\theta < \\theta_{\\alpha/2} \\mid y) = \\dfrac{\\alpha}{2}$; and\n  \n  2. $\\Pr(\\theta > \\theta_{1-\\alpha/2} \\mid y) = \\dfrac{\\alpha}{2}$.\n  \n\n. . .\n\nConvenient conceptually and easy as we just take the $\\alpha/2$ and $1 - \\alpha/2$ quantiles of $\\pi(\\theta \\mid y)$ as $l(y)$ and $u(y)$, respectively.\n\n\n\n## Beta-Binomial Equal-tailed Interval\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-loss-functions_files/figure-revealjs/equal.tail-1.png){width=750%}\n:::\n:::\n\n\n95% Equal -Tail Area interval is $(0.02, 0.41)$\n\n## Monte Carlo Version {.smaller}\n\n- Suppose we don't have $\\pi(\\theta \\mid y)$ is a simple form, but we do have samples \n$\\theta_1, \\ldots, \\theta_T$ from $\\pi(\\theta \\mid y)$\n\n\n\n- We can use these samples to obtain Monte Carlo (MC) estimates of posterior summaries\n$$\\hat{\\theta} = \\textsf{E}[\\theta \\mid y] \\approx \\frac{1}{T} \\sum_{t= 1}^T \\theta_t$$\n\n\n\n\n\n- what about MC quantile estimates?\n\n\n\n\n- Find the 2.5th and 97.5th percentile from the empirical distribution\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta = rbeta(1000, a + y, b + n - y)\nquantile(theta, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      2.5%      97.5% \n0.02141993 0.39572970 \n```\n:::\n:::\n\n\n\n\n## Equal-Tail Interval {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-loss-functions_files/figure-revealjs/MC.equal.tail-1.png){width=75%}\n:::\n:::\n\n\n\n**Note**  there are values of $\\theta$ outside the quantile-based credible interval, with higher density than some values inside the interval. \n\n\n\n\n## HPD Region {.smaller}\n\n- A $100 \\times (1-\\alpha)%$ [**highest posterior density (HPD)**]{style=\"color:red\"} region is a subset $s(y)$ of the parameter space $\\Theta$ such that\n\n  1. $\\Pr(\\theta \\in s(y) \\mid y) = 1-\\alpha$; and\n  \n  2. If $\\theta_a \\in s(y)$ and $\\theta_b \\notin s(y)$, then $p(\\theta_a \\mid y) > p(\\theta_b \\mid y)$  (highest density set)\n\n\n\n- $\\Rightarrow$ **All** points in a HPD region have higher posterior density than points outside the region. \n\n\n\n- The basic idea is to gradually move a horizontal line down across the density, including in the HPD region all values of $\\theta$ with a density above the horizontal line.\n\n\n\n- Stop moving the line down when the posterior probability of the values of $\\theta$ in the region reaches $1-\\alpha$.\n\n\n##  Simulation Based using  the `coda` Package {.smaller}\n\n\n::: {.cell}\n\n:::\n\n\n:::: {.columns}\n\n::: {.column width=50%}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-loss-functions_files/figure-revealjs/HPD-MCMC-1.png){width=432}\n:::\n:::\n\n:::\n\n::: {.column width=50%}\n\n\n\n&nbsp;\n\n&nbsp;\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(coda)\nHPDinterval(as.mcmc(theta))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           lower     upper\nvar1 0.005930904 0.3669906\nattr(,\"Probability\")\n[1] 0.95\n```\n:::\n:::\n\n:::\n\n::::\n\n## Properties of HPD Sets {.smaller}\n\n- Shortest length interval (or volume) for the given coverage\n\n\n\n- Equivalent to Equal-Tail Intervals if the posterior is unimodal and symmetric\n\n\n\n- May not be an interval if the posterior distribution is multi-modal \n\n\n\n\n\n- In general, not invariant under monotonic transformations of $\\theta$. (Why?)\n\n\n\n- More computationally intensive to solve exactly!  \n\n. . .\n\n::::{.callout-note}\nSee \"The Bayesian Choice\"  by Christian Robert [Section 5.5.5 ](https://link-springer-com.proxy.lib.duke.edu/content/pdf/10.1007%2F0-387-71599-1.pdf)\n for more info on Loss Functions for Interval Estimation\n::::\n\n\n\n\n\n\n\n## Connections between Bayes and MLE Based Frequentist Inference {.smaller}\n\n[Berstein von Mises (BvM) Theorems]{style=\"color:red\"}) aka Bayesian Central Limit Theorems\n\n\n\n-   examine limiting form of the posterior distribution $\\pi(\\theta \\mid y)$ as $n \\to \\infty$\n\n\n\n- $\\pi(\\theta \\mid y)$ goes to a Gaussian  under regularity conditions \n\n\n\n   - centered at the MLE\n\n\n\n   - variance given by the inverse of the Expected Fisher Information  (var of MLE)\n\n\n\n- The most important implication of the BvM is that Bayesian inference is asymptotically correct from a frequentist point of view\n\n\n\n\n-  Used to justify Normal Approximations to the posterior distribution (eg Laplace approximations)\n\n\n\n\n## Model Misspecification ?  {.smaller}\n\n-  We might have chosen a bad sampling model/likelihood\n\n\n\n-  posterior still converges to a Gaussian  centered at the MLE under the misspecified model, but wrong variance\n\n\n\n- 95% Bayesian credible sets do not have correct frequentist coverage\n\n\n\n- See [Klein & van der Vaart](https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-6/issue-none/The-Bernstein-Von-Mises-theorem-under-misspecification/10.1214/12-EJS675.full) for more rigorous treatment if interested\n\n\n\n- parametric model is \"close\" to the true data-generating process\n\n\n\n- model diagnostics & changing the model can reduce the gap between model we are using and the true data generating process\n\n\n\n\n",
    "supporting": [
      "02-loss-functions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}