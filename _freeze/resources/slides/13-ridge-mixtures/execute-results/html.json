{
  "hash": "86dcf6d729276312400650266ff8247e",
  "result": {
    "markdown": "---\ntitle: \"Lecture 13:  Ridge Regression, Lasso and Mixture Priors\"\nsubtitle: \"STA702\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n## Ridge Regression {.smaller}\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\text{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\nModel: $\\Y = \\one_n \\alpha + \\X \\b + \\eps$\n\n-   typically expect the intercept $\\alpha$ to be a different order of\n    magnitude from the other predictors. Adopt a two block prior with\n    $p(\\alpha) \\propto 1$\n\n-   Prior $\\b \\mid \\phi \\sim \\N(\\zero_b, \\frac{1} {\\phi \\kappa} \\I_p$)\n    implies the $\\b$ are exchangable *a priori* (i.e. distribution is\n    invariant under permuting the labels and with a common scale and\n    mean)\n\n-   Posterior for $\\b$ $$\\b \\mid \\phi, \\kappa, \\Y \\sim \n    \\textsf{N}\\left((\\kappa I_p + X^TX)^{-1} X^T Y,  \\frac{1}{\\phi}(\\kappa I_p + X^TX)^{-1}\n    \\right)$$\n\n-   assume that $\\X$ has been centered and scaled so that\n    $\\X^T\\X = \\corr(\\X)$ and $\\one_n^T \\X = \\zero_p$\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX = scale(X)/sqrt{nrow(X) - 1}\n```\n:::\n\n\n## Bayes Ridge Regression {.smaller}\n\n-   related to penalized maximum likelihood estimation\n    $$-\\frac{\\phi}{2}\\left(\\|\\Y - \\X \\b\\|^2 + \\kappa \\| \\b \\|^2 \\right)\n    $$\n\n-   frequentist's expected mean squared error loss for using $\\bv_n$\n    $$\\E_{\\Y \\mid \\b_*}[\\| \\bv_n - \\b_* \\|^2] = \\sigma^2 \\sum_{j = 1}^2 \\frac{\\lambda_j}{(\\lambda_j + \\kappa)^2} +\n    \\kappa^2 \\b_*^T(\\X^T\\X + \\kappa \\I_p)^{-2} \\b_*$$\n\n-   eigenvalues of $\\X^T\\X = \\V \\Lambdab \\V^T$ with\n    $[\\Lambdab]_{jj} = \\lambda_j$\n\n-   can show that there **always** is a value of $\\kappa$ where is\n    smaller for the (Bayes) Ridge estimator than MLE\n\n-   Unfortunately the optimal choice depends on \"true\" $\\b_*$!\n\n-   orthogonal $\\X$ leads to James-Stein solution related to Empirical\n    Bayes\n\n## Choice of $\\kappa$? {.smaller}\n\n-   fixed *a priori* Bayes (and how to choose?)\\\n\n-   Cross-validation (frequentist)\n\n-   Empirical Bayes? (frequentist/Bayes)\n\n-   Should there be a common $\\kappa$? (same shrinkage across all\n    variables?)\n\n-   Or a $\\kappa_j$ per variable? (or shared among a group of variables\n    (eg. factors) ?)\n\n-   Treat as unknown!\n\n## Mixture of Conjugate Priors {.smaller}\n\n-   can place a prior on $\\kappa$ or $\\kappa_j$ for fully Bayes\n\n-   similar option for $g$ in the $g$ priors\n\n-   often improved robustness over fixed choices of hyperparameter\n\n-   may not have cloosed form posterior but sampling is still often\n    easy!\n\n-   Examples:\n\n    -   Bayesian Lasso (Park & Casella, Hans)\n    -   Generalized Double Pareto (Armagan, Dunson & Lee)\n    -   Horseshoe (Carvalho, Polson & Scott)\n    -   Normal-Exponential-Gamma (Griffen & Brown)\n    -   mixtures of $g$-priors (Liang et al)\n\n## Lasso {.smaller}\n\nTibshirani (JRSS B 1996) proposed estimating coefficients through $L_1$\nconstrained least squares \\`\\`Least Absolute Shrinkage and Selection\nOperator''\n\n-   Control how large coefficients may grow\n    \\begin{align}  \\min_{\\b} & \\| \\Y  - \\one_n \\alpha - \\X \\b \\|^2 \\\\\n      & \\textsf{  subject to }    \\sum |\\beta_j| \\le t\n    \\end{align}\n\n-   Equivalent Quadratic Programming Problem for \\`\\`penalized''\n    Likelihood\n    $$\\min_{\\b} \\| \\Y - \\one_n \\alpha - \\X\\b\\|^2 + \\lambda \\|\\b\\|_1$$\n\n-   Equivalent to finding posterior mode $$\n    \\max_{\\b} -\\frac{\\phi}{2} \\{ \\| \\Y - \\one_n \\alpha - \\X \\b\\|^2 + \\lambda \\|\\b\\|_1 \\}\n    $$\n\n## Bayesian Lasso {.smaller}\n\nPark & Casella (JASA 2008) and Hans (Biometrika 2010) propose Bayesian\nversions of the Lasso\\\n\\begin{eqnarray*}\n    \\Y \\mid \\alpha, \\b, \\phi & \\sim & \\N(\\one_n \\alpha + \\X \\b, \\I_n/\\phi)  \\\\\n    \\b \\mid \\alpha, \\phi, \\taub & \\sim & \\N(\\zero, \\diag(\\taub^2)/\\phi)  \\\\\n    \\tau_1^2 \\ldots, \\tau_p^2 \\mid \\alpha, \\phi & \\iid & \\Ex(\\lambda^2/2)  \\\\\n    p(\\alpha, \\phi) & \\propto& 1/\\phi  \\\\\n  \\end{eqnarray*}\n\n-   Can show that\n    $\\beta_j \\mid \\phi, \\lambda \\iid DE(\\lambda \\sqrt{\\phi})$\n    $$\\int_0^\\infty \\frac{1}{\\sqrt{2 \\pi s}}\n    e^{-\\frac{1}{2} \\phi \\frac{\\beta^2}{s }}\n    \\, \\frac{\\lambda^2}{2} e^{- \\frac{\\lambda^2 s}{2}}\\, ds =\n    \\frac{\\lambda \\phi^{1/2}}{2} e^{-\\lambda \\phi^{1/2} |\\beta|}\n    $$\n\n-   equivalent to penalized regression with\n    $\\lambda^* = \\lambda/\\phi^{1/2}$\n\n-   Scale Mixture of Normals (Andrews and Mallows 1974)\n\n## Gibbs Sampling {.smaller}\n\n-   Integrate out $\\alpha$:\n    $\\alpha \\mid \\Y, \\phi \\sim \\N(\\ybar,  1/(n \\phi)$\\\n\n-   $\\b \\mid \\taub, \\phi, \\lambda, \\Y \\sim \\N(, )$\\\n\n-   $\\phi \\mid \\taub, \\b, \\lambda, \\Y \\sim \\G( , )$\\\n\n-   $1/\\tau_j^2 \\mid \\b, \\phi, \\lambda, \\Y \\sim \\textsf{InvGaussian}( , )$\n\n-   For $X \\sim \\textsf{InvGaussian}(\\mu, \\lambda)$, the density is $$\n    f(x) =  \\sqrt{\\frac{\\lambda^2}{2 \\pi}}  x^{-3/2} e^{- \\frac{1}{2} \\frac{\n      \\lambda^2( x - \\mu)^2} {\\mu^2 x}} \\qquad x > 0\n    $$\n\n. . .\n\n::: callout-warning\n## Homework\n\nDerive the full conditionals for $\\b$, $\\phi$, $1/\\tau^2$ for the model\nin [Park & Casella](http://www.stat.ufl.edu/~casella/Papers/Lasso.pdf)\n:::\n\n## Choice of Estimator {.smaller}\n\n-   Posterior mode (like in the LASSO) may set some coefficients exactly\n    to zero leading to variable selection - optimization problem\n    (quadratic programming)\n\n-   Posterior distribution for $\\beta_j$ does not assign any probability\n    to $\\beta_j = 0$ so posterior mean results in no selection, but\n    shrinkage of coeffiecients to prior mean of zero\n\n-   In both cases, large coefficients may be over-shrunk (true for LASSO\n    too)!\n\n-   Issue is that the tails of the prior under the double exponential\n    are not heavier than the normal likelihood\n\n-   Only one parameter $\\lambda$ that controls shrinkage and selection\n    (with the mode)\n\n-   Need priors with heavier tails than the normal!!!\n\n## Shrinkage Comparison with Posterior Mean {.smaller}\n\n::: columns\n\n\n::: {.column width=\"50%\"}\n![](lasso-HS-shrinakge.png)\n:::\n\n::: {.column width=\"50%\"}\nHS - Horseshoe of Carvalho, Polson & Scott (slight difference in CPS  notation) \n\n\\begin{align}\n\\b \\mid \\phi, \\taub & \\sim \\N(\\zero_p, \\frac{\\diag(\\taub^2)}{ \\phi    }) \\\\\n\\tau_j \\mid \\lambda & \\iid \\Ca^+(0, \\lambda^2) \\\\\n\\lambda & \\sim \\Ca^+(0, 1) \\\\\np(\\alpha, \\phi) & \\propto 1/\\phi)\n\\end{align}\n\n- resulting prior on $\\b$ has heavy tails like a Cauchy! \n\n\n:::\n:::\n\n## Bounded Influence for Mean {.smaller}\n\n-  canonical representation (normal means problem) $\\Y =\n\\I_p \\b + \\eps$  so $\\hat{\\beta}_i = y_i$\n$$\nE[\\beta_i \\mid \\Y] = \\int_0^1 (1 - \\psi_i) y^*_i p(\\psi_i \\mid \\Y)\\ d\\psi_i = (1 - \\E[\\psi_i \\mid y^*_i]) y^*_i$$\n\n\n- $\\psi_i = 1/(1 + \\tau_i^2)$ shrinkage factor \n\n- Posterior mean\n$E[\\beta \\mid y] = y + \\frac{d} {d y} \\log m(y)$\nwhere $m(y)$ is the predictive density under the prior (known $\\lambda$) \n \n- Bounded Influence: if $\\lim_{|y| \\to \\infty} \\frac{d}{dy} \\log m(y) = c$ (for some constant $c$)\n\n- HS has bounded influence where $c = 0$ so\n$$\\lim_{|y| \\to \\infty} E[\\beta \\mid y) \\to y $$\n- DE has bounded influence but $(c \\ne 0$); bound does not decay to zero and bias for large  $|y_i|$\n\n\n## Properties for Shrinkage and Selection {.smaller}\n\nFan & Li (JASA 2001) discuss Variable\nSelection via Nonconcave Penalties and Oracle Properties \n\n- Model $Y = \\one_n \\alpha + \\X \\b + \\eps$ with $\\X^T\\X = \\I_p$  (orthonormal) and $\\eps \\sim N(0, \\I_n)$\n- Penalized Log Likelihood \n$$\\frac 1 2 \\|\\Y - \\hat{\\Y}\\|^2 +\\frac 1 2 \\sum_j(\\beta_j - \\hat{\\beta}_j)^2 +  \\sum_j \\text{ pen}_\\lambda(|\\beta_j|)$$ \n- duality $\\text{pen}_\\lambda(|\\beta|) \\equiv  - \\log(p(|\\beta_j|))$ (negative log prior)\n\n- Objectives:\n  - Unbiasedness: for large $|\\beta_j|$ \n  - Sparsity: thresholding rule sets small coefficients to 0 \n  - Continuity:  continuous in $\\hat{\\beta}_j$\n  \n##  Conditions on Prior/Penalty {.smaller}\n\n\nDerivative of  $\\frac 1 2 \\sum_j(\\beta_j - \\hat{\\beta}_j)^2 +  \\sum_j \\text{pen}_\\lambda(|\\beta_j|)$\nis $\\sgn(\\beta_j)\\left\\{|\\beta_j| + \\text{pen}^\\prime_\\lambda(|\\beta_j|) \\right\\} - \\hat{\\beta}_j$\n\n. . .\n\n- Conditions:\n\n  + unbiased: if $\\text{pen}^\\prime_\\lambda(|\\beta|) = 0$ for large $|\\beta|$; estimator is $\\hat{\\beta}_j$\n  + thresholding: $\\min \\left\\{ |\\beta_j| + \\text{pen}^\\prime_\\lambda(|\\beta_j|)\\right\\} > 0$ then estimator is 0 if \n$|\\hat{\\beta}_j| < \\min \\left\\{ |\\beta_j| + \\text{pen}^\\prime_\\lambda(|\\beta_j|) \\right\\}$\n\n   + continuity:  minimum of $|\\beta_j| + \\text{pen}^\\prime_\\lambda(|\\beta_j|)$ is at zero\n\n- Can show that LASSO/ Bayesian Lasso fails conditions for unbiasedness\n\n- What about other Bayes methods?\n\n. . .\n\n::: {.callout-warning} \n## Homework\nCheck the conditions for the DE, Generalized Double Pareto and Cauchy priors\n:::\n\n\n## Selection {.smaller}\n\n- Only get variable selection if we use the posterior mode\n\n- If selection is a goal of analysis build it into the model/analysis/post-analysis\n\n    + prior belief that coefficient is zero\n    \n    + selection solved as a post-analysis decision problem\n\n- Even if selection is not  an objective, account for the uncertainty that some predictors may be unrelated  \n    \n    ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}