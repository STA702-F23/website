{
  "hash": "efe6f719cb56d1d033c69890e525b712",
  "result": {
    "markdown": "---\ntitle: \"Welcome to STA 702\"\nsubtitle: \"Course Overview\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: true\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## What is this course about?\n\n-   Learn the foundations and theory of Bayesian inference in the context of several models.\n\n-   Use Bayesian models to answer inferential questions.\n\n-   Apply the models to several different problems.\n\n-   Understand the advantages/disadvantages of Bayesian methods vs classical methods\n\n. . .\n\n<i class=\"fa fa-quote-left fa-2x fa-pull-left fa-border\" aria-hidden=\"true\"></i>\n\n<i class=\"fa fa-quote-right fa-2x fa-pull-right fa-border\" aria-hidden=\"true\"></i> A Bayesian version will usually make things better...\n\n-- Andrew Gelman.\n\n## Instructional Team\n\n*Instructor:* [Dr Merlise Clyde](https://www2.stat.duke.edu/~clyde)\n\n<i class=\"fa fa-envelope\"></i>   [clyde\\@duke.edu](mailto:clyde@duke.edu) <br> <i class=\"fa fa-home\"></i>   223 Old Chemistry <br> <i class=\"fa fa-home\"></i>   <https://www2.stat.duke.edu/~clyde> <br>\n\n. . .\n\n \n\n*Teaching Assistant:* [Rick Presman](https://scholars.duke.edu/person/rick.presman)\n\n<i class=\"fa fa-envelope\"></i>   [rick.presman\\@duke.edu](mailto:rick.presman@duke.edu)\n\n \n\n<i class=\"fa fa-calendar\"></i>   See course website for Office Hours, Policies and more!\n\n## Prerequisites\n\n-   random variables, common families of probability distribution functions and expectations\n-   conditional distributions\n-   transformations of random variables and change of variables\n-   principles of statistical inference (likelihoods)\n-   sampling distributions and hypothesis testing\n-   concepts of convergence\n\n. . .\n\nReview Chapters 1 to 5 of the [Casella and Berger book](https://mybiostats.files.wordpress.com/2015/03/casella-berger.pdf)\n\n## Computing\n\n-   Labs/HW will involve computing in R!\n\n-   Write your own MCMC samplers and run code long enough to show convergence\n\n-   You can learn `R` on the fly\n\n    -   see [Resources Tab on website](/resources.html)\n    -   materials from 2023 Bootcamp/Orientation\n\n## Grading Policies {.smaller}\n\n-   5% class\n\n-   20% HW\n\n-   10% Lab\n\n-   20% Midterm I\n\n-   20% Midterm II\n\n-   25% Final\n\n-   No Late Submissions for HW/Lab; Drop the lowest score\n\n-   You are encouraged to discuss assignments, but copying others work is considered a misconduct violation and will result in a 0 on the assignment\n\n-   Confirm that you have access to Sakai, Gradescope, and GitHub\n\n## Course structure and policies\n\n-   See the [Syllabus](/syllabus.html)\n\n-   Make use of the teaching team's office hours, we're here to help!\n\n-   Do not hesitate to come to my office hours or you can also make an appointment to discuss a homework problem or any aspect of the course.\n\n-   Please make sure to check your email daily for announcements\n\n-   Use the [<i class=\"fa fa-github\"></i> Reporting an issue](https://github.com/sta702-F23/website/issues/new) link to report broken links or missing content\n\n## Important Dates {.smaller}\n\n|                         |                                          |\n|-------------------------|------------------------------------------|\n| Tues, Aug 29            | Classes begin                            |\n| Fri, Sept 8             | Drop/Add ends                            |\n| Friday, Oct 13          | Midterm I (*tentative*)                  |\n| Sat - Tues, Oct 14 - 17 | Fall Break                               |\n| Tues, Nov 20            | Midterm II (*tentative*)                 |\n| Friday, Dec 1           | Graduate Classes End                     |\n| Dec 2 - Dec 12          | Graduate Reading Period                  |\n| **Sat, Dec 16**         | **Final Exam** (Perkins 060 2:00-5:00pm) |\n\n. . .\n\nSee [Class Schedule](../../schedule.html) for slides, readings, HW, Labs, etc\n\n## Topics {.smaller}\n\n-   Basics of Bayesian Models\n-   Loss Functions, Inference and Decision Making\n-   Predictive Distributions\n-   Predictive Distributions and Model Checking\n-   Bayesian Hypothesis Testing\n-   Multiple Testing\n-   MCMC (Gibbs & Metropolis Hastings Algorithms)\n-   Model Uncertainty/Model Choice\n-   Bayesian Generalized Linear Models\n-   Hiearchical Modeling and Random Effects\n-   Hamiltonian Monte Carlo\n-   Nonparametric Bayes Regression\n\n# Bayes Rules! Getting Started!\n\n## Basics of Bayesian inference\n\nGenerally (unless otherwise stated), in this course, we will use the following notation. Let\n\n-   $Y$ is a random variable from some probability distribution $p(y \\mid \\theta)$\n\n-   $\\mathcal{Y}$ be the [sample space]{style=\"color:red\"} (possible outcomes for $Y$)\n\n-   $y$ is the [observed data]{style=\"color:red\"}\n\n-   $\\theta$ is the unknown [parameter of interest]{style=\"color:red\"}\n\n-   $\\Theta$ be the [parameter space]{style=\"color:red\"}\n\n-   e.g. $Y \\sim \\textsf{Ber}(\\theta)$ where $\\theta = \\Pr(Y = 1)$\n\n## Frequentist inference\n\n-   Given data $y$, how would we estimate the population parameter $\\theta$?\n\n    -   Maximum likelihood estimate (MLE)\n\n    -   Method of moments\n\n    -   and so on...\n\n-   Frequentist MLE finds the one value of $\\theta$ that maximizes the likelihood\n\n-   Typically uses large sample (asymptotic) theory to obtain confidence intervals and do hypothesis testing.\n\n## What are Bayesian methods?\n\n-   [Bayesian methods]{style=\"color:red\"} are data analysis tools derived from the principles of Bayesian inference and provide\n\n    -   parameter estimates with good statistical properties;\n\n    -   parsimonious descriptions of observed data;\n\n    -   predictions for missing data and forecasts of future data with full uncertainty quantification; and\n\n    -   a computational framework for model estimation, selection, decision making and validation.\n\n    -   builds on likelihood inference\n\n## Bayes' theorem {.smaller}\n\n-   Let's take a step back and quickly review the basic form of Bayes' theorem.\n\n-   Suppose there are some events $A$ and B having probabilities $\\Pr(A)$ and $\\Pr(B)$.\n\n-   Bayes' rule gives the relationship between the marginal probabilities of A and B and the conditional probabilities.\n\n-   In particular, the basic form of [Bayes' rule]{style=\"color:red\"} or [Bayes' theorem]{style=\"color:red\"} is $$\\Pr(A | B) = \\frac{\\Pr(A \\ \\textrm{and} \\ B)}{\\Pr(B)} = \\frac{\\Pr(B|A)\\Pr(A)}{\\Pr(B)}$$\n\n-   $\\Pr(A)$ = marginal probability of event $A$, $\\Pr(B | A)$ = conditional probability of event $B$ given event $A$, and so on.\n\n-   \"reverses the conditioning\" e.g. Probability of Covid given a negative test versus probability of a negative test given Covid\n\n## Bayes' Rule more generally {.smaller}\n\n1.  For each $\\theta \\in \\Theta$, specify a [prior distribution]{style=\"color:red\"} $p(\\theta)$ or $\\pi(\\theta)$, describing our beliefs about $\\theta$ being the true population parameter.\n\n2.  For each $\\theta \\in \\Theta$ and $y \\in \\mathcal{Y}$, specify a [sampling distribution]{style=\"color:red\"} $p(y|\\theta)$, describing our belief that the data we see $y$ is the outcome of a study with true parameter $\\theta$. <br> [Likelihood]{style=\"color:red\"} $L(\\theta|y)$ proportional to $p(y|\\theta)$\n\n3.  After observing the data $y$, for each $\\theta \\in \\Theta$, update the prior distribution to a [posterior distribution]{style=\"color:red\"} $p(\\theta | y)$ or $\\pi(\\theta | y)$, describing our \"updated\" belief about $\\theta$ being the true population parameter.\n\n. . .\n\nGetting from Step 1 to 3? [Bayes' rule]{style=\"color:red\"}!\n\n$$p(\\theta | y) = \\frac{p(\\theta)p(y|\\theta)}{\\int_{\\Theta}p(\\tilde{\\theta})p(y| \\tilde{\\theta}) \\textrm{d}\\tilde{\\theta}} = \\frac{p(\\theta)p(y|\\theta)}{p(y)}$$ where $p(y)$ obtained by [Law of Total Probability]{style=\"color:red\"}\n\n## Notes on prior distributions\n\nMany types of priors may be of interest. These may\n\n-   represent our own beliefs;\n\n-   represent beliefs of a variety of people with differing prior opinions; or\n\n-   assign probability more or less evenly over a large region of the parameter space\n\n-   designed to provide good frequentist behavior when little is known\n\n## Notes on prior distributions\n\n-   [Subjective Bayes]{style=\"color:red\"}: a prior should accurately quantify some individual's beliefs about $\\theta$\n\n-   [Objective Bayes]{style=\"color:red\"}: the prior should be chosen to produce a procedure with \"good\" operating characteristics without including subjective prior knowledge\n\n-   [Weakly informative]{style=\"color:red\"}: prior centered in a plausible region but not overly-informative, as there is a tendency to be over confident about one's beliefs\n\n-   [Empirical Bayes]{style=\"color:red\"}: uses the data to estimate the prior, then pretends it was known\n\n-   [Practical Bayes]{style=\"color:red\"}: Combination\n\n## Notes on prior distributions {.smaller}\n\n-   The prior quantifies 'your' initial uncertainty in $\\theta$ before you observe new data (new information) - this may be necessarily subjective & summarizes experience in a field or prior research.\n\n-   Even if the prior is not \"perfect\", placing higher probability in a ballpark of the truth leads to better performance.\n\n-   Hence, it is very seldom the case that a weakly informative prior is not preferred over no prior. (Model selection is one case where one needs to be careful!)\n\n-   One (very important) role of the prior is to stabilize estimates (shrinkage) in the presence of limited data.\n\n## Next Steps\n\nWork on [Lab 0](/schedule.html)\n\nFinally, here are some readings to entertain you. Make sure to glance through them within the next week. See [Course Resources](/resources.html)\n\n1.  Efron, B., 1986. Why isn't everyone a Bayesian?. The American Statistician, 40(1), pp. 1-5.\n\n2.  Gelman, A., 2008. Objections to Bayesian statistics. Bayesian Analysis, 3(3), pp. 445-449.\n\n3.  Diaconis, P., 1977. Finite forms of de Finetti's theorem on exchangeability. Synthese, 36(2), pp. 271-281.\n\n4.  Gelman, A., Meng, X. L. and Stern, H., 1996. Posterior predictive assessment of model fitness via realized discrepancies. Statistica sinica, pp. 733-760. 5. Dunson, D. B., 2018. Statistics in the big data era: Failures of the machine. Statistics & Probability Letters, 136, pp. 4-9.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}