{
  "hash": "43318693f6b86baea3b31eaac24836fd",
  "result": {
    "markdown": "---\ntitle: \"Lecture 11: Conjugate Priors and Bayesian Regression\"\nsubtitle: \"STA702\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n## Semi-Conjugate Priors in Linear Regression  {.smaller}\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\text{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- Regression Model (Sampling model)\n$$\\Y \\mid \\b, \\phi  \\sim \\N(\\X \\b, \\phi^{-1} \\I_n) \n$$\n\n- Semi-Conjugate Prior Independent Normal Gamma\n$$\\begin{align*}\n\\b & \\sim \\N(\\bv_0, \\Phib_0^{-1}) \\\\\n\\phi & \\sim \\Gam(\\nu_0/2, \\SS_0/2)\n\\end{align*}$$\n\n    + Conditional Normal for $\\b \\mid \\phi, \\Y$  and \n    + Conditional Gamma $\\phi \\mid \\Y, \\b$ \n    + requires Gibbs sampling or other Metropolis-Hastings algorithms\n    \n## Conjugate Priors in Linear Regression    {.smaller}\n\n- Regression Model (Sampling model)\n$$\\Y \\mid \\b, \\phi  \\sim \\N(\\X \\b, \\phi^{-1} \\I_n) \n$$\n- Conjugate Normal-Gamma Model: factor joint prior $p(\\b, \\phi ) = p(\\b \\mid \\phi)p(\\phi)$ \n$$\\begin{align*}\n\\b \\mid \\phi & \\sim \\N(\\bv_0, \\phi^{-1}\\Phib_0^{-1}) & p(\\b \\mid \\phi) & = \\frac{|\\phi \\Phib_0|^{1/2}}{(2 \\pi)^{p/2}}e^{\\left\\{- \\frac{\\phi}{2}(\\b - \\bv_0)^T \\Phib_0 (\\b - \\bv_0)  \\right\\}}\\\\\n\\phi & \\sim \\Gam(v_0/2, \\SS_0/2)  & p(\\phi) & = \\frac{1}{\\Gamma{(\\nu_0/2)}}\n\\left(\\frac{\\SS_0}{2} \\right)^{\\nu_0/2}\n\\phi^{\\nu_0/2 - 1}\n e^{- \\phi \\frac{\\SS_0}{2}}\\\\\n\\Rightarrow (\\b, \\phi) & \\sim \\NG(\\bv_0, \\Phib_0, \\nu_o, \\SS_0)\n\\end{align*}$$\n\n- Normal-Gamma distribution indexed by 4 hyperparameters\n- Note Prior  Covariance for $\\b$ is scaled by $\\sigma^2 = 1/\\phi$ \n\n\n\n##  Finding the Posterior Distribution {.smaller} \n- Likelihood: $\\cL(\\beta, \\phi) \\propto \\phi^{n/2} e^{- \\frac{\\phi}{2} (\\Y - \\X\\b)^T(\\Y - \\X \\b)}$  \n\\begin{eqnarray*}\n p(\\b, \\phi \\mid \\Y) &\\propto&  \\phi^{\\frac {n}{2}}\n e^{- \\frac \\phi 2  (\\Y - \\X\\b)^T(\\Y - \\X \\b) } \\times \\\\\n & & \\phi^{\\frac{\\nu_0}{2} - 1} e^{- \\phi \\frac{\\SS_0}{2} }\\times \n \\phi^{\\frac{p}{2}} e^{- \\frac{\\phi}{2} (\\b - \\bv_0)^T \\Phib_0 (\\b - \\bv_0) }\n\\end{eqnarray*}  \n\n- Quadratic in Exponential \n$$\\exp\\left\\{- \\frac{\\phi}{2} (\\b - \\bv)^T \\Phib (\\b - \\bv) \\right\\} = \\exp\\left\\{-\n  \\frac{\\phi}{2} (\\b^T \\Phib \\b - 2 \\b^T \\Phib \\bv + \\bv^T\\Phib \\bv )\\right\\}$$\n \n\n  - Expand quadratics and regroup terms  \n  - Read off posterior precision from Quadratic in $\\b$  \n  - Read off posterior mean from Linear term in $\\b$  \n  - will need to complete the quadratic in the posterior mean due to $\\phi$\n\n## Expand and Regroup  {.smaller}\n\\begin{eqnarray*}\n p(\\b, \\phi \\mid \\Y) &\\propto&  \\phi^{\\frac {n}{2}}\n e^{- \\frac \\phi 2 ( \\Y^T\\Y - 2 \\b^T \\X^T \\Y  + \\b^T \\X^T \\X \\b)} \\times \\\\\n & & \\phi^{\\frac{\\nu_0}{2} - 1} e^{- \\phi \\frac{\\SS_0}{2} }\\times \n \\phi^{\\frac{p}{2}} e^{- \\frac{\\phi}{2} (\\b\\Phib_0\\b  - 2 \\b^T \\Phib_0 \\bv  + \\bv_0^T \\Phib_0 \\bv_0) }\n\\end{eqnarray*} \n\n. . .\n\n\\begin{eqnarray*}\n p(\\b, \\phi \\mid \\Y) &\\propto&  \\phi^{\\frac {n + p + \\nu_0}{ 2} - 1}\n e^{- \\frac \\phi 2 (\\SS_0 +  \\Y^T\\Y + \\bv_0^T \\Phib_0 \\bv_0) } \\times  \\\\\n & & e^{-\\frac{\\phi}{2} (\\b^T(\\X^T\\X)\\b -2 \\b^T\\textcolor{red}{\\X^T\\X (\\X^T\\X)^{-1}}\\X^T\\Y + \\b\\Phib_0\\b  - 2 \\b^T \\Phib_0 \\bv  ) }  \n\\end{eqnarray*}\n\n. . .\n\n\\begin{eqnarray*} \n & = & \\phi^{\\frac {n + p + \\nu_0}{ 2} - 1}\n e^{- \\frac \\phi 2 (\\SS_0 +  \\Y^T\\Y + \\bv_0^T \\Phib_0 \\bv_0)} \\times  \\\\\n& &  e^{ -\\frac{\\phi}{2} \\left(  \\b^T (\\X^T\\X + \\Phib_0) \\b  \\right) } \\times  \\\\\n& &  e^{  -\\frac{\\phi}{2} \\left( -2 \\b^T (\\X^T\\X \\textcolor{red}{\\bhat}  + \\Phib_0 \\bv_0)\n   \\right)} \n\\end{eqnarray*}\n\n## Complete the Quadratic {.smaller}\n\n\n  \\begin{eqnarray*}\n p(\\b, \\phi \\mid \\Y) &\\propto&  \\phi^{\\frac {n + p + \\nu_0}{ 2} - 1}\n e^{- \\frac \\phi 2 (\\SS_0 +  \\Y^T\\Y+ \\bv_0^T \\Phib_0 \\bv_0\n  )}     \\times \\\\\n& &  e^{ -\\frac{\\phi}{2} \\left(  \\b^T \\textcolor{\\red}{(\\X^T\\X + \\Phib_0)} \\b\n  \\right) }  \\times \\qquad \\qquad  \\qquad \\qquad \\Phib_n \\equiv \\X^T\\X + \\Phib_0  \\\\\n& &  e^{  -\\frac{\\phi}{2} \\left( -2 \\b^T  \\textcolor{red}{\\Phib_n \\Phib_n^{-1}} (\\X^T\\X \\bhat  + \\Phib_0 \\bv_0)\n   \\right)}  \\times   \\qquad \\qquad  \\bv_n \\equiv \\Phib_n^{-1} (\\X^T\\X \\bhat  + \\Phib_0 \\bv_0) \\\\\n& &  e^{ -\\frac{\\phi}{2} ( \\textcolor{red}{\\bv_ n^T \\Phib_n \\bv_n - \\bv_n^T \\Phib_n \\bv_n}) }  \n \\end{eqnarray*}\n \n . . .\n \n \\begin{eqnarray*} \n& = &\n  \\phi^{\\frac {n +  \\nu_0}{ 2} - 1}\n e^{- \\frac \\phi 2 ( \\SS_0   + \\Y^T\\Y+ \\bv_0^T \\Phib_0 \\bv_0 - \\bv_n^T \\Phib_n \\bv_n)}    \\times  \\\\\n& &   \\textcolor{red}{\\phi^{\\frac p 2}}  e^{ -\\frac{\\phi}{2} \\left(  (\\b^T - \\bv_n)^T  \\Phib_n (\\b -\n                               \\bv_n) \\right) }\n \\end{eqnarray*}\n\n\n . . .\n \n \\begin{eqnarray*} \n& \\propto &\n  \\phi^{\\frac {n +  \\nu_0}{ 2} - 1}\n e^{- \\frac \\phi 2 ( \\SS_0   + \\Y^T\\Y+ \\bv_0^T \\Phib_0 \\bv_0 - \\bv_n^T \\Phib_n \\bv_n)}    \\times  \\\\\n& &   \\textcolor{red}{|\\phi \\Phi_n |^{\\frac 1 2}}  e^{ -\\frac{\\phi}{2} \\left(  (\\b^T - \\bv_n)^T  \\Phib_n (\\b -\n                               \\bv_n) \\right) }\n \\end{eqnarray*}\n\n## Posterior Distributions {.smaller}\n\nPosterior density  (up to normalizing contants) $p(\\b, \\phi \\mid \\Y) = p(\\phi \\mid \\Y) p(\\b \\mid \\phi \\Y)$\n   $$\\begin{eqnarray*}\n   p(\\phi \\mid \\Y) p(\\b \\mid \\phi \\Y) & \\propto &\n  \\phi^{\\frac {n +  \\nu_0}{ 2} - 1}\n  e^{- \\frac \\phi 2 ( \\SS_0   + \\Y^T\\Y+ \\bv_0^T \\Phib_0 \\bv_0 - \\bv_n^T \\Phib_n \\bv_n)}  \\times \\\\\n   & & (2 \\pi)^{- \\frac p 2} |\\phi \\Phi_n |^{\\frac 1 2}e^{- \\frac{\\phi}{2} (\\b - \\bv_n)^T \\Phib_n (\\b -\n   \\bv_n) }      \n   \\end{eqnarray*}$$\n   \n. . .\n\nMarginal\n  $$\\begin{eqnarray*}\n   p(\\phi \\mid \\Y)  & \\propto & \n  \\phi^{\\frac {n +  \\nu_0}{ 2} - 1}\n  e^{- \\frac \\phi 2 ( \\SS_0   + \\Y^T\\Y+ \\bv_0^T \\Phib_0 \\bv_0 - \\bv_n^T \\Phib_n \\bv_n)}  \\times \\\\\n   & &  \\int_{\\bbR^p} (2 \\pi)^{- \\frac p 2} |\\phi \\Phi_n |^{\\frac 1 2}e^{- \\frac{\\phi}{2} (\\b - \\bv_n)^T \\Phib_n (\\b -\n   \\bv_n) \\ d\\b}   \\\\\n   & =  & \n  \\phi^{\\frac {n +  \\nu_0}{ 2} - 1}\n  e^{- \\frac \\phi 2 ( \\SS_0   + \\Y^T\\Y+ \\bv_0^T \\Phib_0 \\bv_0 - \\bv_n^T \\Phib_n \\bv_n)} \n   \\end{eqnarray*}$$\n   \n-  Conditional Normal for $\\b \\mid \\phi, \\Y$  and marginal Gamma for $\\phi \\mid \\Y$ \n\n- No need for Gibbs sampling!\n\n\n\n\n\n\n## $\\NG$ Posterior Distribution {.smaller}\n\n$$\\begin{eqnarray*}\n\\b \\mid \\phi, \\Y & \\sim &\\N(\\bv_n, (\\phi \\Phib_n)^{-1})   \\\\\n\\phi \\mid \\Y &\\sim &\\Gam(\\frac{\\nu_n}{2}, \\frac{\\SS_n}{2}) \\\\\n(\\b, \\phi) \\mid \\Y & \\sim & \\NG(\\bv_n, \\Phib_n, \\nu_n, \\SS_n)\n  \\end{eqnarray*}$$\n\nHyperparameters: \n$$\\begin{align*}\n\\Phi_n & =  \\X^T\\X +  \\Phib_0   & \\quad \n\\bv_n &  =  \\Phib_n^{-1} (\\X^T\\X \\bhat  + \\Phib_0 \\bv_0)  \\\\\n\\nu_n &  =   n + \\nu_0  & \\quad  \n\\SS_n &  =   \\SS_0 + \\Y^T\\Y + \\bv_0^T \\Phib_0 \\bv_0 - \\bv_n^T \\Phib_n \\bv_n \n\\end{align*}\n$$\n\n\n. . .\n\n\\begin{align*} \n\\SS_n &  = \\SS_0 + \\| \\Y - \\X \\bv_n \\|^2 +  (\\bv_0 - \\bv_n)^T \\Phib_0 (\\bv_0 - \\bv_n) \\\\\n\n & = \\SS_0 + \\| \\Y - \\X \\bv_n \\|^2 +  \\| \\bv_0 - \\bv_n \\|^2_{\\Phib_0}\n\\end{align*}\n\n- Inner product induced by prior  precision $\\langle u, v \\rangle_A \\equiv u^TAv$ \n\n-  $\\| \\bv_0 - \\bv_n \\|^2_{\\Phib_0}$ mismatch of prior and posterior mean under prior\n\n## Marginal Distribution  {.smaller}\n\n\n \n::: {.Theorem  .unnumbered}\n## Student-t\nLet  $\\tb \\mid \\phi \\sim \\N(m, \\frac{1}{\\phi} \\Sigmab)$ and $\\phi \\sim\n    \\Gam(\\nu/2, \\nu \\shat/2)$. \n    \nThen  $\\tb$ $(p \\times 1)$ has a $p$\n    dimensional multivariate $t$ distribution \n    $$\\tb \\sim t_\\nu( m,\n    \\shat \\Sigmab )$$ \nwith location $m$, scale matrix $\\shat \\Sigmab$ and density\n    \n$$p(\\tb) \\propto  \\left[ 1 + \\frac{1}{\\nu}  \\frac{ (\\tb - m)^T\n    \\Sigmab^{-1} (\\tb - m)}{\\shat} \\right]^{- \\frac{p + \\nu}{2}}$$\n    \n:::\n\n. . .\n\nNote - true  for prior or posterior given $\\Y$\n\n\n## Derivation {.smaller}\n\nMarginal density  $p(\\tb) = \\int_0^\\infty p(\\tb \\mid \\phi) p(\\phi) \\, d\\phi$\n\n\\begin{eqnarray*}\n  p(\\tb) & \\propto & \\int |  \\Sigmab/\\phi|^{-1/2}\ne^{- \\frac{\\phi}{2} (\\tb - m)^T\n      \\Sigmab^{-1} (\\tb - m)}  \\phi^{\\nu/2 - 1} e^{- \\phi \\frac{\\nu\n      \\shat}{2}}\\, d \\phi    \\\\\n  & \\propto & \\int \\phi^{p/2} \\phi^{\\nu/2 - 1}\ne^{- \\phi \\frac{(\\tb - m)^T\n      \\Sigmab^{-1} (\\tb - m)+  \\nu\n      \\shat}{2}}\\, d \\phi    \\\\\n & \\propto & \\int \\phi^{\\frac{p +\\nu}{2} - 1}\ne^{- \\phi \\frac{(\\tb - m)^T\n      \\Sigmab^{-1} (\\tb - m)+  \\nu\n      \\shat}{2}} \\, d \\phi    \\\\\n& = & \\Gamma((p + \\nu)/2 ) \\left( \\frac{(\\tb - m)^T\n      \\Sigmab^{-1} (\\tb - m)+  \\nu\n      \\shat}{2} \\right)^{- \\frac{p + \\nu}{2}}    \\\\\n& \\propto &  \\left( (\\tb - m)^T\n      \\Sigmab^{-1} (\\tb - m)+  \\nu\n      \\shat \\right)^{- \\frac{p + \\nu}{2}}    \\\\\n& \\propto &  \\left( 1 + \\frac{1}{\\nu}  \\frac{(\\tb - m)^T\n      \\Sigmab^{-1} (\\tb - m)}{\\shat}\n       \\right)^{- \\frac{p + \\nu}{2}}\n\\end{eqnarray*}\n\n## Marginal Posterior Distribution of $\\b$ {.smaller}\n\n  \\begin{eqnarray*}\n\\b \\mid \\phi, \\Y  & \\sim & \\N( \\bv_n, \\phi^{-1} \\Phib_n^{-1}) \\\\\n \\phi \\mid \\Y & \\sim & \\Gam\\left(\\frac{\\nu_n}{2},  \\frac{\\SS_n}{ 2} \\right)\n  \\end{eqnarray*}\n\\pause\n\n- Let $\\shat = \\SS_n/\\nu_n$  (Bayesian MSE) \n\n- The marginal posterior distribution of $\\b$ is multivariate Student-t\n$$\n\\b  \\mid \\Y \\sim t_{\\nu_n} (\\bv_n, \\shat \\Phib_n^{-1})\n$$ \\pause\n\n\n- Any linear combination $\\lambda^T\\b$ has a univariate\n$t$ distribution with $\\v_n$ degrees of freedom\n$$\\lambda^T\\b  \\mid \\Y \\sim t_{\\nu_n}\n(\\lambda^T\\bv_n, \\shat \\lambda^T\\Phi_n^{-1}\\lambda)$$ \n\n\n- use for individual $\\b_j$, the mean of $Y$,  $\\x^T \\b$, at  $\\x$,   or predictions $Y^* = {\\x^*}^T \\b + \\epsilon_i^*$\n\n## Predictive Distributions {.smaller}\n\n\nSuppose $\\Y^* \\mid \\b, \\phi \\sim \\N_s(\\X^* \\b , \\I_s/\\phi)$  and is conditionally\nindependent of $\\Y$ given $\\b$ and $\\phi$ \n\n- What is the predictive distribution of $\\Y^* \\mid \\Y$? \n\n- Use the representation that \n$\\Y^* \\eqindis \\X^* \\b + \\eps^*$ and $\\eps^*$ is independent of $\\Y$ given\n$\\phi$ \n\n. . .\n\n\\begin{eqnarray*}\n\\X^* \\b + \\eps^* \\mid \\phi, \\Y & \\sim & \\N(\\X^*\\bv_n, (\\X^{*} \\Phib_n^{-1} \\X^{*T}\n+ \\I_s)/\\phi)   \\\\\n\\Y^* \\mid \\phi, \\Y & \\sim & \\N(\\X^*\\bv_n, (\\X^{*} \\Phi_n^{-1} \\X^{*T}\n+ \\I_s)/\\phi)  \\\\\n\\phi \\mid \\Y & \\sim & \\Gam\\left(\\frac{\\nu_n}{2},\n  \\frac{\\shat \\nu_n}{ 2} \\right) \n\\end{eqnarray*}\n\n- Use the Theorem to conclude that \n$$\\Y^* \\mid \\Y  \\sim  t_{\\nu_n}( \\X^*\\bv_n, \\shat (\\I + \\X^* \\Phib_n^{-1} \\X^T))$$\n\n## Choice of Conjugate (or Semi-Conjugate) Prior {.smaller}\n\n- need to specify Normal prior mean $\\bv_0$ and precision $\\Phib_0$ \n\n- need to specify Gamma shape ($\\nu_o$ prior df) and rate (estimate of  $\\sigma^2$)\n\n- hard in higher dimensions!\n\n- default choices?\n\n    - Jeffreys' prior\n    - unit-information prior\n    - Zellner's g-prior \n    - ridge priors\n    - mixtures of conjugate priors\n    \n## Jeffreys' Prior {.smaller}\n\n- Jeffreys prior is invariant to model parameterization of $\\tb = (\\b ,\\phi)$ \n  $$p(\\tb) \\propto |\\cI(\\tb)|^{1/2}$$\n- $\\cI(\\tb)$ is the  Expected Fisher Information matrix\n$$\\cI(\\theta) = - \\E[ \\left[ \\frac{\\partial^2 \\log(\\cL(\\tb))}{\\partial\n  \\theta_i \\partial \\theta_j} \\right] ]$$\n  \n- log likelihood expressed as function of sufficient statistics\n\n. . .\n\n$$\\log(\\cL(\\b, \\phi))  =  \\frac{n}{2} \\log(\\phi)  - \\frac{\\phi}{2} \\| (\\I_n - \\P_\\x) \\Y\\|^2\n - \\frac{\\phi}{2}(\\b - \\bhat)^T(\\X^T\\X)(\\b - \\bhat)$$ \n\n- projection matrix  $\\P_{\\X} = \\X (\\X^T\\X)^{-1} \\X^T$\n\n## Information matrix {.smaller}\n\\begin{eqnarray*}\n\\frac{\\partial^2 \\log \\cL} { \\partial \\tb \\partial \\tb^T} & = &\n\\left[\n  \\begin{array}{cc}\n    -\\phi (\\X^T\\X) & -(\\X^T\\X) (\\b - \\bhat) \\\\\n  - (\\b - \\bhat)^T (\\X^T\\X) & -\\frac{n}{2} \\frac{1}{\\phi^2} \\\\\n  \\end{array}\n\\right] \\\\\n\\E[\\frac{\\partial^2 \\log \\cL} { \\partial \\tb \\partial \\tb^T}] & = &\n\\left[\n  \\begin{array}{cc}\n    -\\phi (\\X^T\\X) & \\zero_p \\\\\n  \\zero_p^T & -\\frac{n}{2} \\frac{1}{\\phi^2} \\\\\n  \\end{array}\n\\right] \\\\\n& & \\\\\n\\cI((\\b, \\phi)^T) & = & \\left[\n  \\begin{array}{cc}\n    \\phi (\\X^T\\X) & \\zero_p \\\\\n  \\zero_p^T & \\frac{n}{2} \\frac{1}{\\phi^2}\n  \\end{array}\n\\right]\n  \\end{eqnarray*}\n  \n. . .\n\n Jeffreys' Prior  (don't use!)\n  \\begin{eqnarray*}\n  p_J(\\b, \\phi)  & \\propto & |\\cI((\\b, \\phi)^T) |^{1/2}   \n                =  |\\phi \\X^T\\X|^{1/2} \\left(\\frac{n}{2}\n                 \\frac{1}{\\phi^2} \\right)^{1/2} \n  \\propto    \\phi^{p/2 - 1} |\\X^T\\X|^{1/2} \\\\\n  & \\propto & \\phi^{p/2 - 1}  \n  \\end{eqnarray*}\n  \n## Recommended Independent Jeffreys  Prior \n\n-   Treat $\\b$ and $\\phi$ separately  (_orthogonal\n    parameterization_) \n-    $p_{IJ}(\\b) \\propto |\\cI(\\b)|^{1/2}$ and\n      $p_{IJ}(\\phi) \\propto |\\cI(\\phi)|^{1/2}$ \n    \n. . .\n\n$$\n\\cI((\\b, \\phi)^T)  =  \\left[\n  \\begin{array}{cc}\n    \\phi (\\X^T\\X) & \\zero_p \\\\\n  \\zero_p^T & \\frac{n}{2} \\frac{1}{\\phi^2}\n  \\end{array}\n\\right]\n$$\n\n. . .\n\n\\begin{align*} p_{IJ}(\\b) & \\propto |\\phi \\X^T\\X|^{1/2} \\propto 1 \\\\\n               p_{IJ}(\\phi) & \\propto \\phi^{-1} \\\\\n               p_{IJ}(\\beta, \\phi) & \\propto p_{IJ}(\\b) p_{IJ}(\\phi) = \\phi^{-1}\n\\end{align*}               \n\n\n\n## Formal Posterior Distribution {.smaller}\n\n- Use Independent Jeffreys Prior\n$p_{IJ}(\\beta, \\phi) \\propto p_{IJ}(\\b) p_{IJ}(\\phi) = \\phi^{-1}$\n\n- Formal Posterior Distribution\n\\begin{eqnarray*}\n  \\b \\mid \\phi, \\Y & \\sim & \\N(\\bhat, (\\X^T\\X)^{-1} \\phi^{-1})  \\\\\n  \\phi \\mid \\Y & \\sim& \\Gam((n-p)/2, \\| \\Y - \\X\\bhat \\|^2/2) \\\\\n\\b \\mid \\Y & \\sim & t_{n-p}(\\bhat, \\shat (\\X^T\\X)^{-1})\n\\end{eqnarray*}\n\n- Bayesian Credible Sets\n$p(\\b \\in C_\\alpha) \\mid \\Y) = 1- \\alpha$ correspond to frequentist Confidence\nRegions\n$$\\frac{\\x^T\\b - \\x^T \\bhat} {\\sqrt{\\shat \\x^T(\\X^T\\X)^{-1} \\x} }\\sim t_{n-p}$$\n- conditional on $\\Y$ for Bayes and conditional on $\\b$ for frequentist\n\n## Other priors next\n\n## Invariance and Choice of Mean/Precision {.smaller}\n\n-  the model in vector form $Y  \\mid \\beta, \\phi \\sim \\textsf{N}_n (X\\beta, \\phi^{-1} I_n)$\n\n  \n\n- What if we transform  the mean $X\\beta = X H H^{-1} \\beta$ with new $X$ matrix $\\tilde{X} = X H$ where $H$ is $p \\times p$ and invertible and coefficients  $\\tilde{\\beta} = H^{-1} \\beta$.\n\n  \n\n- obtain the posterior for $\\tilde{\\beta}$ using $Y$ and $\\tilde{X}$  \n$$ Y \\mid  \\tilde{\\beta}, \\phi \\sim \\textsf{N}_n (\\tilde{X}\\tilde{\\beta}, \\phi^{-1} I_n)$$\n\n- since $\\tilde{X} \\tilde{\\beta} = X H  \\tilde{\\beta} = X \\beta$  invariance suggests that the posterior for $\\beta$ and $H \\tilde{\\beta}$ should be the same \n\n  \n- plus the posterior of $H^{-1} \\beta$\nand $\\tilde{\\beta}$ should be the same\n\n. . .\n\n::: {.callout-tip title=\"Exercise for the Energetic Student\"}\nWith some linear algebra, show that this is true for a normal prior if $b_0 = 0$ and $\\Phi_0$ is $k X^TX$ for some $k$  \n:::\n \n## Zellner's g-prior {.smaller}\n\n- Popular choice is to take $k = \\phi/g$ which is a special case of Zellner's g-prior\n$$\\beta \\mid \\phi, g \\sim \\textsf{N}\\left(0, \\frac{g}{\\phi} (X^TX)^{-1}\\right)$$\n\n  \n\n- Full conditional \n$$\\beta \\mid \\phi, g \\sim \\textsf{N}\\left(\\frac{g}{1 + g} \\hat{\\beta}, \\frac{1}{\\phi} \\frac{g}{1 + g} (X^TX)^{-1}\\right)$$\n  \n\n- one parameter $g$ controls shrinkage\n\n  \n\n- if $\\phi \\sim \\textsf{Gamma}(v_0/2, s_0/2)$ then posterior is\n$$\\phi \\mid y_1, \\ldots, y_n \\sim \\textsf{Gamma}(v_n/2, s_n/2)$$\n  \n\n- Conjugate so we could skip Gibbs sampling and sample directly from gamma and then conditional normal!\n\n \n## Ridge Regression  {.smaller}\n\n- If $X^TX$ is nearly singular, certain  elements of $\\beta$ or (linear combinations of $\\beta$) may have huge variances under the $g$-prior (or flat prior) as the MLEs are highly unstable!\n\n  \n\n- **Ridge regression** protects against the explosion of variances and ill-conditioning with the conjugate priors:\n$$\\beta \\mid \\phi \\sim \\textsf{N}(0, \\frac{1}{\\phi \\lambda} I_p)$$\n  \n\n- Posterior for $\\beta$  (conjugate case)\n$$\\beta \\mid \\phi, \\lambda, y_1, \\ldots, y_n \\sim \n\\textsf{N}\\left((\\lambda I_p + X^TX)^{-1} X^T Y,  \\frac{1}{\\phi}(\\lambda I_p + X^TX)^{-1}\n\\right)$$\n\n\n\n\n \n##  Bayes Regression {.smaller}\n\n- Posterior mean (or mode) given $\\lambda$ is biased, but can show that there **always** is a value of $\\lambda$  where the frequentist's expected squared error loss is smaller for the Ridge estimator than MLE!\n\n  \n\n- related to penalized maximum likelihood estimation \n\n  \n\n-  Choice of $\\lambda$\n\n  \n\n-  Bayes Regression and choice of $\\Phi_0$ in general is a very important problem and provides the foundation  for many variations on shrinkage estimators, variable selection, hierarchical models, nonparameteric regression and more!\n\n  \n\n- Be sure that you can derive the full conditional posteriors for $\\beta$ and $\\phi$ as well as the joint posterior in the conjugate case!\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}