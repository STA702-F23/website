{
  "hash": "c2222ce2ee1ab8b6e037b70a93f29640",
  "result": {
    "markdown": "---\ntitle: \"Lecture 14: Basics of Bayesian Hypothesis Testing\"\nauthor: \"Merlise Clyde\"\nsubtitle: \"STA702\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n##  Feature Selection via Shrinkage\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\text{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- modal estimates in regression models under certain shrinkage priors will set a subset\nof coefficients to zero\n\n- not true with posterior mean\n\n- multi-modal posterior \n\n- no prior probability that coefficient is zero\n\n- how should we approach selection/hypothesis testing?\n\n- Bayesian Hypothesis Testing\n\n## Basics of Bayesian Hypothesis Testing\n\n\nSuppose we have univariate data $Y_i \\overset{iid}{\\sim} \\mathcal{N}(\\theta, 1)$, $\\Y = (y_i, \\ldots, y_n)^T$\n\n \n\n- goal is to test $\\mathcal{H}_0: \\theta = 0; \\ \\ \\text{vs } \\mathcal{H}_1: \\theta \\neq 0$ \n\n\n- Additional unknowns are $\\mathcal{H}_0$ and $\\mathcal{H}_1$\n\n- Put a prior on the actual hypotheses/models, that is, on $\\pi(\\mathcal{H}_0) = \\Pr(\\mathcal{H}_0 = \\text{True})$ and $\\pi(\\mathcal{H}_1) = \\Pr(\\mathcal{H}_1 = \\text{True})$.\n\n- (Marginal) Likelihood of the hypotheses: \n$\\cal{L}(\\mathcal{H}_i) \\propto p( \\y \\mid \\mathcal{H}_i)$\n      \n \n. . .\n\n$$p( \\y \\mid \\mathcal{H}_0) = \\prod_{i = 1}^n (2 \\pi)^{-1/2} \\exp{- \\frac{1}{2} (y_i - 0)^2}$$\n\n. . . \n\n$$p( \\y \\mid \\mathcal{H}_1)  = \\int_\\Theta p( \\y \\mid \\mathcal{H}_1, \\theta) p(\\theta \\mid \\mathcal{H}_1) \\, d\\theta$$\n \n## Bayesian Approach \n\n- Need priors distributions on parameters under each hypothesis\n    \n    - in our simple normal model, the only additional unknown parameter is $\\theta$\n    - under $\\mathcal{H}_0$, $\\theta = 0$ with probability 1\n    - under $\\mathcal{H}_0$, $\\theta \\in \\mathbb{R}$ we could take $\\pi(\\theta) = \\mathcal{N}(\\theta_0, 1/\\tau_0^2)$.\n\n \n\n-   Compute marginal likelihoods for each hypothesis, that is, $\\cal{L}(\\mathcal{H}_0)$ and $\\cal{L}(\\mathcal{H}_1)$.  \n\n \n\n- Obtain posterior probabilities of $\\cal{H}_0$ and $\\cal{H}_1$ via Bayes Theorem.\n$$\n\\begin{split}\n\\pi(\\mathcal{H}_1 \\mid \\y) = \\frac{ p( \\y \\mid \\mathcal{H}_1) \\pi(\\mathcal{H}_1) }{ p( \\y \\mid \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\y \\mid \\mathcal{H}_1) \\pi(\\mathcal{H}_1)}\n\\end{split}\n$$\n- Provides a joint posterior distribution for $\\theta$ and $\\mathcal{H}_i$:  $p(\\theta \\mid   \\mathcal{H}_i,  \\y)$ and $\\pi(\\mathcal{H}_i \\mid \\y)$\n \n## Hypothesis Tests via  Decision Theory\n\n- Loss function for hypothesis testing\n\n    - $\\hat{\\cal{H}}$ is the chosen hypothesis\n \n    - $\\cal{H}_{\\text{true}}$ is the true hypothesis, $\\cal{H}$ for short\n \n \n\n- Two types of errors:\n\n    - Type I error:  $\\hat{\\cal{H}} = 1$  and  $\\cal{H} = 0$\n\n \n\n    - Type II error:  $\\hat{\\cal{H}} = 0$  and  $\\cal{H} = 1$\n\n \n\n- Loss function:\n$$L(\\hat{\\cal{H}}, \\cal{H}) =  w_1  \\, 1(\\hat{\\cal{H}} = 1, \\cal{H} = 0) + w_2 \\, 1(\\hat{\\cal{H}} = 0, \\cal{H} = 1)$$\n\n    - $w_1$ weights how bad it is to make a Type I error\n\n    - $w_2$ weights how bad it is to make a Type II error\n\n \n## Loss Function Functions and Decisions\n\n- Relative weights $w = w_2/w_1$\n$$L(\\hat{\\cal{H}}, \\cal{H}) =   \\, 1(\\hat{\\cal{H}} = 1, \\cal{H} = 0) + w \\, 1(\\hat{\\cal{H}} = 0, \\cal{H} = 1)$$\n \n\n- Special case $w=1$\n$$L(\\hat{\\cal{H}}, \\cal{H}) =    1(\\hat{\\cal{H}} \\neq \\cal{H})$$ \n- known as 0-1 loss (most common)\n\n \n\n- Bayes Risk (Posterior Expected Loss)\n$$\\textsf{E}_{\\cal{H} \\mid  \\y}[L(\\hat{\\cal{H}}, \\cal{H}) ] =\n1(\\hat{\\cal{H}} = 1)\\pi(\\cal{H}_0 \\mid  \\y) +  1(\\hat{\\cal{H}} = 0) \\pi(\\cal{H}_1 \\mid  \\y)$$\n\n\n\n \n\n- Minimize loss by picking hypothesis with the highest posterior probability \n\n\n\n \n## Bayesian hypothesis testing\n\n- Using Bayes theorem,\n$$\n\\begin{split}\n\\pi(\\mathcal{H}_1 \\mid \\y) = \\frac{ p( \\y \\mid \\mathcal{H}_1) \\pi(\\mathcal{H}_1) }{ p( \\y \\mid \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\y \\mid \\mathcal{H}_1) \\pi(\\mathcal{H}_1)},\n\\end{split}\n$$\n\n\n- If $\\pi(\\mathcal{H}_0) = 0.5$ and $\\pi(\\mathcal{H}_1) = 0.5$ _a priori_, then\n$$\n\\begin{split}\n\\pi(\\mathcal{H}_1 \\mid \\y) & = \\frac{ 0.5 p( \\y \\mid \\mathcal{H}_1) }{ 0.5 p( \\y \\mid \\mathcal{H}_0) + 0.5 p( \\y \\mid \\mathcal{H}_1) } \\\\\n\\\\\n& = \\frac{ p( \\y \\mid \\mathcal{H}_1) }{ p( \\y \\mid \\mathcal{H}_0) + p( \\y \\mid \\mathcal{H}_1) }= \\frac{ 1 }{ \\frac{p( \\y \\mid \\mathcal{H}_0)}{p( \\y \\mid \\mathcal{H}_1)} + 1 }\\\\\n\\end{split}\n$$\n\n \n\n\n \n## Bayes factors\n\n- The ratio $\\frac{p( \\y \\mid \\mathcal{H}_0)}{p( \\y \\mid \\mathcal{H}_1)}$  is a ratio of marginal likelihoods and is known as the **Bayes factor** in favor of $\\mathcal{H}_0$, written as $\\mathcal{BF}_{01}$. Similarly, we can compute $\\mathcal{BF}_{10}$ via the inverse ratio.\n\n\n\n\n- Bayes factors provide a weight of evidence in the data in favor of one model over another.\n  and are used as an alternative to the frequentist p-value.\n\n \n\n- **Rule of Thumb**: $\\mathcal{BF}_{01} > 10$ is strong evidence for $\\mathcal{H}_0$;  $\\mathcal{BF}_{01} > 100$ is decisive evidence for $\\mathcal{H}_0$.\n\n \n\n- In the example (with equal prior probabilities),\n$$\n\\begin{split}\n\\pi(\\mathcal{H}_1 \\mid \\y) = \\frac{ 1 }{ \\frac{p( \\y \\mid \\mathcal{H}_0)}{p( \\y \\mid \\mathcal{H}_1)} + 1 } = \\frac{ 1 }{ \\mathcal{BF}_{01} + 1 } \\\\\n\\end{split}\n$$\n\n\n- the higher the value of $\\mathcal{BF}_{01}$, that is, the weight of evidence in the data in favor of $\\mathcal{H}_0$, the lower the marginal posterior probability that $\\mathcal{H}_1$ is true.\n  \n \n\n- $\\mathcal{BF}_{01} \\uparrow$, $\\pi(\\mathcal{H}_1 \\mid \\y) \\downarrow$.\n\n\n\n\n \n## Posterior Odds and Bayes Factors\n\n\n- Posterior odds $\\frac{\\pi(\\mathcal{H}_0 \\mid \\y)}{\\pi(\\mathcal{H}_1 \\mid \\y)}$\n$$\n\\begin{split}\n\\frac{\\pi(\\mathcal{H}_0 | \\y)}{\\pi(\\mathcal{H}_1 | \\y)} & = \\frac{ p( \\y |\\mathcal{H}_0) \\pi(\\mathcal{H}_0) }{ p( \\y | \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\y | \\mathcal{H}_1) \\pi(\\mathcal{H}_1)} \\div \\frac{ p( \\y | \\mathcal{H}_1) \\pi(\\mathcal{H}_1) }{ p( \\y  \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\y | \\mathcal{H}_1) \\pi(\\mathcal{H}_1)}\\\\\n\\\\\n& = \\frac{ p( \\y | \\mathcal{H}_0) \\pi(\\mathcal{H}_0) }{ p( \\y | \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\y | \\mathcal{H}_1) \\pi(\\mathcal{H}_1)} \\times \\frac{ p( \\y | \\mathcal{H}_0) \\pi(\\mathcal{H}_0) + p( \\y | \\mathcal{H}_1) \\pi(\\mathcal{H}_1)}{ p( \\y | \\mathcal{H}_1) \\pi(\\mathcal{H}_1) }\\\\\n\\\\\n\\therefore \\underbrace{\\frac{\\pi(\\mathcal{H}_0 \\mid \\y)}{\\pi(\\mathcal{H}_1 \\mid \\y)}}_{\\text{posterior odds}} & = \\underbrace{\\frac{ \\pi(\\mathcal{H}_0) }{ \\pi(\\mathcal{H}_1) }}_{\\text{prior odds}} \\times \\underbrace{\\frac{ p( \\y \\mid \\mathcal{H}_0) }{ p( \\y \\mid \\mathcal{H}_1) }}_{\\text{Bayes factor } \\mathcal{BF}_{01}} \\\\\n\\end{split}\n$$\n\n\n \n\n- The Bayes factor can be thought of as the factor by which our prior odds change (towards the posterior odds) in the light of the data.\n\n\n\n\n \n##  Likelihoods & Evidence\n\nMaximized Likelihood. $n = 10$\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](14-hypothesis-testing_files/figure-revealjs/lik-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\np-value = 0.05\n \n##  Marginal Likelihoods & Evidence\n\nMaximized & Marginal Likelihoods\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](14-hypothesis-testing_files/figure-revealjs/marglik-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n  $\\cal{BF}_{10}$ = 1.73 or   $\\cal{BF}_{01}$ = 0.58 \n  \n  Posterior Probability of $\\cal{H}_0$ = 0.3665\n  \n \n## Candidate's Formula (Besag 1989)\n\n\nAlternative expression for BF based on Candidate's Formula or Savage-Dickey ratio \n$$\\cal{BF}_{01} = \\frac{p( \\y \\mid \\cal{H}_0)}\n       {p( \\y \\mid \\cal{H}_1)} =\n  \\frac{\\pi_\\theta(0 \\mid \\cal{H}_1, \\y)} \n       {\\pi_\\theta(0 \\mid \\cal{H}_1)}$$\n\n\n. . .\n\n$$\\pi_\\theta(\\theta \\mid \\cal{H}_i, \\y)  =  \\frac{p(\\y \\mid \\theta, \\cal{H}_i) \\pi(\\theta \\mid \\cal{H}_i)} {p(\\y \\mid \\cal{H}_i)}  \\Rightarrow  \np(\\y \\mid \\cal{H}_i)   = \\frac{p(\\y \\mid \\theta, \\cal{H}_i) \\pi(\\theta \\mid \\cal{H}_i)} {\\pi_\\theta(\\theta \\mid \\cal{H}_i, \\y)}$$\n\n. . .\n\n\n$$\\cal{BF}_{01}  = \\frac{\\frac{p(\\y \\mid \\theta, \\cal{H}_0) \\pi(\\theta \\mid \\cal{H}_0)} {\\pi_\\theta(\\theta \\mid \\cal{H}_0, \\y)} } { \\frac{p(\\y \\mid \\theta, \\cal{H}_1) \\pi(\\theta \\mid \\cal{H}_1)} {\\pi_\\theta(\\theta \\mid \\cal{H}_1, \\y)}}  =   \\frac{\\frac{p(\\y \\mid \\theta = 0) \\delta_0(\\theta)} {\\delta_0(\\theta)} } { \\frac{p(\\y \\mid \\theta, \\cal{H}_1) \\pi(\\theta \\mid \\cal{H}_1)} {\\pi_\\theta(\\theta \\mid \\cal{H}_1, \\y)}} \n =   \\frac{p(\\y \\mid \\theta = 0)}{p(\\y \\mid \\theta, \\cal{H}_1)} \n   \\frac{\\delta_0(\\theta)} {\\delta_0(\\theta)}  \\frac{\\pi_\\theta(\\theta \\mid \\cal{H}_1, \\y)}{\\pi(\\theta \\mid \\cal{H}_1)} $$ \n\n\n- Simplifies to the ratio of the posterior to prior densities  when evaluated $\\theta$ at zero\n\n## Prior\n\nPlots were based on a $\\theta \\mid \\cal{H}_1 \\sim \\textsf{N}(0, 1)$ \n\n \n\n- centered at value for $\\theta$ under $\\cal{H}_0$  (goes back to Jeffreys)\n\n \n\n- \"unit information prior\"  equivalent to a prior sample size is 1\n\n\n- is this a \"reasonable prior\"?\n\n    - What happens if $n \\to \\infty$?\n\n \n\n    - What happens of $\\tau_0 \\to 0$ ?   (less informative)\n\n \n\n\n\n## Choice of Precision\n\n:::: {.columns}\n\n::: {.column width=50%}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](14-hypothesis-testing_files/figure-revealjs/marglik-tau10-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n- $\\tau_0 = 1/10$\n\n\n- Bayes Factor for $\\cal{H}_0$ to $\\cal{H}_1$ is $1.5$\n\n\n\n- Posterior Probability of $\\cal{H}_0$ = 0.6001\n\n:::\n\n::: {.column width=50%}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](14-hypothesis-testing_files/figure-revealjs/marglik-tau1000-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n- $\\tau_0 = 1/1000$\n\n\n- Bayes Factor for $\\cal{H}_0$ to $\\cal{H}_1$ is $14.65$\n\n\n\n- Posterior Probability of $\\cal{H}_0$ = 0.9361\n\n:::\n\n::::\n\n\n## Vague Priors & Hypothesis Testing\n\n\n\n- As $\\tau_0 \\to 0$ the $\\cal{BF}_{01} \\to \\infty$ and  $\\Pr(\\cal{H}_0 \\mid \\y \\to 1$! \n\n\n\n- As we use a less & less informative prior  for $\\theta$ under $\\cal{H}_1$ we obtain more & more evidence for $\\cal{H}_0$ over $\\cal{H}_1$!\n\n\n\n- Known as **Bartlett's Paradox** - the paradox is that a seemingly non-informative prior for $\\theta$ is very informative about $\\cal{H}$!\n\n\n\n- General problem with nested sequence of models.  If we choose vague priors on the additional parameter in the larger model we will be favoring the smaller models under consideration!\n\n\n- Similar phenomenon with increasing sample size (**Lindley's Paradox**)\n\n. . .\n\n::: {.callout-warning}\n**Bottom Line** Don't use vague priors!\n:::\n\n\n. . .\n\nWhat should we use then?\n\n\n## Other Options\n\n- Place a prior on $\\tau_0$\n$$\\tau_0 \\sim \\textsf{Gamma}(1/2, 1/2)$$\n\n- If $\\theta \\mid \\tau_0, \\cal{H}_1 \\sim \\textsf{N}(0, 1/\\tau_0)$,  then $\\theta_0  \\mid \\cal{H}_1$ has a $\\textsf{Cauchy}(0,1)$ distribution!  Recommended by Jeffreys (1961)\n\n\n\n- no closed form expressions for marginal likelihood!\n\n\n\n## Intrinsic Bayes Factors & Priors  (Berger & Pericchi)\n\n- Can't use improper priors under $\\cal{H}_1$\n\n\n\n-  use part of the data $y(l)$ to update an improper prior on $\\theta$ to get a proper posterior  $\\pi(\\theta \\mid \\cal{H}_i, y(l))$\n\n\n\n- use $\\pi(\\theta \\mid y(l), \\cal{H}_i)$ to obtain the posterior for $\\theta$ based on the rest of the training data\n\n\n\n- Calculate a Bayes Factor (avoids arbitrary normalizing constants!)\n\n\n\n- Choice of training sample $y(l)$? \n\n- Berger & Pericchi (1996) propose \"averaging\" over training samples  **intrinsic Bayes Factors**\n\n\n\n\n- **intrinsic prior** on $\\theta$ that leads to the Intrisic Bayes Factor  \n\n",
    "supporting": [
      "14-hypothesis-testing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}