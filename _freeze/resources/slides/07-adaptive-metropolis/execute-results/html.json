{
  "hash": "dcdbbde9e98d783240d9210ca776c039",
  "result": {
    "markdown": "---\ntitle: \"Lecture 7: MCMC Diagnostics & Adaptive Metropolis\"\nsubtitle: \"STA702\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n## Example from Last Class {.smaller}\n\n- Marginal Likelihood \n$$\\cal{L}(\\mu, \\sigma^2, \\sigma^2_\\mu) \\propto (\\sigma^2 + \\sigma^2_\\mu)^{-n/2} \\exp \\left\\{ - \\frac{1}{2} \\frac{\\sum_{i=1}^n\\left(y_i - \\mu \\right)^2}{\\sigma^2 + \\sigma^2_\\mu }\\right\\}$$\n\n\n- Priors with $\\sigma^2 = 1$: $p(\\mu) \\propto 1$\n  and $\\sigma_\\mu \\sim  \\textsf{Cauchy}^+(0,1)$ independent of $\\mu$\n\n  \n\n- Symmetric proposal for $\\mu$ and  $\\sigma_\\tau$    \n\n  \n\n-  Independent normals centered at current values of $\\mu$ and $\\sigma_\\mu$  with covariance $\\frac{2.38^2}{d} \\textsf{Cov}(\\theta)$ where $d = 2$ (the dimension of $\\theta$ ) \n\n \n\n-  $\\delta^2 = 2.38^2/d$ optimal for multivariate normal target  [Roberts, Gelman, and Gilks (1997)](https://projecteuclid.org/journals/annals-of-applied-probability/volume-7/issue-1/Weak-convergence-and-optimal-scaling-of-random-walk-Metropolis-algorithms/10.1214/aoap/1034625254.full) with acceptance rate ranging from 40% to 23.4% (as $d \\to \\infty$)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n \n## Convergence diagnostics {.smaller}\n\n- Diagnostics available to help decide on number of burn-in & collected samples.\n\n  \n\n- **Note**: no definitive tests of convergence but you should do as many diagnostics as you can, on all parameters in your model.\n\n  \n\n- With \"experience\", visual inspection of trace plots perhaps most useful approach.\n\n  \n\n- There are a number of useful automated tests in R.\n\n  \n\n- **CAUTION**: diagnostics cannot guarantee that a chain has converged, but they can indicate it has not converged.\n\n\n\n \n## Diagnostics in R {.smaller}\n\n\n\n\n- The most popular package for MCMC diagnostics in R is `coda`.\n\n  \n\n- `coda` uses a special MCMC format so you must always convert your posterior matrix into an MCMC object. \n\n  \n\n- For the example, we have the following in R.\n\n. . .\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#library(coda)\ntheta.mcmc <- mcmc(theta,start=1) #no burn-in (simple problem!)\n```\n:::\n\n\n\n\n \n## Diagnostics in R {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(theta.mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n             Mean     SD Naive SE Time-series SE\nmu       -0.07977 0.1046 0.001046       0.002839\nsigma_mu  0.17550 0.1273 0.001273       0.004397\n\n2. Quantiles for each variable:\n\n              2.5%     25%      50%      75%  97.5%\nmu       -0.283420 -0.1508 -0.08193 -0.00848 0.1337\nsigma_mu  0.007995  0.0758  0.15024  0.25228 0.4693\n```\n:::\n:::\n\n\n- The naive SE is the **standard error of the mean**, which captures simulation error of the mean rather than the posterior uncertainty. \n\n- The time-series SE adjusts the naive SE for **autocorrelation**.\n\n\n\n \n## Effective Sample Size {.smaller}\n\n- The **effective sample size** translates the number of MCMC samples $S$ into an equivalent number of independent samples.\n\n  \n\n- It is defined as\n$$\\textrm{ESS} = \\dfrac{S}{1 + 2 \\sum_k \\rho_k},$$\n\n -  $S$ is the sample size and $\\rho_k$ is the lag $k$ autocorrelation.\n  \n  \n\n- For our data, we have\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neffectiveSize(theta.mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       mu  sigma_mu \n1356.6495  838.2613 \n```\n:::\n:::\n\n\n\n\n  \n\n- So our 10,000 samples are equivalent to 1356.6 independent samples for $\\mu$  and 838.3 independent samples for $\\sigma_\\mu$.\n\n\n\n\n \n## Trace plot for mean\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-adaptive-metropolis_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\n \n## Trace plot for $\\sigma_\\mu$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-adaptive-metropolis_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\nOK  (be careful of scaling in plots!)\n\n\n\n \n## Autocorrelation {.smaller}\n\n- Another way to evaluate convergence is to look at the autocorrelation between draws of our Markov chain.\n\n  \n\n- The lag $k$ autocorrelation, $\\rho_k$, is the correlation between each draw and its $k$th lag, defined as\n$$\\rho_k = \\dfrac{\\sum_{s=1}^{S-k}(\\theta_s - \\bar{\\theta})(\\theta_{s+k} - \\bar{\\theta})}{\\sum_{s=1}^{S-k}(\\theta_s - \\bar{\\theta})^2}$$\n\n  \n\n- We expect the autocorrelation to decrease as $k$ increases. \n\n  \n\n- If autocorrelation remains high as $k$ increases, we have slow mixing due to the inability of the sampler to move around the space well.\n\n\n\n \n## Autocorrelation for mean {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-adaptive-metropolis_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\nSo-So\n\n\n \n## Autocorrelation for variance  {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-adaptive-metropolis_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\nworse\n\n\n\n\n \n## Gelman-Rubin  {.smaller}\n\nGelman  & Rubin suggested a diagnostic $R$ based on taking separate  chains with dispersed initial values to test convergence\n\n  \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-adaptive-metropolis_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n \n## Gelman-Rubin Diagnostic {.smaller}\n\n- Run m > 2 chains of length 2S from overdispersed starting values.\n- Discard the first S draws in each chain.\n-  Calculate the pooled within-chain variance $W$ and between-chain variance $B$.\n$$R = \\frac{\\frac{S-1}{S} W + \\frac{1}{S} B }{W}$$\n\n  \n\n- numerator and denominator are both unbiased estimates of the variance if the two chains have converged\n\n  \n  +  otherwise $W$ is an underestimate (hasn't explored enough)\n  +  numerator will overestimate as $B$ is too large (overdispersed starting points)\n  \n  \n\n- As $S \\to \\infty$ and $B \\to 0$,  $R \\to 1$\n\n  \n\n- version in `R` is slightly different \n\n \n## Gelman-Rubin Diagnostic  {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntheta.mcmc = mcmc.list(mcmc(theta1, start=5000), mcmc(theta2, start=5000))\ngelman.diag(theta.mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPotential scale reduction factors:\n\n         Point est. Upper C.I.\nmu                1          1\nsigma_mu          1          1\n\nMultivariate psrf\n\n1\n```\n:::\n:::\n\n\n  \n\n-  Values of $R > 1.1$ suggest lack of convergence\n\n  \n- Looks OK\n \n  \n\n- See also `gelman.plot`\n\n \n## Geweke statistic {.smaller}\n\n- Geweke proposed taking two non-overlapping parts of a single Markov chain (usually the first 10% and the last 50%) and comparing the mean of both parts, using a difference of means test\n\n  \n\n- The null hypothesis would be that the two parts of the chain are from the same distribution. \n\n  \n\n- The test statistic is a z-score with standard errors adjusted for autocorrelation, and if the p-value is significant for a variable, you need more draws. \n\n- Output in R is the Z score\n\n \n## Geweke Diagnostic  {.smaller}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngeweke.diag(theta.mcmc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n\nFraction in 1st window = 0.1\nFraction in 2nd window = 0.5 \n\n      mu sigma_mu \n -0.7779   0.7491 \n\n\n[[2]]\n\nFraction in 1st window = 0.1\nFraction in 2nd window = 0.5 \n\n      mu sigma_mu \n  0.4454   0.6377 \n```\n:::\n:::\n\n\n- The output is the z-score itself (not the p-value).\n \n## Practical advice on diagnostics {.smaller}\n\n- There are more tests we can use: Raftery and Lewis diagnostic, Heidelberger and Welch, etc.\n\n  \n\n- The Gelman-Rubin approach is quite appealing in using multiple chains\n\n  \n\n- Geweke (and Heidelberger and Welch) sometimes reject even when the trace plots look good.\n\n  \n\n- Overly sensitive to minor departures from stationarity that do not impact inferences.\n\n  \n\n\n\n- Most common method of assessing convergence is visual examination of trace plots.\n\n\n \n\n \n\n \n##  Improving Results {.smaller}\n\n-  more iterations and multiple chains\n\n  \n\n- thinning to reduce correlations and increase ESS\n\n  \n\n- change the proposal distribution $q$\n\n- adaptive Metropolis to tune $q$\n\n\n\n \n## Proposal Distribution {.smaller}\n\n\n\n\n- Common choice \n$$\\textsf{N}(\\theta^\\star; \\theta^{(s)}, \\delta^2 \\Sigma)$$\n  \n\n-  rough estimate of $\\Sigma$ based on the asymptotic Gaussian approximation $\\textsf{Cov}(\\theta \\mid y)$ and $\\delta = 2.38/\\sqrt{\\text{dim}(\\theta)}$ \n\n  \n  + find the MAP estimate (posterior mode)  $\\hat{\\theta}$\n  \n  \n  + take \n  $$\\Sigma =  \\left[- \n  \\frac{\\partial^2 \\log(\\cal{L}(\\theta)) + \\log(\\pi(\\theta))}\n       {\\partial \\theta \\partial \\theta^T} \\right]^{-1}_{\\theta = \\hat{\\theta}}$$\n  \n  \n\n- ignore prior and use inverse of Fisher Information (covariance of MLE)\n\n \n## Learn Covariance in Proposal? {.smaller}\n\n- Can we learn the proposal distribution?\n\n- ad hoc?  \n\n    - run an initial MCMC for an initial tuning phase (e.g. 1000 samples) with a fixed $\\delta$ and estimate $\\Sigma(\\theta)$ from samples.  \n    - run more to tweak $\\delta$ to get acceptance rate between $23\\%-40\\%$.\n    - fix the kernel for final run\n\n\n-  MCMC doesn't allow you to use the full history of the chain $\\theta^{(1)}, \\ldots, \\theta^{(s)}$ in constructing the proposal distributions as it violates the Markov assumption\n\n- even with no further \"learning\", no guarantee we will converge to posterior!\n\n  \n\n- more elegant approach -  formal **adaptive Metropolis**\n\n  \n\n  + keep adapting the entire time!\n\n. . . \n  \n::: {.callout-warning}  \nad hoc adaptation may mess up convergence !  \n:::  \n   \n \n## Adaptive MCMC  {.smaller}\n\n\n\n  \n\n-  run  RWM with a Gaussian proposal for a fixed number of iterations for $s < s_0$\n\n  \n\n- estimate of covariance at state $s$\n$$\\Sigma^{(s)} = \\frac{1}{s}\\left(\\sum_{i=1}^s \\theta^{(i)} {\\theta^{(i)}}^T - \ns \\bar{\\theta}^{(s)} {\\bar{\\theta}^{(s)}}^T\\right)$$\n\n  \n\n- proposal for $s > s_0$ with $\\delta = 2.38/\\sqrt{d}$\n$$\\theta^* \\sim \\textsf{N}(\\theta^{(s)}, \\delta^2 (\\Sigma^{(s)} + \\epsilon I_d))$$\n\n  \n\n- $\\epsilon > 0$ insures covariance is positive definite\n\n  \n\n- if $s_0$ is too large will take longer for adaptation to be seen\n\n \n-  need conditions for vanishing adaptation e.g.  that the proposal depends less and less on recent states in the chain - see [Roberts & Rosenthal (2009)](https://www-tandfonline-com.proxy.lib.duke.edu/doi/pdf/10.1198/jcgs.2009.06134?needAccess=true%7D%7BRoberts%20and%20Rosenthal%20(2009) )for examples and other conditions\n  \n\n\n \n## Example again  \n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-adaptive-metropolis_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\nAcceptance rate now around 30-35 % of 10,000 iterations!\n\n",
    "supporting": [
      "07-adaptive-metropolis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}