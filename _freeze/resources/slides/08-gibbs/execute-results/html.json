{
  "hash": "1d1a71c69d9d59f4ebf76b3a7ce710ac",
  "result": {
    "markdown": "---\ntitle: \"Lecture 8: Metropolis-Hastings, Gibbs and Blocking\"\nsubtitle: \"STA702\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n##  Metropolis-Hastings  (MH) {.smaller}\n\n- Metropolis requires that the proposal distribution be symmetric\n\n  \n\n- Hastings (1970)  generalizes Metropolis algorithms to allow asymmetric proposals - aka Metropolis-Hastings or MH  $q(\\theta^* \\mid \\theta^{(s)})$ does not need to be the same as $q(\\theta^{(s)} \\mid \\theta^*)$\n\n  \n-  propose $\\theta^*  \\mid \\theta^{(s)} \\sim q(\\theta^* \\mid \\theta^{(s)})$\n\n  \n- Acceptance probability \n$$\\min \\left\\{ 1, \\frac{\\pi(\\theta^*) \\cal{L}(\\theta^*)/q(\\theta^* \\mid \\theta^{(s)})}\n{\\pi(\\theta^{(s)}) \\cal{L}(\\theta^{(s)})/q( \\theta^{(s)} \\mid \\theta^*)} \\right\\}$$\n\n  \n\n- adjustment for asymmetry in acceptance ratio is key to ensuring convergence to stationary distribution!\n\n \n## Special cases {.smaller}\n\n- Metropolis\n\n  \n\n- Independence chain\n\n  \n\n- Gibbs samplers\n\n  \n\n- Metropolis-within-Gibbs\n\n  \n\n- combinations of the above!\n\n \n## Independence Chain {.smaller}\n\n- suppose we have a good approximation $\\tilde{\\pi}(\\theta \\mid y)$   to  $\\pi(\\theta \\mid y)$\n\n  \n\n-  Draw $\\theta^* \\sim \\tilde{\\pi}(\\theta \\mid y)$ _without_ conditioning on $\\theta^{(s)}$\n\n  \n\n- acceptance probability \n$$\\min \\left\\{ 1, \\frac{\\pi(\\theta^*) \\cal{L}(\\theta^*)/\\tilde{\\pi}(\\theta^* \\mid \\theta^{(s)})}\n{\\pi(\\theta^{(s)}) \\cal{L}(\\theta^{(s)})/\\tilde{\\pi}( \\theta^{(s)} \\mid \\theta^*)} \\right\\}$$\n\n  \n\n- what happens if the approximation is really accurate?\n\n  \n\n- probability of acceptance is $\\approx 1$\n\n  \n\n- Important caveat for convergence:  tails of the posterior should be at least as heavy as the tails of the posterior  (Tweedie 1994)\n\n  \n\n-  Replace Gaussian by a Student-t with low degrees of freedom\n\n  \n\n- transformations of $\\theta$\n\n\n\n## Blocked Metropolis-Hastings {.smaller}\n\nSo far all algorithms update all of the parameters simultaneously\n\n  \n\n- convenient to break problems in to $K$ blocks and update them separately\n\n  \n\n- $\\theta = (\\theta_{[1]}, \\ldots, \\theta_{[K]}) = (\\theta_1, \\ldots, \\theta_p)$\n\n\n\n  \n\n- At iteration $s$, for $k = 1, \\ldots, K$ Cycle thru blocks: (fixed order or random order)\n\n    - propose $\\theta^*_{[k]} \\sim q_k(\\theta_{[k]} \\mid \\theta_{[<k]}^{(s)}, \\theta_{[>k]}^{(s-1)})$\n   \n    - set $\\theta_{[k]}^{(s)} = \\theta^*_{[k]}$ with probability\n   $$\\min \\left\\{ 1, \\frac{\n   \\pi(\\theta_{[<k]}^{(s)},\\theta_{[k]}^*, \n        \\theta_{[>k]}^{(s-1)})\n   \\cal{L}(\\theta_{[<k]}^{(s)},\\theta_{[k]}^*,\n           \\theta_{[>k]}^{(s-1)})/\n   q_k(\\theta_{[k]}^* \\mid \\theta_{[<k]}^{(s)},    \n       \\theta_{[>k]}^{(s-1)})}\n  {\\pi(\\theta_{[<k]}^{(s)},\\theta_{[k]}^{(s-1)}, \n        \\theta_{[>k]}^{(s-1)})\n   \\cal{L}(\\theta_{[<k]}^{(s)},\\theta_{[k]}^{(s-1)},\n           \\theta_{[>k]}^{(s-1)})/\n   q_k(\\theta_{[k]}^{(s-1)} \\mid \\theta_{[<k]}^{(s)},    \n       \\theta_{[>k]}^{(s-1)})} \\right\\}$$\n \n## Gibbs Sampler {.smaller}\n\n- The Gibbs Sampler is special case of Blocked MH\n\n\n\n- proposal distribution $q_k$ for the $k$th block is the **full conditional** distribution for $\\theta_{[k]}$\n$$\\begin{split}\n\\pi(\\theta_{[k]} \\mid \\theta_{[-k]}, y) & = \\frac{\\pi(\\theta_{[k]} , \\theta_{[-k]} \\mid y)}{ \\pi(\\theta_{[-k]} \\mid y))} \\propto \\pi(\\theta_{[k]} , \\theta_{[-k]} \\mid y)\\\\\n\\   & \\propto \\cal{L}(\\theta_{[k]} , \\theta_{[-k]})\\pi(\\theta_{[k]} , \\theta_{[-k]})\n\\end{split}$$\n  \n\n- Acceptance probability \n$$\\min \\left\\{ 1, \\frac{\n   \\pi(\\theta_{[<k]}^{(s)},\\theta_{[k]}^*, \n        \\theta_{[>k]}^{(s-1)})\n   \\cal{L}(\\theta_{[<k]}^{(s)},\\theta_{[k]}^*,\n           \\theta_{[>k]}^{(s-1)})/\n   q_k(\\theta_{[k]}^* \\mid \\theta_{[<k]}^{(s)},    \n       \\theta_{[>k]}^{(s-1)})}\n  {\\pi(\\theta_{[<k]}^{(s)},\\theta_{[k]}^{(s-1)}, \n        \\theta_{[>k]}^{(s-1)})\n   \\cal{L}(\\theta_{[<k]}^{(s)},\\theta_{[k]}^{(s-1)},\n           \\theta_{[>k]}^{(s-1)})/\n   q_k(\\theta_{[k]}^{(s-1)} \\mid \\theta_{[<k]}^{(s)},    \n       \\theta_{[>k]}^{(s-1)})} \\right\\}$$\n       \n  \n\n- Simplifies so that acceptance probability is always 1!\n\n  \n\n- even though joint distribution is messy, full conditionals may be (conditionally) conjugate and easy to sample from!\n\n\n \n##  Univariate Normal Example {.smaller}\n\nModel\n$$\\begin{align*}\nY_i \\mid \\mu, \\sigma^2 & \\overset{iid}{\\sim} \\textsf{N}(\\mu, 1/\\phi) \\\\\n\\mu & \\sim \\textsf{N}(\\mu_0, 1/\\tau_0) \\\\\n\\phi & \\sim  \\textsf{Gamma}(a/2, b/2)\n\\end{align*}$$\n\n  \n-  Joint prior is a product of independent  Normal-Gamma\n\n  \n-  Is  $\\pi(\\mu, \\phi \\mid y_1, \\ldots, y_n)$ also a Normal-Gamma family?\n\n\n\n \n## Full Conditional for the  Mean {.smaller}\n\nThe full conditional distributions  $\\mu \\mid \\phi, y_1, \\ldots, y_n$ \n$$\\begin{align*}\n\\mu & \\mid \\phi, y_1, \\ldots, y_n \\sim \\textsf{N}(\\hat{\\mu}, 1/\\tau_n) \\\\\n\\hat{\\mu} & = \\frac{\\tau_0 \\mu_0  + n \\phi \\bar{y}}{\\tau_0 + n \\phi} \\\\\n\\tau_n & = \\tau_0 + n \\phi\n\\end{align*}$$\n  \n  \n  \n \n## Full Conditional for the Precision {.smaller}\n\n- Full conditional for $\\phi$\n$$\\begin{align*}\n\\phi  \\mid \\mu, y_1, \\ldots, y_n & \\sim \\textsf{Gamma}( a_n/2, b_n/2) \\\\\na_n & = a + n \\\\\nb_n & = b + \\sum_i (y_i - \\mu)^2\n\\end{align*}$$\n\n . . . \n\n$$\\textsf{E}[\\phi \\mid \\mu, y_1, \\ldots, y_n] = \\frac{(a + n)/2}{(b + \\sum_i (y_i - \\mu)^2 )/2}$$\n\n  \n- What happens with a non-informative prior  i.e\n$a = b = \\epsilon$ as $\\epsilon \\to 0$?\n\n . . .\n  \n::: {.callout-warning}\nProper full conditionals  with improper priors do not ensure proper joint posterior!\n:::\n \n## Normal Linear Regression Example {.smaller}\n\n- Model\n$$\\begin{align*}\nY_i \\mid \\beta, \\phi & \\overset{iid}{\\sim} \\textsf{N}(x_i^T\\beta, 1/\\phi) \\\\\nY \\mid \\beta, \\phi & \\sim \\textsf{N}(X \\beta, \\phi^{-1} I_n) \\\\\n\\beta & \\sim \\textsf{N}(b_0, \\Phi_0^{-1}) \\\\\n\\phi & \\sim \\textsf{N}(v_0/2, s_0/2)\n\\end{align*}$$\n\n- $x_i$ is a $p \\times 1$ vector of predictors and $X$ is $n \\times p$ matrix\n\n  \n\n- $\\beta$ is a $p \\times 1$ vector of coefficients\n\n  \n\n- $\\Phi_0$ is a $p \\times p$ prior precision matrix \n\n  \n\n- Multivariate Normal density for $\\beta$\n$$\\pi(\\beta \\mid b_0, \\Phi_0) = \\frac{|\\Phi_0|^{1/2}}{(2 \\pi)^{p/2}}\\exp\\left\\{- \\frac{1}{2}(\\beta - b_0)^T \\Phi_0 (\\beta - b_0)  \\right\\}$$\n \n## Full Conditional for $\\beta$ {.smaller}\n\n$$\\begin{align*}\n\\beta & \\mid \\phi, y_1, \\ldots, y_n \\sim \\textsf{N}(b_n, \\Phi_n^{-1}) \\\\\nb_n & =  (\\Phi_0 + \\phi X^TX)^{-1}(\\Phi_0 b_0  +  \\phi X^TX \\hat{\\beta})\\\\\n\\Phi_n & = \\Phi_0 + \\phi X^TX\n\\end{align*}$$\n\n \n## Derivation continued {.smaller}\n\n\n \n## Full Conditional for $\\phi$ {.smaller}\n\n$$ \\phi \\mid \\beta, y_1, \\ldots, y_n \\sim \\textsf{Gamma}((v_0 + n)/2, (s_0 + \\sum_i(y_i - x^T_i \\beta)))$$\n\n\n\n \n##  Choice of Prior Precision {.smaller}\n\n- Non-Informative $\\Phi_0 \\to 0$ \n\n  \n\n- Formal Posterior given $\\phi$\n  $$\\beta \\mid \\phi, y_1, \\ldots, y_n \\sim \\textsf{N}(\\hat{\\beta}, \\phi^{-1} (X^TX)^{-1})$$\n  \n  \n\n- needs $X^TX$ to be full rank for distribution to be unique\n\n \n## Invariance and Choice of Mean/Precision {.smaller}\n\n-  the model in vector form\n$$ Y \\sim \\textsf{N}_n (X\\beta, \\phi^{-1} I_n)$$\n\n  \n\n- What if we transform the $X$ matrix by $\\tilde{X} = X H$ where $H$ is $p \\times p$ and invertible\n\n  \n\n- obtain the posterior for $\\tilde{\\beta}$ using $Y$ and $\\tilde{X}$  \n$$ Y \\sim \\textsf{N}_n (\\tilde{X}\\tilde{\\beta}, \\phi^{-1} I_n)$$\n\n- since $\\tilde{X} \\tilde{\\beta} = X H  \\tilde{\\beta} = X \\beta$  invariance suggests that the posterior for $\\beta$ and $H \\tilde{\\beta}$ should be the same \n\n  \n- or the posterior of $H^{-1} \\beta$\nand $\\tilde{\\beta}$ should be the same\n\n  \n\n- with some linear algebra we can show that this is true if $b_0 = 0$ and $\\Phi_0$ is $k X^TX$ for some $k$  (show!)\n\n \n## Zellner's g-prior {.smaller}\n\n- Popular choice is to take $k = \\phi/g$ which is a special case of Zellner's g-prior\n$$\\beta \\mid \\phi, g \\sim \\textsf{N}\\left(0, \\frac{g}{\\phi} (X^TX)^{-1}\\right)$$\n\n  \n\n- Full conditional \n$$\\beta \\mid \\phi, g \\sim \\textsf{N}\\left(\\frac{g}{1 + g} \\hat{\\beta}, \\frac{1}{\\phi} \\frac{g}{1 + g} (X^TX)^{-1}\\right)$$\n  \n\n- one parameter $g$ controls shrinkage\n\n  \n\n- if $\\phi \\sim \\textsf{Gamma}(v_0/2, s_0/2)$ then posterior is\n$$\\phi \\mid y_1, \\ldots, y_n \\sim \\textsf{Gamma}(v_n/2, s_n/2)$$\n  \n\n- Conjugate so we could skip Gibbs sampling and sample directly from gamma and then conditional normal!\n\n \n## Ridge Regression  {.smaller}\n\n- If $X^TX$ is nearly singular, certain  elements of $\\beta$ or (linear combinations of $\\beta$) may have huge variances under the $g$-prior (or flat prior) as the MLEs are highly unstable!\n\n  \n\n- **Ridge regression** protects against the explosion of variances and ill-conditioning with the conjugate priors:\n$$\\beta \\mid \\phi \\sim \\textsf{N}(0, \\frac{1}{\\phi \\lambda} I_p)$$\n  \n\n- Posterior for $\\beta$  (conjugate case)\n$$\\beta \\mid \\phi, \\lambda, y_1, \\ldots, y_n \\sim \n\\textsf{N}\\left((\\lambda I_p + X^TX)^{-1} X^T Y,  \\frac{1}{\\phi}(\\lambda I_p + X^TX)^{-1}\n\\right)$$\n\n\n\n\n \n##  Bayes Regression {.smaller}\n\n- Posterior mean (or mode) given $\\lambda$ is biased, but can show that there **always** is a value of $\\lambda$  where the frequentist's expected squared error loss is smaller for the Ridge estimator than MLE!\n\n  \n\n- related to penalized maximum likelihood estimation \n\n  \n\n-  Choice of $\\lambda$\n\n  \n\n-  Bayes Regression and choice of $\\Phi_0$ in general is a very important problem and provides the foundation  for many variations on shrinkage estimators, variable selection, hierarchical models, nonparameteric regression and more!\n\n  \n\n- Be sure that you can derive the full conditional posteriors for $\\beta$ and $\\phi$ as well as the joint posterior in the conjugate case!\n\n\n \n## Comments {.smaller}\n\n\n\n- Why don't we treat each individual $\\beta_j$ as a separate block?\n\n  \n\n-  Gibbs always accepts, but can mix slowly if parameters in different blocks are highly correlated!\n\n  \n\n- Use block sizes in Gibbs that are as big as possible to improve  mixing (proven faster convergence)\n\n  \n\n- Collapse the sampler by integrating out as many parameters as possible  (as long as resulting sampler has good mixing)\n\n  \n\n- can use Gibbs steps and (adaptive) Metropolis Hastings steps together\n\n  \n\n\n-  Introduce latent variables (data augmentation) to allow Gibbs steps  (Next class)\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}