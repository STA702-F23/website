{
  "hash": "868e69988614f7c235e99bc758ec100c",
  "result": {
    "markdown": "---\ntitle: \"The Normal Model & Prior/Posterior Predictive Distributions\"\nsubtitle: \"STA 702: Lecture 3\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Outline\n\n\n- Normal Model\n\n\n- Predictive Distributions\n\n\n - Prior Predictive; useful for prior elicitation \n  \n\n- Posterior Predictive; predicting/forecasting future events\n  \n\n- Comparing Estimators\n  \n \n## Normal Model Setup {.smaller}\n\n- Suppose we have independent observations \n$$\\mathbf{y} = (y_1,y_2,\\ldots,y_n)^T$$ where each $Y_i  \\mid \\theta, \\sigma^2 \\stackrel{iid}{\\sim} \\textsf{N}(\\theta, \\sigma^2)$ \n\n  \n\n- We will see that it is more convenient to work with $\\tau = 1/\\sigma^2$   (precision)\n\n\n- reparameterizing the model for the data we have\n$$Y_i \\mid \\theta, \\tau  \\sim \\mathcal{N}(\\theta, \\tau^{-1})$$\n  \n\n- for simplicity  we will treat $\\tau$ as known initially.  \n\n\n\n- Need to specify a prior for $\\theta$ on $\\mathbb{R}$ \n \n\n## Prior for a Normal Mean {.smaller}\n\n- Natural choice is a Normal/Gaussian distribution  (Conjugate prior)\n$$\\theta \\sim \\textsf{N}(\\theta_0, 1/\\tau_0)$$\n  \n\n- $\\theta_0$ is the prior mean - best guess for $\\theta$ using information other than $\\mathbf{y}$\n\n  \n- $\\tau_0$ is the prior precision and expresses our certainty about this guess\n\n  \n\n- one notion of non-informative is to let $\\tau_0 \\to 0$\n \n  \n\n- better justification is as Jeffreys' prior (uniform measure)  \n$\\pi(\\theta) \\propto 1$\n\n- parameterization invariant and invariant to location/scale changes in the data  (group invariance)\n\n. . .\n\n::: {.callout-important title=\"Exercise for the Energetic Student\"}\nYou should be able to derive Jeffreys prior!\n:::\n \n## Posterior Distribution (1 observaton) {.smaller}\n\n- Posterior  $$p(\\theta \\mid y) \\propto \\exp\\left\\{- \\frac 1 2 [\\tau (y - \\theta) ^2 + \\tau_0(\\theta - \\theta_0) ^2 \\right\\} \\, d\\theta$$\n\n- Quadratic in exponential term:\n$\\tau_0(\\theta - \\theta_0)^2 = \\tau_0 \\theta^2 - 2 \\tau_0 \\theta_0 \\theta + \\tau_0 \\theta_0^2$\n\n    - Expand quadratics, regroup and read off precision from  quadtric term in $\\theta$ and mean from linear term in $\\theta$\n\n\n\n- posterior precision is the sum of prior precision and data precision $\\tau_0 + \\tau$\n\n\n- posterior mean $\\hat{\\theta} = \\frac{\\tau_0} {\\tau_0 + \\tau} \\theta_0 + \\frac{\\tau}{\\tau_0 + \\tau} y$; precision weighted average of prior mean and MLE\n\n  \n\n\n\n - conjugate family updating\n$\\theta \\mid y \\sim \\textsf{N} \\left(\\hat{\\theta}, \\frac{1}{\\tau_0 + \\tau} \\right)$\n\n \n## Marginal  Distribution {.smaller}\n\n- Recall that the [**marginal distribution**]{style=\"color:red\"} is\n$$p({y}) = p(y_1,\\ldots,y_n) = \\int_\\Theta p(y_1,\\ldots,y_n \\mid \\theta) \\pi(\\theta)\\, d\\theta$$\n\n  \n\n- this is also called the [**prior predictive**]{style=\"color:red\"} distribution and is independent of any unknown parameters\n\n\n- We may care about making predictions before we even see any data.\n\n  \n\n- This is often useful as a way to see if the sampling distribution or prior we have chosen is appropriate, after integrating out all unknown parameters.\n\n\n  \n\n\n\n  \n \n## Prior Predictive  for a Single Case {.smaller}\n\n$$\\begin{split} p(y) & \\propto \\int_\\mathbb{R} p(y \\mid \\theta) \\pi(\\theta) \\, d\\theta \\\\\n& \\propto \\int_\\mathbb{R}\\exp\\left\\{- \\frac 1 2 \\tau (y - \\theta) ^2 \\right\\} \\exp\\left\\{-  \\frac 1 2 \\tau_0(\\theta - \\theta_0) ^2 \\right\\} \\, d\\theta\n\\end{split}$$\n\n . . . \n\n\n  \n:::: {.columns}\n::: {.column width=\"50%\"}\nIntegration\n\n1)  [**Expand**]{style=\"color:red\"} quadratics  in exp terms\n\n  \n\n2)  [**Group**]{style=\"color:red\"} terms with $\\theta^2$ and $\\theta$\n\n  \n\n3) Read off  [**posterior precision**]{style=\"color:red\"} and  \n[**posterior mean**]{style=\"color:red\"}\n\n  \n\n4)  [**Complete the square**]{style=\"color:red\"} \n\n  \n\n5)  [**Integrate**]{style=\"color:red\"} out $\\theta$ to obtain marginal!\n\n:::\n\n\n\n::: {.column width=\"50%\"}\n1) Linear combinations of Normals are Normal!\n$$Y \\stackrel{D}{=} \\theta + \\epsilon, \\quad \\epsilon \\sim N(0, 1/\\tau) \\quad \\theta \\sim N(\\theta_0, 1/\\tau_0)$$\n\n2) Find Mean of sum\n\n3) Find Variance of sum\n\n4) Marginal $Y \\sim  N(\\theta_0, 1/\\tau_0 + 1/\\tau)$\n:::\n\n::::\n\n## Prior Predictive {.smaller}\n\n- marginal distribution for $Y$ (prior predictive)\n$$Y \\sim \\textsf{N}\\left(\\theta_0, \\frac{1}{\\tau_0} + \\frac{1}{\\tau}\\right) \\text{ or } \\textsf{N}(\\theta_0, \\sigma^2 + \\sigma^2_0)$$\n\n  \n\n- two sources of variability:  data variability  and prior variability\n\n\n\n\n- useful to think about observable quantities when choosing the prior\n\n  \n\n- sample directly  from the prior predictive and assess whether the samples are consistent with our prior knowledge\n\n  \n\n- if not, go back and  modify the prior & repeat\n\n  \n\n- sequential substitution sampling  (repeat $T$ times)\n\n  1) draw $\\theta^{(t)} \\sim \\pi(\\theta)$\n\n  2) draw $y^{(t)} \\mid \\theta^{(t)} \\sim p(y \\mid \\theta^{(t)})$\n\n\n  \n\n- takes into account uncertain about $\\theta$ and variability in $Y$!\n\n\n \n## Posterior Updating {.smaller}\n\n\n\n- Sequential updating using the previous result as our prior!\n\n  \n\n- New prior after seeing 1 observation is $$\\textsf{N}(\\theta_1, 1/\\tau_1)$$\n\n  \n\n- prior mean weighted average\n  $$\\theta_1 \\equiv \\frac{\\tau_0 \\theta_0 + \\tau y_1}{\\tau_0 + \\tau_1}$$\n  \n  \n\n- prior precision after 1 observation\n  $$\\tau_1 \\equiv \\tau_0 + \\tau$$\n\n  \n\n\n- prior variance is now $\\sigma^2_1 = 1/\\tau_1$\n\n \n## Posterior Predictive for $y_2$ given $y_1$  {.smaller}\n\n- Conditional $p(y_2 \\mid y_1) = p(y_2, y_1)/p(y_1)$   (Hard way!)\n\n  \n\n- Use latent variable representation\n$$p(y_2 \\mid y_1) = \\int_\\Theta \\frac{p(y_2, \\mid \\theta) p( y_1 \\mid \\theta ) \\pi(\\theta) \\, d\\theta}{p(y_1)}$$\n  \n\n- simplify to previous problem and use results\n$$p(y_2 \\mid y_1) =  \\int_\\Theta p(y_2 \\mid \\theta) \\pi(\\theta \\mid y_1) \\, d\\theta$$\n  \n\n- (Posterior) Predictive\n$$Y_2 \\mid y_1 \\sim \\textsf{N}(\\theta_1, \\sigma^2 + \\sigma^2_1)$$\n \n## Iterated Expectations  {.smaller}\n\nBased on expressions we have an exponential of a quadratic in $y_2$ so know that distribution is Gaussian\n\n  \n\n- Find the mean and variance using iterated expectations:\n\n  \n\n- mean \n$$\\textsf{E}[Y_2 \\mid y_1] = \\textsf{E}_{\\theta \\mid y_1}[\\textsf{E}_{Y_2 \\mid y_1, \\theta} [Y_2 \\mid y_1, \\theta] \\mid y_1]$$\n\n- Conditional Variance $\\textsf{Var}[Y_2  \\mid y_1]$\n\n- Iterated expectations (prove!)\n$$\\textsf{Var}[Y_2  \\mid y_1] = \\textsf{E}_{\\theta \\mid y_1}[\\textsf{Var}_{Y_2 \\mid y_1, \\theta} [Y_2 \\mid y_1, \\theta] \\mid y_1] + \\textsf{Var}_{\\theta \\mid y_1}[\\textsf{E}_{Y_2 \\mid y_1, \\theta} [Y_2 \\mid y_1, \\theta] \\mid y_1]$$\n\n\n \n## Updated Posterior for $\\theta$ {.smaller}\n\n$$p(\\theta \\mid y_1, y_2) \\propto p(y_2 \\mid \\theta) p(y_1 \\mid \\theta) \\pi(\\theta)$$\n\n  \n\n$$p(\\theta \\mid y_1, y_2) \\propto  p(y_2 \\mid \\theta) p(\\theta \\mid y_1)$$\n\n. . .\n\n\nApply previous updating rules\n\n  \n\n- new posterior mean \n$$\\theta_2 = \\frac{\\tau_1 \\theta_1  + \\tau y_2}{\\tau_1 + \\tau} = \\frac{\\tau_0 \\theta_0 + 2 \\tau \\bar{y}}\n{\\tau_0 + 2 \\tau}$$\n  \n\n- new precision\n$$ \\tau_2 = \\tau_1 + \\tau = \\tau_0 + 2 \\tau$$\n\n \n## After $n$ observations  {.smaller}\n\n- Posterior for $\\theta$ \n$$\\theta \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\frac{\\tau_0 \\theta_0 + n \\tau \\bar{y}}\n{\\tau_0 + n \\tau}, \\frac{1}{ \\tau_0 + n \\tau} \\right)$$\n\n  \n\n- Posterior Predictive Distribution for $Y_{n+1}$\n$$Y_{n+1} \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\frac{\\tau_0 \\theta_0 + n \\tau \\bar{y}}\n{\\tau_0 + n \\tau}, \\frac{1}{\\tau} + \\frac{1}{ \\tau_0 + n \\tau} \\right)$$\n\n  \n\n- Shrinkage of the MLE to the prior mean\n\n  \n\n- More accurate estimation of $\\theta$ as $n \\to \\infty$ (reducible error)\n\n  \n\n- Cannot reduce the error for prediction $Y_{n+1}$ due to $\\sigma^2$\n\n  \n\n- predictive distribution for a next observation given _everything_ we know - prior and likelihood\n\n \n## Results with Jeffreys' Prior {.smaller}\n - What if $\\tau_0 \\to 0$? (or $\\sigma^2_0 \\to \\infty$)\n \n  \n\n- Prior predictive $\\textsf{N}(\\theta_0, \\sigma^2_0 + \\sigma^2 )$  (not  proper in the limit)\n\n  \n\n- Posterior for $\\theta$ (formal posterior)\n$$\\theta \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\frac{\\tau_0 \\theta_0 + n \\tau \\bar{y}}\n{\\tau_0 + n \\tau}, \\frac{1}{ \\tau_0 + n \\tau} \\right)$$\n\n . . . \n\n$$\\to  \\qquad \\theta \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\bar{y}, \n \\frac{1}{n \\tau} \\right)$$\n  \n\n- Recovers the MLE as the posterior mode!\n\n  \n\n- Posterior variance of $\\theta = \\sigma^2/n$ (same as variance of the MLE)\n \n\n \n## Posterior Predictive Distribution {.smaller}\n\n- Posterior predictive distribution for $Y_{n+1}$\n$$Y_{n+1} \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\frac{\\tau_0 \\theta_0 + n \\tau \\bar{y}}\n{\\tau_0 + n \\tau}, \\frac{1}{\\tau} + \\frac{1}{ \\tau_0 + n \\tau} \\right)$$\n\n  \n\n- Under Jeffreys' prior\n$$Y_{n+1} \\mid y_1, \\ldots, y_n \\sim \\textsf{N}\\left( \\bar{y}, \\sigma^2 (1 + \\frac{1}{n} )\\right)$$\n  \n\n- Captures extra uncertainty due to not knowing $\\theta$ (compared to plug-in approach where we plug in MLE in sampling model!\n\n \n## Comparing Estimators {.smaller}\n\n- Expected loss (from frequentist perspective) of using Bayes Estimator\n\n  \n\n- Posterior mean is optimal under squared error loss (min Bayes Risk)  [also absolute error loss]\n\n  \n\n- Compute Mean Square Error (or Expected Average Loss)\n$$\\textsf{E}_{\\bar{y} \\mid \\theta}\\left[\\left(\\hat{\\theta} - \\theta \\right)^2 \\mid \\theta \\right]$$\n\n . . .\n\n $$ = \\textsf{Bias}(\\hat{\\theta})^2 + \\textsf{Var}(\\hat{\\theta})$$\n  \n\n- For the MLE $\\bar{Y}$ this is just the variance of $\\bar{Y}$ or $\\sigma^2/n$\n\n \n## MSE for Bayes {.smaller}\n\n- Frequentist Risk\n$$\\textsf{E}_{\\bar{y} \\mid \\theta}\\left[\\left(\\hat{\\theta} - \\theta \\right)^2 \\mid \\theta \\right] = \\textsf{MSE} =  \\textsf{Bias}(\\hat{\\theta})^2 + \\textsf{Var}(\\hat{\\theta})$$\n\n- Bias of Bayes Estimate\n$$\\textsf{E}_{\\bar{Y} \\mid \\theta}\\left[ \\frac{\\tau_0 \\theta_0 + \\tau n \\bar{Y}}\n{\\tau_0  + \\tau n}\\right] = \n\\frac{\\tau_0(\\theta_0 - \\theta)}{\\tau_0 + \\tau n}$$\n\n  \n\n- Variance\n$$\\textsf{Var}\\left(\\frac{\\tau_0 \\theta_0 + \\tau n \\bar{Y}}{\\tau_0 + \\tau n} - \\theta  \\mid \\theta \\right)  = \\frac{\\tau n}{(\\tau_0 + \\tau n)^2}$$\n  \n\n- (Frequentist) expected Loss  when truth is $\\theta$\n$$\\textsf{MSE} = \\frac{\\tau_0^2(\\theta - \\theta_0)^2 + \\tau n}{(\\tau_0 + \\tau n)^2}$$\n  \n\n\n\n \n## Plot {.smaller}\n\nBehavior ?\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03-normal-predictive-distributions_files/figure-revealjs/MSE-1.png){fig-align='center' width=75%}\n:::\n:::\n\n\n\n \n## Updating with $n$ Observations {.smaller}\n\n-  Can update sequentially as before -or-\n \n -  We can use the $\\cal{L}(\\theta)$ based on $n$ observations  and repeat completing the square with the original prior  $\\theta \\sim \\textsf{N}(\\theta_0, 1/\\tau_0)$\n\n- same answer!\n\n- The likelihood for $\\theta$ is proportional to the sampling model\n$$p(y \\mid \\theta,\\tau)  = \n\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} \\tau^{\\frac{1}{2}}  \\exp{\\left\\{-\\frac{1}{2} \\tau (y_i-\\theta)^2\\right\\}}$$\n\n\n . . . \n\n::: {.callout-important title=\"Exercise\"}\nRewrite in terms of sufficient statistics!\n:::\n\n## Exercises for Practice\n\n\n\n:::: {.callout-important title=\"Exercise 1\"}\n\nUse $\\cal{L}(\\theta)$  based on $n$ observations and $\\pi(\\theta)$ to find  $\\pi(\\theta \\mid y_1, \\ldots, y_n)$ based on the sufficient statistics\n\n:::\n  \n  . . .\n  \n\n::: {.callout-important title=\"Exercise 2\"}\n\nUse $\\pi(\\theta \\mid y_1, \\ldots, y_n)$ to find the posterior predictive distribution for $Y_{n+1}$\n\n:::\n\n\n\n \n## Simplification {.smaller}\n\n\n$$\\begin{split}\n\\cal{L}(\\theta) & \\propto \\tau^{\\frac{n}{2}} \\ \\exp\\left\\{-\\frac{1}{2} \\tau \\sum_{i=1}^n (y_i-\\theta)^2\\right\\}\\\\\n& \\propto \\tau^{\\frac{n}{2}} \\ \\exp\\left\\{-\\frac{1}{2} \\tau \\sum_{i=1}^n \\left[ (y_i-\\bar{y}) - (\\theta - \\bar{y}) \\right]^2 \\right\\}\\\\\n\\\\\n& \\propto \\tau^{\\frac{n}{2}} \\ \\exp\\left\\{-\\frac{1}{2} \\tau \\left[ \\sum_{i=1}^n (y_i-\\bar{y})^2 + \\sum_{i=1}^n(\\theta - \\bar{y})^2 \\right] \\right\\}\\\\\n& \\propto \\tau^{\\frac{n}{2}} \\ \\exp\\left\\{-\\frac{1}{2} \\tau \\left[ \\sum_{i=1}^n (y_i-\\bar{y})^2 + n(\\theta - \\bar{y})^2 \\right] \\right\\}\\\\\n& \\propto \\tau^{\\frac{n}{2}} \\ \\exp\\left\\{-\\frac{1}{2} \\tau s^2(n-1) \\right\\} \\ \\exp\\left\\{-\\frac{1}{2} \\tau n(\\theta - \\bar{y})^2 \\right\\}\n\\end{split}$$\n\n \n",
    "supporting": [
      "03-normal-predictive-distributions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}