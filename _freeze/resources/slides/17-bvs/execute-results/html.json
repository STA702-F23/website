{
  "hash": "9600e89e4d26db680a52247ce985ba4e",
  "result": {
    "markdown": "---\ntitle: \"Lecture 17: Bayesian Variable Selection and Model Averaging\"\nauthor: \"Merlise Clyde\"\nsubtitle: \"STA702\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## Diabetes Example\n\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\text{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(8675309)\nsource(\"yX.diabetes.train.txt\")\ndiabetes.train = as.data.frame(diabetes.train)\nsource(\"yX.diabetes.test.txt\")\ndiabetes.test = as.data.frame(diabetes.test)\ncolnames(diabetes.test)[1] = \"y\"\n\nstr(diabetes.train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t342 obs. of  65 variables:\n $ y      : num  -0.0147 -1.0005 -0.1444 0.6987 -0.2222 ...\n $ age    : num  0.7996 -0.0395 1.7913 -1.8703 0.113 ...\n $ sex    : num  1.064 -0.937 1.064 -0.937 -0.937 ...\n $ bmi    : num  1.296 -1.081 0.933 -0.243 -0.764 ...\n $ map    : num  0.459 -0.553 -0.119 -0.77 0.459 ...\n $ tc     : num  -0.9287 -0.1774 -0.9576 0.256 0.0826 ...\n $ ldl    : num  -0.731 -0.402 -0.718 0.525 0.328 ...\n $ hdl    : num  -0.911 1.563 -0.679 -0.757 0.171 ...\n $ tch    : num  -0.0544 -0.8294 -0.0544 0.7205 -0.0544 ...\n $ ltg    : num  0.4181 -1.4349 0.0601 0.4765 -0.6718 ...\n $ glu    : num  -0.371 -1.936 -0.545 -0.197 -0.979 ...\n $ age^2  : num  -0.312 -0.867 1.925 2.176 -0.857 ...\n $ bmi^2  : num  0.4726 0.1185 -0.0877 -0.6514 -0.2873 ...\n $ map^2  : num  -0.652 -0.573 -0.815 -0.336 -0.652 ...\n $ tc^2   : num  -0.091 -0.6497 -0.0543 -0.6268 -0.6663 ...\n $ ldl^2  : num  -0.289 -0.521 -0.3 -0.45 -0.555 ...\n $ hdl^2  : num  -0.0973 0.8408 -0.3121 -0.2474 -0.5639 ...\n $ tch^2  : num  -0.639 -0.199 -0.639 -0.308 -0.639 ...\n $ ltg^2  : num  -0.605 0.78 -0.731 -0.567 -0.402 ...\n $ glu^2  : num  -0.578 1.8485 -0.4711 -0.6443 -0.0258 ...\n $ age:sex: num  0.69 -0.139 1.765 1.609 -0.284 ...\n $ age:bmi: num  0.852 -0.142 1.489 0.271 -0.271 ...\n $ age:map: num  0.0349 -0.3346 -0.5862 1.1821 -0.3025 ...\n $ age:tc : num  -0.978 -0.246 -1.927 -0.72 -0.244 ...\n $ age:ldl: num  -0.803 -0.203 -1.504 -1.2 -0.182 ...\n $ age:hdl: num  -0.7247 0.0147 -1.2661 1.6523 0.1046 ...\n $ age:tch: num  -0.254 -0.176 -0.31 -1.598 -0.216 ...\n $ age:ltg: num  0.0644 -0.2142 -0.163 -1.1657 -0.3474 ...\n $ age:glu: num  -0.636 -0.239 -1.359 0.071 -0.438 ...\n $ sex:bmi: num  1.304 0.935 0.915 0.142 0.635 ...\n $ sex:map: num  0.258 0.289 -0.381 0.5 -0.697 ...\n $ sex:tc : num  -1.02 0.131 -1.051 -0.274 -0.112 ...\n $ sex:ldl: num  -0.927 0.236 -0.913 -0.638 -0.452 ...\n $ sex:hdl: num  -0.647 -1.188 -0.377 1.189 0.238 ...\n $ sex:tch: num  -0.411 0.47 -0.411 -1.062 -0.296 ...\n $ sex:ltg: num  0.2988 1.2093 -0.0866 -0.6032 0.4857 ...\n $ sex:glu: num  -0.6171 1.6477 -0.8069 -0.0239 0.7283 ...\n $ bmi:map: num  0.189 0.191 -0.477 -0.195 -0.702 ...\n $ bmi:tc : num  -1.5061 -0.0595 -1.1853 -0.3231 -0.3239 ...\n $ bmi:ldl: num  -1.267 0.183 -0.976 -0.407 -0.536 ...\n $ bmi:hdl: num  -0.869 -1.41 -0.286 0.586 0.251 ...\n $ bmi:tch: num  -0.505 0.505 -0.484 -0.614 -0.388 ...\n $ bmi:ltg: num  0.1014 1.1613 -0.4085 -0.5893 0.0716 ...\n $ bmi:glu: num  -0.862 1.693 -0.89 -0.337 0.358 ...\n $ map:tc : num  -0.687 -0.148 -0.131 -0.451 -0.21 ...\n $ map:ldl: num  -0.5407 0.0388 -0.1034 -0.6114 -0.036 ...\n $ map:hdl: num  -0.235 -0.672 0.254 0.745 0.252 ...\n $ map:tch: num  -0.29 0.207 -0.258 -0.835 -0.29 ...\n $ map:ltg: num  -0.214 0.428 -0.427 -0.811 -0.748 ...\n $ map:glu: num  -0.541 0.659 -0.314 -0.23 -0.812 ...\n $ tc:ldl : num  -0.144 -0.551 -0.139 -0.509 -0.581 ...\n $ tc:hdl : num  0.8363 -0.3457 0.6304 -0.2579 -0.0392 ...\n $ tc:tch : num  -0.405 -0.326 -0.404 -0.295 -0.451 ...\n $ tc:ltg : num  -0.901 -0.259 -0.571 -0.392 -0.569 ...\n $ tc:glu : num  0.0202 0.0196 0.2073 -0.396 -0.4283 ...\n $ ldl:hdl: num  0.889 -0.446 0.705 -0.207 0.26 ...\n $ ldl:tch: num  -0.463 -0.243 -0.463 -0.21 -0.506 ...\n $ ldl:ltg: num  -0.6536 0.2724 -0.3783 -0.0708 -0.5638 ...\n $ ldl:glu: num  -0.0194 0.4995 0.1032 -0.4013 -0.6234 ...\n $ hdl:tch: num  0.703 -0.5 0.692 0.171 0.651 ...\n $ hdl:ltg: num  0.0179 -1.9846 0.3839 0.0399 0.3043 ...\n $ hdl:glu: num  0.654 -2.948 0.689 0.452 0.113 ...\n $ tch:ltg: num  -0.592 0.531 -0.574 -0.253 -0.537 ...\n $ tch:glu: num  -0.371 1.114 -0.362 -0.522 -0.34 ...\n $ ltg:glu: num  -0.584 2.184 -0.468 -0.526 0.183 ...\n```\n:::\n:::\n\n\n## MCMC with BAS\n\n\n::: {.cell layout-align=\"center\" hash='17-bvs_cache/revealjs/MCMC_87d8d5704d33389212317393d9288ca6'}\n\n```{.r .cell-code}\nlibrary(BAS)\ndiabetes.bas = bas.lm(y ~ ., data=diabetes.train,\n                      prior = \"JZS\",\n                      method=\"MCMC\",\n                      n.models = 10000,\n                      MCMC.iterations=500000,\n                      thin = 10,\n                      initprobs=\"eplogp\",\n                      force.heredity=FALSE)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='17-bvs_cache/revealjs/MCMCtime_11f174715ebb9ac151b6876433c1ad29'}\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 19.523   0.951  20.530 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"number of unique models 5008\"\n```\n:::\n:::\n\n\n- increase `MCMC.iterations`?\n\n- check diagnostics\n\n## Estimates of Posterior Probabilities\n\n- relative frequencies  $\\hat{P}_{RF}(\\g \\mid \\Y) = \\frac{\\text{# times } \\g \\in S }{S}$\n\n    - Ergodic average converges to $p(\\g \\mid \\Y)$ as $S \\to \\infty$\n\n    - asymptoptically unbaised\n\n- renormalized posterior probabilities \n$\\hat{P}_{RN}(\\g \\mid \\Y) = \\frac{p(\\Y \\mid \\g) p(\\g)} {\\sum_{\\g \\in S} p(\\Y \\mid \\g) p(\\g)}$\n\n    - also asymptoptically unbaised\n\n    - Fisher consistent (e.g if we happen to enumerate all models in $S$ iterations we recover the truth)\n\n- if we run long enough the two should agree\n\n- also look at other summaries i.e posterior inclusion probabilities \n$$\\hat{p}(\\gamma_j = 1 \\mid \\Y) = \\sum_S \\gamma_j \\hat{P}(\\g \\mid \\Y)$$\n\n## Diagnostic Plot\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndiagnostics(diabetes.bas, type=\"pip\")\n```\n\n::: {.cell-output-display}\n![](17-bvs_files/figure-revealjs/pip-1.png){fig-align='center' width=1800}\n:::\n:::\n\n\n- model probabilities converge much slower!\n\n## Out of Sample Prediction\n\n- What is the optimal value to predict $\\Y^{\\text{test}}$ given $\\Y$ under squared error?\n\n- Iterated expectations leads to BMA for $\\E[\\Y^{\\text{test}} \\mid \\Y]$\n\n- Prediction under model averaging\n$$\\hat{Y} = \\sum_S (\\hat{\\alpha}_\\g + \\Xg^{\\text{test}} \\hat{\\b}_{\\g}) \\hat{p}(\\g \\mid \\Y)$$\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred.bas = predict(diabetes.bas,\n                   newdata=diabetes.test,\n                   estimator=\"BMA\",\n                   se=TRUE)\nmean((pred.bas$fit- diabetes.test$y)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4558026\n```\n:::\n:::\n\n\n## Credible Intervals & Coverage\n\n- posterior predictive distribution \n$$\np(\\y^{\\text{test}} \\mid \\y) = \\sum_\\g p(\\y^{\\text{test}} \\mid \\y, \\g)p(\\g \\mid \\y)\n$$\n- integrate out $\\alpha$ and $\\bg$ to get a normal predictive given $\\phi$ and $\\g$ (and $\\y$)\n\n- integrate out $\\phi$ to get a t distribution given $\\g$ and $\\y$\n\n- credible intervals via sampling \n\n    - sample a model from $p(\\g \\mid \\y)$\n    - conditional on a model sample $y \\sim p(\\y^{\\text{test}} \\mid \\y, \\g)$\n    - compute quantiles from sammple $y$\n    \n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nci.bas = confint(pred.bas);\ncoverage = mean(diabetes.test$y > ci.bas[,1] & diabetes.test$y < ci.bas[,2])\ncoverage\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.99\n```\n:::\n:::\n\n\n## 95% Prediction intervals\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ci.bas)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n\n```{.r .cell-code}\npoints(diabetes.test$y, col=2, pch=15)\n```\n\n::: {.cell-output-display}\n![](17-bvs_files/figure-revealjs/pred-in-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n## Selection and Prediction\n\n- BMA  - optimal for squared error loss Bayes\n$$\\E[\\| \\Y^{\\text{test}} - a\\|^2 \\mid \\y] = \\E[\\| \\Y^{\\text{test}} - \\E[\\Y^{\\text{test}}\\mid \\y] \\|^2 \\mid \\y]  + \\| \\E[\\Y^{\\text{test}}\\mid \\y] - a\\|^2  $$\n\n \n\n- What if we want to use only a single model for prediction under quared error loss?\n\n- HPM: Highest Posterior Probability model is optimal for selection, but not prediction\n\n- MPM: Median Probabilty model (select model where PIP > 0.5)\n (optimal under certain conditions; nested models)\n \n-  BPM: Best Probability Model - Model closest to BMA under loss\n      (usually includes more predictors than HPM or MPM)\n\n## Example\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred.bas = predict(diabetes.bas,\n                   newdata=diabetes.test,\n                   estimator=\"BPM\",\n                   se=TRUE)\n#MSE\nmean((pred.bas$fit- diabetes.test$y)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4740667\n```\n:::\n\n```{.r .cell-code}\n#Coverage\nci.bas = confint(pred.bas)\nmean(diabetes.test$y > ci.bas[,1] &\n     diabetes.test$y < ci.bas[,2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.98\n```\n:::\n:::\n\n\n \n## Summary\n\n\n-  Choice of prior on $\\bg$ \n    - orthogonally invariant priors - multivariate Spike & Slab\n    - products of independent Spike & Slab priors\n    - non-semi-conjugate \n    \n-  priors on the models (sensitivity)\n-  computation (MCMC, \"stochastic search\", variational, orthogonal \n   data augmentation, reversible jump-MCMC)\n-  posterior summaries - select a model or \"average\" over all models\n\n.  .  .\n\nOther aspects of model selection?\n\n - transformations of $Y$\n - functions of $X$: interactions or nonlinear functions such as splines kernels\n - choice of error distribution\n ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}