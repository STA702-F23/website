{
  "hash": "9a2fd8c48a92b0c18e442a2d1beddd06",
  "result": {
    "markdown": "---\ntitle: \"Lecture 18: Outliers and Robust Regression\"\nauthor: \"Merlise Clyde\"\nsubtitle: \"STA702\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    smaller: true\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n## Body Fat Data\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\text{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\BF}{\\textsf{BF}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n:::: {.columns}\n::: {.column width=50%}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-outliers_files/figure-revealjs/lm-1.png){fig-align='center' width=2100}\n:::\n:::\n\n:::\n::: {.column width=50%}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-outliers_files/figure-revealjs/lmsub-1.png){fig-align='center' width=2100}\n:::\n:::\n\n\n:::\n::::\n\nWhich analysis do we use?  with Case 39 or not -- or something different?\n\n## Cook's Distance\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(bodyfat.lm, which=5)\n```\n\n::: {.cell-output-display}\n![](18-outliers_files/figure-revealjs/cooksd-1.png){fig-align='center' width=2400}\n:::\n:::\n\n\n## Options for Handling Outliers\n\nWhat are outliers?  \n\n- Are there scientific grounds for eliminating the case?  \n\n- Test if the case  has a different mean than population  \n\n- Report results with and without the case  \n\n- Model Averaging to Account for Model Uncertainty?   \n\n- Full model $\\Y = \\X \\b + \\I_n\\d + \\epsilon$  \n\n- $\\d$ is a $n \\times 1$ vector;  $\\b$ is $p \\times 1$\n\n- All observations have a potentially different mean!\n\n## Outliers in Bayesian Regression\n\n- Hoeting, Madigan and Raftery (in various permutations)\n      consider the problem of simultaneous variable selection and\n      outlier identification\n      \n-   This is implemented in the package `BMA` in the function\n    `MC3.REG`\n\n- This has the advantage that more than 2 points may be considered as\noutliers at the same time\n\n- The function uses a Markov chain to identify both important\n  variables and potential outliers, but is coded in Fortran so should\n  run reasonably quickly. \n\n- Can also use `BAS` or other variable selection programs \n\n## Model Averaging and Outliers  \n\n- Full model $\\Y = \\X \\b + \\I_n\\d + \\epsilon$  \n\n- $\\d$ is a $n \\times 1$ vector;  $\\b$ is $p \\times 1$\n\n- $2^n$ submodels $\\gamma_i = 0 \\Leftrightarrow \\delta_i = 0$\n\n- If $\\gamma_i = 1$ then case $i$ has a different mean ``mean\n  shift'' outliers\n  \n##    Mean Shift $=$ Variance Inflation\n  \n- Model  $\\Y = \\X \\b + \\I_n\\d + \\epsilon$\n\n- Prior \n$$\\begin{align}\n\\qquad \\delta_i \\mid \\gamma_i & \\sim N(0, V \\sigma^2 \\gamma_i) \\\\\n\\qquad \\gamma_i & \\sim  \\Ber(\\pi)\n\\end{align}\n$$\n\n\n\n\n- Then $\\epsilon_i$ given $\\sigma^2$ is independent of $\\delta_i$\nand\n$$\\epsilon^*_i \\equiv \\epsilon_i + \\delta_i \\mid \\sigma^2 \\left\\{\n\\begin{array}{llc}\n  N(0, \\sigma^2) & wp &(1 - \\pi) \\\\\n  N(0, \\sigma^2(1 + V)) & wp & \\pi\n\\end{array}\n\\right.\n$$\n\n- Model  $\\Y = \\X \\b + \\epsilon^*$   **variance inflation**\n\n- $V+1 = K = 7$ in the paper by Hoeting et al. package `BMA`\n\n## Simultaneous Outlier and Variable Selection\n\n\n::: {.cell layout-align=\"center\" hash='18-outliers_cache/revealjs/unnamed-chunk-5_7e01a19c3f6b755fcb5e6f8ec321d4ad'}\n\n```{.r .cell-code}\nlibrary(BMA)\nbodyfat.bma = MC3.REG(all.y = bodyfat$Bodyfat, all.x = as.matrix(bodyfat$Abdomen),\n                      num.its = 10000, outliers = TRUE)\nsummary(bodyfat.bma)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nMC3.REG(all.y = bodyfat$Bodyfat, all.x = as.matrix(bodyfat$Abdomen),     num.its = 10000, outliers = TRUE)\n\nModel parameters: PI = 0.02 K = 7 nu = 2.58 lambda = 0.28 phi = 2.85\n\n  15  models were selected\n Best  5  models (cumulative posterior probability =  0.9939 ): \n\n           prob     model 1   model 2   model 3   model 4   model 5 \nvariables                                                           \n  all.x    1         x         x         x         x         x      \noutliers                                                            \n  39       0.94932   x         x         .         x         .      \n  204      0.04117   .         .         .         x         .      \n  207      0.10427   .         x         .         .         x      \n                                                                    \npost prob           0.814572  0.095383  0.044490  0.035024  0.004385\n```\n:::\n:::\n\n\n## BAS with Truncated Prior\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\" hash='18-outliers_cache/revealjs/bas_e6170320cf421e2fc6cddb065f9189f6'}\n\n```{.r .cell-code}\nbodyfat.w.out = cbind(bodyfat[, c(\"Bodyfat\", \"Abdomen\")],\n                      diag(nrow(bodyfat)))\n\nbodyfat.bas = bas.lm(Bodyfat ~ ., data=bodyfat.w.out, \n                     prior=\"hyper-g-n\", a=3, method=\"MCMC\",\n                     MCMC.it=2^18, \n                     modelprior=tr.beta.binomial(1,254, 50))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-outliers_files/figure-revealjs/basimage-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n## Change Error Assumptions\n\nUse a Student-t error model\n$$\\begin{eqnarray*}\nY_i & \\ind & t(\\nu, \\alpha + \\beta x_i, 1/\\phi) \\\\ \nL(\\alpha, \\beta,\\phi) & \\propto & \\prod_{i = 1}^n \\phi^{1/2} \\left(1 +\n\\frac{\\phi (y_i - \\alpha - \\beta x_i)^2}{\\nu}\\right)^{-\\frac{(\\nu +\n  1)}{2}}\n\\end{eqnarray*}$$\n\n- Use Prior $p(\\alpha, \\beta, \\phi) \\propto 1/\\phi$ \\pause\n\n\n- Posterior distribution\n$$ p(\\alpha, \\beta, \\phi \\mid Y) \\propto \\phi^{n/2 - 1} \\prod_{i = 1}^n  \\left(1 +\n\\frac{\\phi (y_i - \\alpha - \\beta x_i)^2}{\\nu}\\right)^{-\\frac{(\\nu +\n  1)}{2}}$$ \n  \n## Bounded Influence\n\n-  Treat $\\sigma^2$ as given,  then **influence** of individual observations on the posterior distribution of $\\b$  in the model where $\\E[\\Y_i] = \\x_i^T\\b$ is investigated through the score function: \n$$\n\\frac{d} {d \\b} \\log p (\\b \\mid \\Y) = \\frac{d} {d \\b} \\log p(\\b) +  \\sum_{i = 1}^n \\x_i g(y_i - \\x^T_i \\b)\n$$ \n\n- influence function of the error distribution (unimodal, continuous, differentiable, symmetric)\n$$ g(\\eps) = - \\frac{d} {d \\eps} \\log p(\\eps)\n$$\n\n\n- An outlying observation $y_j$ is accommodated if the posterior distribution for $p(\\b \\mid \\Y_{(i)})$ converges to $p(\\b \\mid \\Y)$  for all $\\b$ as $|\\Y_i| \\to \\infty$.   \n\n\n- Requires error models with influence functions that go to zero such as the Student $t$ (O'Hagan, 1979, West 1984)  \n\n## Choice of df for Student-$t$\n\nInvestigate the Score function\n$$\n\\frac{d} {d \\b} \\log p (\\b \\mid \\Y) = \\frac{d} {d \\b} \\log p(\\b) +  \\sum_{i = 1}^n \\x_i g(y_i - \\x^T_i \\b)\n$$ \n\n. . . \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-outliers_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=2700}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n- Score function for $t$ with $\\alpha$ degrees of freedom has turning points at $\\pm \\sqrt{\\alpha}$ \n \n\n\n-  $g'(\\eps)$ is negative when $\\eps^2 > \\alpha$  (standardized errors) \n\n-  Contribution of observation to information matrix is negative and the observation is doubtful \n\n\n\n-  Suggest taking $\\alpha = 8$ or $\\alpha = 9$ to reject errors larger than $\\sqrt{8}$ or $3$ sd.\n\n:::\n::::\n\n## Scale-Mixtures of Normal Representation\n\n- Latent Variable Model\n\\begin{eqnarray*}\n  Y_i \\mid \\alpha, \\beta, \\phi, \\lambda & \\ind & N(\\alpha + \\beta x_i,\n  \\frac{1}{\\phi \\lambda_i}) \\\\\n \\lambda_i & \\iid & G(\\nu/2, \\nu/2) \\\\\n p(\\alpha, \\beta, \\phi) & \\propto & 1/\\phi  \n\\end{eqnarray*}\n\n- Joint Posterior Distribution: \n\\begin{eqnarray*}\np((\\alpha, \\beta, \\phi, \\lambda_1, \\ldots, \\lambda_n \\mid Y)\n  \\propto \\,  & &\n\\phi^{n/2} \\exp\\left\\{ - \\frac{\\phi}{2} \\sum \\lambda_i(y_i - \\alpha  - \\beta x_i)^2 \\right\\} \\times \\\\\n&  & \\phi^{-1} \\\\\n&  &\\prod_{i=1}^n \\lambda_i^{\\nu/2 - 1} \\exp(- \\lambda_i \\nu/2)\n\\end{eqnarray*}\n\n- Integrate out ``latent'' $\\lambda$'s to obtain marginal  $t$ distribution\n\n## JAGS - Just Another Gibbs Sampler \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr.model = function() {\n  df <- 9\n  for (i in 1:n) {\n    mu[i] <- alpha0 + alpha1*(X[i] - Xbar)\n    lambda[i] ~ dgamma(df/2, df/2)\n    prec[i] <- phi*lambda[i]\n    Y[i] ~ dnorm(mu[i], prec[i])\n  }\n  phi ~ dgamma(1.0E-6, 1.0E-6)\n  alpha0 ~ dnorm(0, 1.0E-6)\n  alpha1 ~ dnorm(0,1.0E-6)\n  beta0 <- alpha0 - alpha1*Xbar  # transform back\n  beta1 <- alpha1\n  sigma <- pow(phi, -.5)\n  mu34 <- beta0 + beta1*2.54*34  #mean for a man w/ a 34 in waist\n  y34 ~ dt(mu34,phi, df)   # integrate out lambda_34 \n}\n```\n:::\n\n::: {.callout-warning}\n## Warning!   Normals  and Student-t are parameterized in terms of precisions!\n:::\n\n## What output to Save?\n\nThe parameters to be monitored and returned to `R` are specified with\nthe variable `parameters`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nparameters = c(\"beta0\", \"beta1\", \"sigma\", \"mu34\", \"y34\", \"lambda[39]\")\n```\n:::\n\n\n\n-  Use of `<-` for assignment for parameters that calculated from the other\n  parameters. (See R-code for definitions of these parameters.)  \n  \n-  `mu34` and `y34` are the mean functions and predictions for a man with a 34in waist.\n\n-  `lambda[39]` saves only the 39th case of $\\lambda$ \n\n-   To save a whole vector (for example all lambdas, just give the\n  vector name)\n  \n  \n## Running JAGS from `R`\n\nInstall jags from [sourceforge](https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/)\n\n. . .\n\n\n::: {.cell layout-align=\"center\" hash='18-outliers_cache/revealjs/jagsrun_75068296150fca8ce1a31d12c7af8e05'}\n\n```{.r .cell-code}\nlibrary(R2jags)\n\n# Create a data list with inputs for Winpost/Jags\n\nbf.data = list(Y = bodyfat$Bodyfat, X=bodyfat$Abdomen)\nbf.data$n = length(bf.data$Y)\nbf.data$Xbar = mean(bf.data$X)\n\n# run jags\nbf.sim = jags(bf.data, inits=NULL, par=parameters,\n              model=rr.model, n.chains=2, n.iter=20000)\n```\n:::\n\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create an MCMC object \nlibrary(coda)\nbf.post = as.mcmc(bf.sim$BUGSoutput$sims.matrix)  \n```\n:::\n\n## Posterior Distributions  \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-outliers_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n## Posterior of $\\lambda_{39}$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-outliers_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=2400}\n:::\n:::\n\n\n## Comparison\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n95% Confidence/Credible Intervals for $\\beta$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n|             |     2.5 %|    97.5 %|\n|:------------|---------:|---------:|\n|lm all       | 0.5750739| 0.6875349|\n|robust bayes | 0.6016984| 0.7184886|\n|lm w/out 39  | 0.6144288| 0.7294781|\n:::\n:::\n\n\n- Results intermediate without having to remove any observations!\n\n- Case 39 down weighted by $\\lambda_{39}$ in posterior for $\\beta$\n\n- Under prior $E[\\lambda_{i}] = 1$  \n\n- large residuals lead to smaller $\\lambda$\n$$\\lambda_j \\mid \\text{rest}, Y \\sim G \\left(\\frac{\\nu + 1}{2}, \\frac{\\phi(y_j - \\alpha -\n\\beta x_j)^2 + \\nu}{2} \\right)$$\n\n- \n\n## Prior Distributions on Parameters\n\n- As a general recommendation, the prior distribution should have\n``heavier'' tails than the likelihood \n\n-  with $t_9$ errors use a $t_\\alpha$ with $\\alpha < 9$ \n-  also represent via scale mixture of normals \n-  Horseshoe, Double Pareto, Cauchy all have heavier tails \n\n## Summary\n\n-  Classical diagnostics useful for EDA (checking data, potential outliers/influential points) or posterior predictive checks\n-  BMA/BVS and Bayesian robust regression avoid interactive decision making about outliers\n-  Robust Regression (Bayes) can still identify outliers through distribution on weights\n\n-  continuous versus mixture distribution on scale parameters\n\n-  Other mixtures (sub populations?) on scales and $\\b$?\n\n\n- Be careful about what predictors or transformations are used in the model as some outliers may be a result of model misspecification!\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}