{
  "hash": "ad4b4939ec5f1da3b89ad5dec32daaa8",
  "result": {
    "markdown": "---\ntitle: \"Lecture 12: Choice of Priors in Regression\"\nsubtitle: \"STA702\"\nauthor: \"Merlise Clyde\"\ninstitute: \"Duke University\"\nformat: \n  revealjs:\n    theme: [simple, custom.scss]\n    slide-number: true\n    incremental: true\n    scrollable: false\n    controls: true\n    fragments: true\n    preview-links: auto\n    logo: ../../img/icon.png\n    footer: <https://sta702-F23.github.io/website/>\n    chalkboard: \n      boardmarker-width: 1\n      chalk-width: 2\n      chalk-effect: 0\n    embed-resources: false\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"    \neditor: \n  markdown: \n    wrap: 72\nexecute: \n  echo: false\nnumber-sections: false\nfilters:\n  - custom-numbered-blocks  \ncustom-numbered-blocks:\n  groups:\n    thmlike:\n      colors: [948bde, 584eab]\n      boxstyle: foldbox.simple\n      collapse: false\n      listin: [mathstuff]\n    todos: default  \n  classes:\n    Theorem:\n      group: thmlike\n    Corollary:\n      group: thmlike\n    Conjecture:\n      group: thmlike\n      collapse: true  \n    Definition:\n      group: thmlike\n      colors: [d999d3, a01793]\n    Feature: default\n    TODO:\n      label: \"To do\"\n      colors: [e7b1b4, 8c3236]\n      group: todos\n      listin: [stilltodo]\n    DONE:\n      label: \"Done\"\n      colors: [cce7b1, 86b754]  \n      group: todos  \n---\n\n\n\n\n## Conjugate Priors in Linear Regression    {.smaller}\n\\usepackage{xcolor}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator{\\sgn}{sgn}\n\\newcommand{\\e}{\\mathbf{e}}\n\\renewcommand{\\P}{\\mathbf{P}}\n\\newcommand{\\F}{\\mathbf{F}}\n\\newcommand{\\R}{\\textsf{R}}\n\\newcommand{\\mat}[1] {\\mathbf{#1}}\n\\newcommand{\\E}{\\textsf{E}}\n\\newcommand{\\SE}{\\textsf{SE}}\n\\newcommand{\\SSE}{\\textsf{SSE}}\n\\newcommand{\\RSS}{\\textsf{RSS}}\n\\newcommand{\\FSS}{\\textsf{FSS}}\n\\renewcommand{\\SS}{\\textsf{SS}}\n\\newcommand{\\MSE}{\\textsf{MSE}}\n\\newcommand{\\SSR}{\\textsf{SSR}}\n\\newcommand{\\Be}{\\textsf{Beta}}\n\\newcommand{\\St}{\\textsf{St}}\n\\newcommand{\\Ca}{\\textsf{C}}\n\\newcommand{\\Exp}{\\textsf{Exp}}\n\\newcommand{\\GDP}{\\textsf{GDP}}\n\\newcommand{\\NcSt}{\\textsf{NcSt}}\n\\newcommand{\\Bin}{\\textsf{Bin}}\n\\newcommand{\\NB}{\\textsf{NegBin}}\n\\renewcommand{\\NG}{\\textsf{NG}}\n\\newcommand{\\N}{\\textsf{N}}\n\\newcommand{\\Ber}{\\textsf{Ber}}\n\\newcommand{\\Poi}{\\text{Poi}}\n\\newcommand{\\Gam}{\\textsf{Gamma}}\n\\newcommand{\\BB}{\\textsf{BB}}\n\\newcommand{\\Gm}{\\textsf{G}}\n\\newcommand{\\Un}{\\textsf{Unif}}\n\\newcommand{\\Ex}{\\textsf{Exp}}\n\\newcommand{\\DE}{\\textsf{DE}}\n\\newcommand{\\tr}{\\textsf{tr}}\n\\newcommand{\\cF}{{\\cal{F}}}\n\\newcommand{\\cL}{{\\cal{L}}}\n\\newcommand{\\cI}{{\\cal{I}}}\n\\newcommand{\\cB}{{\\cal{B}}}\n\\newcommand{\\cP}{{\\cal{P}}}\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\pperp}{\\mathrel{{\\rlap{$\\,\\perp$}\\perp\\,\\,}}}\n\\newcommand{\\OFP}{(\\Omega,\\cF, \\P)}\n\\newcommand{\\eps}{\\boldsymbol{\\epsilon}}\n\\newcommand{\\1}{\\mathbf{1}_n}\n\\newcommand{\\gap}{\\vspace{8mm}}\n\\newcommand{\\ind}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm ind}}}\n\\newcommand{\\iid}{\\mathrel{\\mathop{\\sim}\\limits^{\\rm iid}}}\n\\newcommand{\\simiid}{\\ensuremath{\\mathrel{\\mathop{\\sim}\\limits^{\\rm\niid}}}}\n\\newcommand{\\eqindis}{\\mathrel{\\mathop{=}\\limits^{\\rm D}}}\n\\newcommand{\\SSZ}{S_{zz}}\n\\newcommand{\\SZW}{S_{zw}}\n\\newcommand{\\Var}{\\textsf{Var}}\n\\newcommand{\\corr}{\\textsf{corr}}\n\\newcommand{\\diag}{\\textsf{diag}}\n\\newcommand{\\var}{\\textsf{var}}\n\\newcommand{\\Cov}{\\textsf{Cov}}\n\\newcommand{\\Sam}{{\\cal S}}\n\\def\\H{\\mathbf{H}}\n\\newcommand{\\I}{\\mathbf{I}}\n\\newcommand{\\Y}{\\mathbf{Y}}\n\\newcommand{\\tY}{\\tilde{\\mathbf{Y}}}\n\\newcommand{\\Yhat}{\\hat{\\mathbf{Y}}}\n\\newcommand{\\Yobs}{\\mathbf{Y}_{{\\cal S}}}\n\\newcommand{\\barYobs}{\\bar{Y}_{{\\cal S}}}\n\\newcommand{\\barYmiss}{\\bar{Y}_{{\\cal S}^c}}\n\\def\\bv{\\mathbf{b}}\n\\def\\X{\\mathbf{X}}\n\\def\\tX{\\tilde{\\mathbf{X}}}\n\\def\\x{\\mathbf{x}}\n\\def\\xbar{\\bar{\\mathbf{x}}}\n\\def\\Xbar{\\bar{\\mathbf{X}}}\n\\def\\Xg{\\mathbf{X}_{\\boldsymbol{\\gamma}}}\n\\def\\Ybar{\\bar{\\Y}}\n\\def\\ybar{\\bar{y}}\n\\def\\y{\\mathbf{y}}\n\\def\\Yf{\\mathbf{Y_f}}\n\\def\\W{\\mathbf{W}}\n\\def\\L{\\mathbf{L}}\n\\def\\w{\\mathbf{w}}\n\\def\\U{\\mathbf{U}}\n\\def\\V{\\mathbf{V}}\n\\def\\Q{\\mathbf{Q}}\n\\def\\Z{\\mathbf{Z}}\n\\def\\z{\\mathbf{z}}\n\\def\\v{\\mathbf{v}}\n\\def\\u{\\mathbf{u}}\n\n\\def\\zero{\\mathbf{0}}\n\\def\\one{\\mathbf{1}}\n\\newcommand{\\taub}{\\boldsymbol{\\tau}}\n\\newcommand{\\betav}{\\boldsymbol{\\beta}}\n\\newcommand{\\alphav}{\\boldsymbol{\\alpha}}\n\\newcommand{\\A}{\\mathbf{A}}\n\\def\\a{\\mathbf{a}}\n\\def\\K{\\mathbf{K}}\n\\newcommand{\\B}{\\mathbf{B}}\n\\def\\b{\\boldsymbol{\\beta}}\n\\def\\bhat{\\hat{\\boldsymbol{\\beta}}}\n\\def\\btilde{\\tilde{\\boldsymbol{\\beta}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\bg{\\boldsymbol{\\beta_\\gamma}}\n\\def\\bgnot{\\boldsymbol{\\beta_{(-\\gamma)}}}\n\\def\\mub{\\boldsymbol{\\mu}}\n\\def\\tmub{\\tilde{\\boldsymbol{\\mu}}}\n\\def\\muhat{\\hat{\\boldsymbol{\\mu}}}\n\\def\\tb{\\boldsymbol{\\theta}}\n\\def\\tk{\\boldsymbol{\\theta}_k}\n\\def\\tj{\\boldsymbol{\\theta}_j}\n\\def\\Mk{\\boldsymbol{{\\cal M}}_k}\n\\def\\M{\\boldsymbol{{\\cal M}}}\n\\def\\Mj{\\boldsymbol{{\\cal M}}_j}\n\\def\\Mi{\\boldsymbol{{\\cal M}}_i}\n\\def\\Mg{{\\boldsymbol{{\\cal M}_\\gamma}}}\n\\def\\Mnull{\\boldsymbol{{\\cal M}}_{N}}\n\\def\\gMPM{\\boldsymbol{\\gamma}_{\\text{MPM}}}\n\\def\\gHPM{\\boldsymbol{\\gamma}_{\\text{HPM}}}\n\\def\\Mfull{\\boldsymbol{{\\cal M}}_{F}}\n\\def\\tg{\\boldsymbol{\\theta}_{\\boldsymbol{\\gamma}}}\n\\def\\g{\\boldsymbol{\\gamma}}\n\\def\\eg{\\boldsymbol{\\eta}_{\\boldsymbol{\\gamma}}}\n\\def\\G{\\mathbf{G}}\n\\def\\cM{\\cal M}\n\\def\\D{\\Delta}\n\\def \\shat{{\\hat{\\sigma}}^2}\n\\def\\uv{\\mathbf{u}}\n\\def\\l {\\lambda}\n\\def\\d{\\delta}\n\\def\\Sigmab{\\boldsymbol{\\Sigma}}\n\\def\\Phib{\\boldsymbol{\\Phi}}\n\\def\\Lambdab{\\boldsymbol{\\Lambda}}\n\\def\\lambdab{\\boldsymbol{\\lambda}}\n\\def\\Mg{{\\cal M}_\\gamma}\n\\def\\S{{\\cal{S}}}\n\\def\\qg{p_{\\boldsymbol{\\gamma}}}\n\\def\\pg{p_{\\boldsymbol{\\gamma}}}\n\\def\\T{\\boldsymbol{\\Theta}}\n\\def\\Tb{\\boldsymbol{\\Theta}}\n\\def\\t{\\mathbf{t}}\n\n\n\n- Regression Model (Sampling model)\n$$\\Y \\mid \\b, \\phi  \\sim \\N(\\X \\b, \\phi^{-1} \\I_n) \n$$\n- Conjugate Normal-Gamma Model: factor joint prior $p(\\b, \\phi ) = p(\\b \\mid \\phi)p(\\phi)$ \n$$\\begin{align*}\n\\b \\mid \\phi & \\sim \\N(\\bv_0, \\phi^{-1}\\Phib_0^{-1}) & p(\\b \\mid \\phi) & = \\frac{|\\phi \\Phib_0|^{1/2}}{(2 \\pi)^{p/2}}e^{\\left\\{- \\frac{\\phi}{2}(\\b - \\bv_0)^T \\Phib_0 (\\b - \\bv_0)  \\right\\}}\\\\\n\\phi & \\sim \\Gam(v_0/2, \\SS_0/2)  & p(\\phi) & = \\frac{1}{\\Gamma{(\\nu_0/2)}}\n\\left(\\frac{\\SS_0}{2} \\right)^{\\nu_0/2}\n\\phi^{\\nu_0/2 - 1}\n e^{- \\phi \\frac{\\SS_0}{2}}\\\\\n\\Rightarrow (\\b, \\phi) & \\sim \\NG(\\bv_0, \\Phib_0, \\nu_o, \\SS_0)\n\\end{align*}$$\n\n\n- Need to specify the  4 hyperparameters of the Normal-Gamma distribution!\n\n- hard in higher dimensions!\n\n## Choice of Conjugate Prior {.smaller}\n\n\n\nSeek default choices\n\n- Jeffreys' prior\n- unit-information prior\n- Zellner's g-prior \n- ridge regression priors\n- mixtures of conjugate priors\n    + Zellner-Siow Cauchy Prior\n    + (Bayesian) Lasso\n    + Horseshoe\n    \n. . . \n\nWhich? Why?    \n    \n## Jeffreys' Prior {.smaller}\n\n- Jeffreys prior is invariant to model parameterization of $\\tb = (\\b ,\\phi)$ \n  $$p(\\tb) \\propto |\\cI(\\tb)|^{1/2}$$\n- $\\cI(\\tb)$ is the  Expected Fisher Information matrix\n$$\\cI(\\theta) = - \\E[ \\left[ \\frac{\\partial^2 \\log(\\cL(\\tb))}{\\partial\n  \\theta_i \\partial \\theta_j} \\right] ]$$\n  \n- log likelihood expressed as function of sufficient statistics\n\n. . .\n\n$$\\log(\\cL(\\b, \\phi))  =  \\frac{n}{2} \\log(\\phi)  - \\frac{\\phi}{2} \\| (\\I_n - \\P_\\x) \\Y\\|^2\n - \\frac{\\phi}{2}(\\b - \\bhat)^T(\\X^T\\X)(\\b - \\bhat)$$ \n\n- projection  $\\P_{\\X} = \\X (\\X^T\\X)^{-1} \\X^T$ onto the column space of $\\X$\n\n## Information matrix {.smaller}\n\\begin{eqnarray*}\n\\frac{\\partial^2 \\log \\cL} { \\partial \\tb \\partial \\tb^T} & = &\n\\left[\n  \\begin{array}{cc}\n    -\\phi (\\X^T\\X) & -(\\X^T\\X) (\\b - \\bhat) \\\\\n  - (\\b - \\bhat)^T (\\X^T\\X) & -\\frac{n}{2} \\frac{1}{\\phi^2} \\\\\n  \\end{array}\n\\right] \\\\\n\\E[\\frac{\\partial^2 \\log \\cL} { \\partial \\tb \\partial \\tb^T}] & = &\n\\left[\n  \\begin{array}{cc}\n    -\\phi (\\X^T\\X) & \\zero_p \\\\\n  \\zero_p^T & -\\frac{n}{2} \\frac{1}{\\phi^2} \\\\\n  \\end{array}\n\\right] \\\\\n& & \\\\\n\\cI((\\b, \\phi)^T) & = & \\left[\n  \\begin{array}{cc}\n    \\phi (\\X^T\\X) & \\zero_p \\\\\n  \\zero_p^T & \\frac{n}{2} \\frac{1}{\\phi^2}\n  \\end{array}\n\\right]\n  \\end{eqnarray*}\n  \n. . .\n\n \n  \\begin{eqnarray*}\n  p_J(\\b, \\phi)  & \\propto & |\\cI((\\b, \\phi)^T) |^{1/2}   \n                =  |\\phi \\X^T\\X|^{1/2} \\left(\\frac{n}{2}\n                 \\frac{1}{\\phi^2} \\right)^{1/2} \n  \\propto    \\phi^{p/2 - 1} |\\X^T\\X|^{1/2} \\\\\n  & \\propto & \\phi^{p/2 - 1}  \n  \\end{eqnarray*}\n  \n  . . . \n  \n  Jeffreys' did not recommend - marginal for $\\phi$ dies not account for dimension $p$\n  \n## Recommended Independent Jeffreys  Prior  {.smaller}\n\n-   Treat $\\b$ and $\\phi$ separately  (_orthogonal\n    parameterization_ which implies asymptoptic independence of $\\b$ and $\\phi$) \n-    $p_{IJ}(\\b) \\propto |\\cI(\\b)|^{1/2}$ and\n      $p_{IJ}(\\phi) \\propto |\\cI(\\phi)|^{1/2}$ \n    \n. . .\n\n$$\n\\cI((\\b, \\phi)^T)  =  \\left[\n  \\begin{array}{cc}\n    \\phi (\\X^T\\X) & \\zero_p \\\\\n  \\zero_p^T & \\frac{n}{2} \\frac{1}{\\phi^2}\n  \\end{array}\n\\right]\n$$\n\n. . .\n\n\\begin{align*} p_{IJ}(\\b) & \\propto |\\phi \\X^T\\X|^{1/2} \\propto 1 \\\\\n               p_{IJ}(\\phi) & \\propto \\phi^{-1} \\\\\n               p_{IJ}(\\beta, \\phi) & \\propto p_{IJ}(\\b) p_{IJ}(\\phi) = \\phi^{-1}\n\\end{align*}               \n\n. . .\n\nTwo group _reference prior_\n\n\n## Formal Posterior Distribution {.smaller}\n\n- Use Independent Jeffreys Prior\n$p_{IJ}(\\beta, \\phi) \\propto p_{IJ}(\\b) p_{IJ}(\\phi) = \\phi^{-1}$\n\n- Formal Posterior Distribution\n\\begin{eqnarray*}\n  \\b \\mid \\phi, \\Y & \\sim & \\N(\\bhat, (\\X^T\\X)^{-1} \\phi^{-1})  \\\\\n  \\phi \\mid \\Y & \\sim& \\Gam((n-p)/2, \\| \\Y - \\X\\bhat \\|^2/2) \\\\\n\\b \\mid \\Y & \\sim & t_{n-p}(\\bhat, \\shat (\\X^T\\X)^{-1})\n\\end{eqnarray*}\n\n- Bayesian Credible Sets\n$p(\\b \\in C_\\alpha\\mid \\Y) = 1- \\alpha$ correspond to frequentist Confidence\nRegions\n$$\\frac{\\x^T\\b - \\x^T \\bhat} {\\sqrt{\\shat \\x^T(\\X^T\\X)^{-1} \\x} }\\sim t_{n-p}$$\n- conditional on $\\Y$ for Bayes and conditional on $\\b$ for frequentist\n\n## Unit Information Prior {.smaller}\n\nUnit information prior $\\b \\mid \\phi \\sim \\N(\\bhat, n\n   (\\X^T\\X)^{-1}/\\phi)$\n\n- Based on a fraction of the likelihood $p(\\b ,\\phi) \\propto \\cL(\\b, \\phi)^{1/n}$ \n\n. . .\n\n$$\\log(p(\\b, \\phi) \\propto  \\frac{1}{n}\\frac{n}{2} \\log(\\phi)  - \\frac{\\phi}{2} \\frac{\\| (\\I_n - \\P_\\x) \\Y\\|^2}{n}\n - \\frac{\\phi}{2}(\\b - \\bhat)^T\\frac{(\\X^T\\X)}{n}(\\b - \\bhat)$$ \n\n- ``average information'' in one observation is  $\\phi \\X^T\\X /n$  or \"unit information\"\n\n  \n-  Posterior mean $\\frac{n}{1 + n} \\bhat +  \\frac{1}{1 + n}\\bhat = \\bhat$\n\n- Posterior Distribution\n$$\\b \\mid \\Y, \\phi \\sim \\N\\left( \\bhat, \\frac{n}{1 + n} (\\X^T\\X)^{-1}\n  \\phi^{-1} \\right)$$ \n  \n## Unit Information Prior  {.smaller}\n\n- Advantages:\n\n    - Proper\n    - Invariant to model parameteriztion of $\\X$ (next) \n    - Equivalent to MLE (no bias) and  tighter intervals\n\n\n- Disadvantages\n\n    - cannot represent  _prior_  beliefs; \n    - double use of data!  \n    - no shrinkage of $\\b$  with noisy data  (larger variance than biased estimators)\n\n. . . \n\n::: {.callout-tip title=\"Exercise for the Energetic Student\"}\n\n-  What would be a \"Unit information prior\"  for $\\phi$?\n-  What is the marginal posterior for $\\b$ using both unit-information priors?\n:::\n\n## Invariance and Choice of Mean/Precision {.smaller}\n\n-  the model in vector form $Y  \\mid \\beta, \\phi \\sim \\textsf{N}_n (X\\beta, \\phi^{-1} I_n)$\n\n  \n\n- What if we transform  the mean $X\\beta = X H H^{-1} \\beta$ with new $X$ matrix $\\tilde{X} = X H$ where $H$ is $p \\times p$ and invertible and coefficients  $\\tilde{\\beta} = H^{-1} \\beta$.\n\n  \n\n- obtain the posterior for $\\tilde{\\beta}$ using $Y$ and $\\tilde{X}$  \n$$ Y \\mid  \\tilde{\\beta}, \\phi \\sim \\textsf{N}_n (\\tilde{X}\\tilde{\\beta}, \\phi^{-1} I_n)$$\n\n- since $\\tilde{X} \\tilde{\\beta} = X H  \\tilde{\\beta} = X \\beta$  invariance suggests that the posterior for $\\beta$ and $H \\tilde{\\beta}$ should be the same \n\n  \n- plus the posterior of $H^{-1} \\beta$\nand $\\tilde{\\beta}$ should be the same\n\n. . .\n\n::: {.callout-tip title=\"Exercise for the Energetic Student\"}\nWith some linear algebra, show that this is true for a normal prior if $b_0 = 0$ and $\\Phi_0$ is $k X^TX$ for some $k$  \n:::\n \n## Zellner's g-prior {.smaller}\n\n- Popular choice is to take $k = \\phi/g$ which is a special case of Zellner's g-prior\n$$\\beta \\mid \\phi, g \\sim \\textsf{N}\\left(\\bv_0, \\frac{g}{\\phi} (X^TX)^{-1}\\right)$$\n\n  \n\n- Full conditional \n$$\\beta \\mid \\phi, g \\sim \\textsf{N}\\left(\\frac{g}{1 + g} \\hat{\\beta} + \\frac{1}{1 + g}\\bv_0, \\frac{1}{\\phi} \\frac{g}{1 + g} (X^TX)^{-1}\\right)$$\n  \n\n- one parameter $g$ controls shrinkage\n\n- invariance  under linear transformations of $\\X$ with $\\bv_0 = 0$ or  transform mean $\\tilde{\\bv}_0 = H^{-1}\\bv_0$\n\n  \n\n- often paired with the Jeffereys' reference prior for $\\phi$\n\n- allows an informative mean, but keeps the same correlation structure as the MLE\n  \n## Zellner's Blocked g-prior {.smaller}\n\n- Zellner also realized that different blocks might have different degrees of prior information \n\n- Two blocks $\\X_1$ and $\\X_2$ with $\\X_1^T \\X_2 = 0$ so Fisher Information is block diagonal\n\n- Model $\\Y = \\X_1 \\alphav + \\X_2 \\b + \\eps$\n  \n- Priors\n  \\begin{align}\n  \\alphav \\mid \\phi & \\sim \\N(\\alphav_1, \\frac{g_{\\alphav}}{\\phi} (\\X_1^T\\X_1)^{-1})\\\\\n  \\b \\mid \\phi & \\sim \\N(\\bv_0, \\frac{g_{\\b}}{\\phi} (\\X_2^T\\X_2)^{-1})\n  \\end{align}\n \n - Important case $\\X_1 = \\one_n$ corresponding to intercept with limiting case  $g_{\\alphav} \\to \\infty$\n $$p(\\alphav) \\propto 1$$\n\n## Potential Problems {.smaller}\n\n- The posterior in Jeffereys' prior(s), the unit information prior, and Zellner's g-priors depend on\n$(\\X^T\\X)^{-1}$ and the MLE $\\hat{\\b}$ \n\n- If $\\X^T\\X = \\U \\Lambdab \\U^T$ is nearly singular ($\\lambda_j \\approx 0$ for one or more eigenvalues), certain  elements of $\\beta$ or (linear combinations of $\\beta$) may have huge posterior variances and the MLEs (and posterior means) are highly unstable!\n\n- there is no unique posterior distribution if any $\\lambda_j = 0$!  ($p > n$ or non-full rank)\n\n- Posterior Precision and Mean in conjugate prior \n \\begin{align}\n \\Phib_n & = \\X^T\\X + \\Phib_0 \\\\\n \\bv_n & = \\Phib^{-1} (\\X^T\\Y + \\Phib_0 \\bv_0)\n \\end{align}\n \n- Need a proper prior with $\\Phib_0 >0$ (OK for $\\bv_0 = 0$ )\n\n- Simplest case: take $\\Phib_0 = \\kappa \\I_p$ so that $\\Phib_n = \\X^T\\X + \\kappa \\I_p = \\U(\\Lambdab + \\kappa \\I_p) \\U^T > 0$\n\n\n## Ridge Regression  {.smaller}\n\nModel: $\\Y = \\one_n \\alpha + \\X \\b + \\eps$\n\n- WLOG assume that $\\X$ has been centered and scaled so that $\\X^T\\X = \\corr(\\X)$ \n\n- typically expect the  intercept $\\alpha$ to be a different order of magnitude from the other predictors. \n\n    - Adopt a two block prior with $p(\\alpha) \\propto 1$\n    - If $\\X$ is centered, $\\one_n^T \\X = \\zero_p$\n\n\n- Prior $\\b \\mid \\phi \\sim \\N(\\zero_b, \\frac{1} {\\phi \\kappa} \\I_p$) implies  the $\\bv$ are exchangable _a priori_  (i.e. distribution is invariant under permuting the labels and with a common scale and mean)\n\n    - if different predictors have different variances, rescale $\\X$ to have variance 1 \n\n\n\n  \n\n\n\n- Posterior for $\\b$ \n$$\\b \\mid \\phi, \\kappa, \\Y \\sim \n\\textsf{N}\\left((\\kappa I_p + X^TX)^{-1} X^T Y,  \\frac{1}{\\phi}(\\kappa I_p + X^TX)^{-1}\n\\right)$$\n\n\n\n\n \n##  Bayes Ridge Regression {.smaller}\n\n- Posterior mean (or mode) given $\\kappa$ is biased, but can show that there **always** is a value of $\\kappa$  where the frequentist's expected squared error loss is smaller for the Ridge estimator than MLE!\n\n- Unfortunately the optimal choice depends on \"true\" $\\b$!\n\n  \n\n- related to penalized maximum likelihood estimation \n$$-\\frac{\\phi}{2}\\left(\\|\\Y - \\X \\b\\|^2 + \\kappa \\| \\b \\|^2 \\right)\n$$\n\n  \n\n-  Choice of $\\kappa$ ?\n    + Cross-validation (frequentist)\n    + Empirical Bayes?   (frequentist/Bayes)\n    + fixed _a priori_ Bayes (and how to choose)\n\n- Should there be a common $\\kappa$? Or a $\\kappa_j$ per variable? (or shared in a group?)\n\n## Mixture of Conjugate Priors {.smaller}\n\n- can place a prior on $\\kappa$ or $\\kappa_j$ for fully Bayes\n\n- similar issue for $g$ in the $g$ priors\n\n- often improved robustness over fixed choices of hyperparameter\n\n- may not have cloosed form posterior but sampling is still often easy!\n\n- Examples: Bayesian Lasso, Double Laplace, Horseshoe prior, mixtures of $g$-priors\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/font-awesome/css/all.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/font-awesome/css/v4-shims.min.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}