[
  {
    "objectID": "slides/00-course-overview.html#what-is-this-course-about",
    "href": "slides/00-course-overview.html#what-is-this-course-about",
    "title": "Welcome to STA 702",
    "section": "What is this course about?",
    "text": "What is this course about?\n\nLearn the foundations and theory of Bayesian inference in the context of several models.\nUse Bayesian models to answer inferential questions.\nApply the models to several different problems.\nUnderstand the advantages/disadvantages of Bayesian methods vs classical methods\n\n\n\n A Bayesian version will usually make things better…\n– Andrew Gelman."
  },
  {
    "objectID": "slides/00-course-overview.html#instructional-team",
    "href": "slides/00-course-overview.html#instructional-team",
    "title": "Welcome to STA 702",
    "section": "Instructional Team",
    "text": "Instructional Team\nInstructor: Dr Merlise Clyde\n   clyde@duke.edu     223 Old Chemistry     https://www2.stat.duke.edu/~clyde \n\n \nTeaching Assistant: Rick Presman\n   rick.presman@duke.edu\n \n   See course website for Office Hours, Policies and more!"
  },
  {
    "objectID": "slides/00-course-overview.html#prerequisites",
    "href": "slides/00-course-overview.html#prerequisites",
    "title": "Welcome to STA 702",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nrandom variables, common families of probability distribution functions and expectations\nconditional distributions\ntransformations of random variables and change of variables\nprinciples of statistical inference (likelihoods)\nsampling distributions and hypothesis testing\nconcepts of convergence\n\n\nReview Chapters 1 to 5 of the Casella and Berger book"
  },
  {
    "objectID": "slides/00-course-overview.html#computing",
    "href": "slides/00-course-overview.html#computing",
    "title": "Welcome to STA 702",
    "section": "Computing",
    "text": "Computing\n\nLabs/HW will involve a lot of computing in R!\nWrite your own MCMC samplers and run code long enough to show convergence\nYou can learn R on the fly\n\nsee Resources Tab on website\nmaterials from 2023 Bootcamp/Orientation"
  },
  {
    "objectID": "slides/00-course-overview.html#grading-policies",
    "href": "slides/00-course-overview.html#grading-policies",
    "title": "Welcome to STA 702",
    "section": "Grading Policies",
    "text": "Grading Policies\n\n5% class\n20% HW\n10% Lab\n20% Midterm I\n20% Midterm II\n25% Final\nNo Late Submissions for HW/Lab; Drop the lowest score\nConfirm that you have access to Sakai, Gradescope, and GitHub.\n\n\n\n\nhttps://sta702-F23.github.io/website/"
  },
  {
    "objectID": "reading/01-reading.html",
    "href": "reading/01-reading.html",
    "title": "Lecture 1: Basics of Bayesian inference",
    "section": "",
    "text": "A First Course in Bayesian Statistical Methods by Peter D. Hoff.:\n\nSection 1: Introduction and examples\nSection 3.1: The binomial model\nSection 3.4: Discussion and further references\n\nBayesian Data Analysis (Third Edition) by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin:\n\nSection 1.1: The three steps of Bayesian data analysis\nSection 1.2: General notation for statistical inference\nSection 1.3: Bayesian inference\nSection 1.9: Computation and software\nSection 1.10: Bayesian inference in applied statistics\nSection 2.1: Estimating a probability from binomial data\nSection 2.2: Posterior as compromise between data and prior information\nSection 2.4: Informative prior distributions"
  },
  {
    "objectID": "reading/01-reading.html#readings",
    "href": "reading/01-reading.html#readings",
    "title": "Lecture 1: Basics of Bayesian inference",
    "section": "",
    "text": "A First Course in Bayesian Statistical Methods by Peter D. Hoff.:\n\nSection 1: Introduction and examples\nSection 3.1: The binomial model\nSection 3.4: Discussion and further references\n\nBayesian Data Analysis (Third Edition) by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin:\n\nSection 1.1: The three steps of Bayesian data analysis\nSection 1.2: General notation for statistical inference\nSection 1.3: Bayesian inference\nSection 1.9: Computation and software\nSection 1.10: Bayesian inference in applied statistics\nSection 2.1: Estimating a probability from binomial data\nSection 2.2: Posterior as compromise between data and prior information\nSection 2.4: Informative prior distributions"
  },
  {
    "objectID": "reading/01-reading.html#optional",
    "href": "reading/01-reading.html#optional",
    "title": "Lecture 1: Basics of Bayesian inference",
    "section": "Optional",
    "text": "Optional\n\nBayesian Computation with R (Second Edition) by Jim Albert:\n\nSection 2: Introduction to Bayesian thinking"
  },
  {
    "objectID": "labs/lab-00-getting-started.html",
    "href": "labs/lab-00-getting-started.html",
    "title": "Lab 0: Getting Started with R and Sweave/Knitr",
    "section": "",
    "text": "Due: 11:59pm, Thursday, Sept 1"
  },
  {
    "objectID": "labs/lab-00-getting-started.html#rrstudio",
    "href": "labs/lab-00-getting-started.html#rrstudio",
    "title": "Lab 0: Getting Started with R and Sweave/Knitr",
    "section": "R/RStudio",
    "text": "R/RStudio\nYou all should have R and RStudio installed on your computers or feel free to use the Department servers. If you want a local version of RStudion, first install the latest version of R here: https://cran.rstudio.com (remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system. You may also need to install git as well."
  },
  {
    "objectID": "labs/lab-00-getting-started.html#github",
    "href": "labs/lab-00-getting-started.html#github",
    "title": "Lab 0: Getting Started with R and Sweave/Knitr",
    "section": "Github",
    "text": "Github\nCreate a github account if you do not have one here https://github.com"
  },
  {
    "objectID": "labs/lab-00-getting-started.html#r-sweave",
    "href": "labs/lab-00-getting-started.html#r-sweave",
    "title": "Lab 0: Getting Started with R and Sweave/Knitr",
    "section": "R Sweave",
    "text": "R Sweave\nYou are required to use R Sweave/knitr to type up HW and lab reports. Don’t worry we will guide you along the way!"
  },
  {
    "objectID": "labs/lab-00-getting-started.html#gradescope",
    "href": "labs/lab-00-getting-started.html#gradescope",
    "title": "Lab 0: Getting Started with R and Sweave/Knitr",
    "section": "Gradescope",
    "text": "Gradescope\nYou MUST submit both your .Rnw and .pdf files to the course site on Gradescope here: https://www.gradescope.com/courses/431637/assignments/ Make sure to knit to pdf and to submit under the right assignment entry. For this first lab you do not have to submit, just practice!"
  },
  {
    "objectID": "index.html#bayesian-statistical-modeling-and-data-analysis",
    "href": "index.html#bayesian-statistical-modeling-and-data-analysis",
    "title": "STA 702 Fall 2023",
    "section": "Bayesian Statistical Modeling and Data Analysis",
    "text": "Bayesian Statistical Modeling and Data Analysis\n\nCourse Overview\nThis course provides an introduction to Bayesian statistics targeted towards building a foundation for later research in developing models appropriate to complex data applications and to methodology research developing new modeling/inferential frameworks and algorithms. Topics include the basic foundations of Bayesian inferences – prior distributions, likelihood functions, posterior distributions, loss functions, and Bayes estimators/decisions with illustration in simple cases. Posterior computation in non-conjugate models with Markov chain Monte Carlo (MCMC) algorithms in addition to approximations to posteriors based on Laplace and variational approaches will be covered. We will build (and critique) models for a variety of data types and structures including regression, classification, and dependent data, hierarchical models for the borrowing of information, and methods for dealing with model uncertainty. Throughout we will discuss the difference between classical and Bayesian paradigms as well as advantages/disadvantages of Bayes. Time permitting we will discuss generalized Bayes.\n\n\nLearning Objectives\nBy the end of this course, students should be able to\n\nUnderstand the basics of Bayesian inference, that is, be able to define likelihood functions, prior distributions, posterior distributions, prior predictive distributions and posterior predictive distributions.\nDerive posterior distributions, prior predictive distributions and posterior predictive distributions, for common likelihood-prior combinations of distributions.\nInterpret the results of fitted models and conduct checks to ascertain that the models have converged.\nUse the Bayesian methods and models covered in class to analyze real data sets.\nAssess the adequacy of Bayesian models to any given data and make a decision on what to do in cases when certain models are not appropriate for a given data set."
  },
  {
    "objectID": "index.html#course-info",
    "href": "index.html#course-info",
    "title": "STA 702 Fall 2023",
    "section": "Course Info",
    "text": "Course Info\n\nInstructional Team and Office Hours\n\n\n\nRole\nName\nEmail\nOffice Hours\nLocation\n\n\n\n\nInstructor\nDr Merlise Clyde\n\nMon 1:15- 2:15, Tu & Thur 11:30  or by appointment (I have lots of 30 minute gaps!) \n223E Old Chem\n\n\nTA\nRick Presman\n\nTBA\nTBA\n\n\n\n\n\nMeeting Times\n\nLecture\n   Tuesdays and Thursdays (10:05am - 11:15am)\n   Perkins LINK 060 (Classroom 1)\n\n\nLabs\n   Fridays (11:45pm - 1:00pm)\n   Perkins LINK 060 (Classroom 1)\n\n\nZoom meetings\nOccasionally we may need to meet over Zoom for class/lab or Office hours. The easiest way for you to join the different Zoom meetings is to log in to Sakai, go to the “Zoom meetings” tab, and click “Upcoming Meetings”. For the recordings (for lecture/lab and discussion sessions were recorded), also log in to Sakai, go to the “Zoom meetings” tab, and click “Cloud Recordings”. Those will be available few minutes after the sessions.\n\n\n\nTexts\n\n\n\n \nTitle\nAuthor(s)\nPublisher\n\n\n\n\n\nA First Course in Bayesian Statistical Methods\nPeter D. Hoff, 2009\nSpringer\n\n\n\nBayesian Data Analysis (Third Edition)\nAndrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin\nChapman and Hall/CRC\n\n\n\nAll books are available available online from Duke library. See the Resources tab for additional links\n\n\nMaterials\nLecture notes and slides, and assigned readings will be posted on the course website. Homework and Lab Assignments will be posted on Github \n\n\nImportant Dates\n\n\n\n \n \n\n\n\n\nTues, Aug 29\nClasses begin\n\n\nFri, Sept 8\nDrop/Add ends\n\n\nSat - Tues, Oct 14 - 17\nFall Break\n\n\nFriday, Oct 13\nMidterm I (tentative)\n\n\nTues, Nov 20\nMidterm II (tentative)\n\n\nFriday, Dec 1\nGraduate Classes End\n\n\nDec 2 - Dec 12\nGraduate Reading Period\n\n\nSat, Dec 16\nFinal Exam (Perkins 060 2:00-5:00pm)\n\n\n\n\n\nGreen Classroom\n This course has achieved Duke’s Green Classroom Certification. The certification indicates that the faculty member teaching this course has taken significant steps to green the delivery of this course. Your faculty member has completed a checklist indicating their common practices in areas of this course that have an environmental impact, such as paper and energy consumption. Some common practices implemented by faculty to reduce the environmental impact of their course include allowing electronic submission of assignments, providing online readings and turning off lights and electronics in the classroom when they are not in use. The eco-friendly aspects of course delivery may vary by faculty, by course and throughout the semester. Learn more at https://sustainability.duke.edu/action/certification.\n\n\nAcknowledgement\nThis web page contains materials developed or adapted by Dr. Alexander Volfovsky, Dr. David B. Dunson, Dr. Rebecca Carter Steorts and Dr Michael Akande."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": " Syllabus",
    "section": "",
    "text": "When in doubt about anything at all, ask questions!!!\n\nPrerequisites\nALL students are expected to be familiar with all the topics covered within the required prerequisites to be in this course. That is - mathematical statistics and probability, linear algebra, and multivariate calculus. Students are also expected to be familiar with R and are encouraged to learn LaTeX during the course.\n\n\nWorkload\nWork hours will include time spent going through the preassigned readings, attending lectures and lab sessions, and doing all graded work.\n\n\nGraded Work\nGraded work for the course will consist of homework assignments, lab exercises, two midterms and a final exam. Regrade requests for problem sets and lab exercises must be done via Gradescope AT MOST 24 hours after grades are released! Regrade requests for quizzes, midterm, and final exams must be done via Gradescope AT MOST 12 hours after grades are released! Always write in complete sentences and show your steps.\nStudents’ final grades will be determined as shown below:\n\nFruit prices\n\n\nComponent\nPercentage\n\n\n\n\nHomework\n20%\n\n\nMidterm\n20%\n\n\nMidterm II\n20%\n\n\nLab exercises\n10%\n\n\nParticipation\n5%\n\n\nFinal Exam\n25%\n\n\n\nThere are no make-ups for any of the graded work except for medical or familial emergencies or for reasons approved by the instructor BEFORE the due date. See the instructor in advance of relevant due dates to discuss possible alternatives.\nGrades may be curved at the end of the semester. Cumulative averages of 90% – 100% are guaranteed at least an A-, 80% – 89% at least a B-, and 70% – 79% at least a C-, however the exact ranges for letter grades will be determined at the end of the course.\n\n\nDescriptions of graded work\n\nProblem sets\nHomework will be handed out on a weekly basis. They will be based on both the lectures and labs and will be announced every Thursday or Friday – be sure to check the website regularly! Also, please note that any work that is not legible by the instructor or TAs will not be graded (given a score of 0). Every write-up must be clearly written in full sentences and clear English. Any assignment that is completely unclear to the instructors and/or TAs, may result in a grade of a 0. For programming exercises, we will be using R/knitr with \\(\\LaTeX\\) for preparing assignments using github classroom for data analysis.\nEach student MUST write up and turn in her or his own answers. You are encouraged to talk to each other regarding homework problems or to the instructor/TA. However, the write-up, solution, and code must be entirely your own work. No sharing of solutions or code! The assignments must be submitted on Gradescope under Assignments. Note that you will not be able to make online submissions after the due date, so be sure to submit before or by the Gradescope-specified deadline.\nSolutions will be curated from student solutions with proper attribution. Every week the TAs will select a representative correct solution for the assigned problems and put them together into one solutions set with each answer being attributed to the student who wrote it. If you would like to OPT OUT of having your homework solutions used for the class solutions, please let the Instructor and TAs know in advance.\nFinally, your lowest homework score will be dropped!\n\n\nLab exercises\nThe objective of the lab assignments is to give you more hands-on experience with Bayesian data analysis. Attend the lab session and learn a concept or two and some R from the TA, and then work on the computational part of the problem sets. Each lab assignment should be submitted in timely fashion. You are REQUIRED to use R/knitr (or R/Rmarkdown in some cases).\n\n\nMidterm Exams\nThere will be two inclass midterm exams. Detailed instructions on the midterm will be made available later but please check dates on the calendar well in advance!\n\n\nFinal Exam\nThere will be a final exam after the reading week. If you miss any quiz or the midterm, your grade will depend more on the final exam score since there are no make-up exams. You cannot miss the final exam! Please check the important dates on the homepage for the date and time of the final before making plans to return home at the end of the semester. Detailed instructions on the final will be made available later.\n\n\n\nLate Submission Policy\n\nno late submission of homework or lab assignments, however we will drop the lowest score in each.\n\n\n\nCourse Topics\n\nBasics of Bayesian Models\nLoss Functions, Inference and Decision Making\nPredictive Distributions\nPredictive Distributions and Model Checking\nBayesian Hypothesis Testing\nMultiple Testing\nMCMC (Gibbs & Metropolis Hastings Algorithms)\nBayesian Generalized Linear Models\nHiearchical Modeling and Random Effects\nHamiltonian Monte Carlo\nNonParametric Bayes\n\nFor a detailed day-by-day list of topics, please refer to the Course Schedule\n\n\nAcademic integrity\nDuke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, respect, and accountability. Citizens of this community commit to reflect upon and uphold these principles in all academic and nonacademic endeavors, and to protect and promote a culture of integrity.\nRemember the Duke Community Standard that you have agreed to abide by:\n\nTo uphold the Duke Community Standard:\n\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\nCheating or plagiarism on any graded assessments, lying about an illness or absence and other forms of academic dishonesty are a breach of trust with classmates and faculty, violate the Duke Community Standard, and will not be tolerated. Such incidences will result in a 0 grade for all parties involved. Additionally, there may be penalties to your final class grade along with being reported to the Office of Student Conduct. Review the academic dishonesty policies at https://studentaffairs.duke.edu/conduct/z-policies/academic-dishonesty.\n\n\nDiversity & Inclusiveness\nThis course is designed so that students from all backgrounds and perspectives all feel welcome both in and out of class. Please feel free to talk to me (in person or via email) if you do not feel well-served by any aspect of this class, or if some aspect of class is not welcoming or accessible to you. My goal is for you to succeed in this course, therefore, let me know immediately if you feel you are struggling with any part of the course more than you know how to manage. Doing so will not affect your grades, but it will allow me to provide the resources to help you succeed in the course.\n\n\nDisability Statement\nStudents with disabilities who believe that they may need accommodations in the class are encouraged to contact the Student Disabilities Access Office at 919-668-1267 or disabilities@aas.duke.edu as soon as possible to better ensure that such accommodations are implemented in a timely fashion.\n\n\nOther Information\nIt can be a lot more pleasant oftentimes to get one-on-one answers and help. Make use of the teaching team’s office hours, we’re here to help! Do not hesitate to talk to me during office hours or by appointment to discuss a problem set or any aspect of the course. Questions related to course assignments and honesty policy should be directed to me. When the teaching team has announcements for you we will send an email to your Duke email address. Be sure to check your email daily.\nMost of the course components will be held in person, but occasionally may need to be held online using Zoom meetings. If you have any concerns, issues or challenges, let the instructor know as soon as possible. Also, all students are strongly encouraged to rely on the forums in Sakai, for interacting among yourself and asking other students questions. You can also ask the instructor or the TAs questions on there and we will try to respond as soon as possible. If you experience any technical issues with joining or using the forums, let the instructor know.\n\n\nProfessionalism\nTry as much as possible to refrain from texting or using your computer for anything other than coursework during class and labs. Again, the more engaged you are, the quicker you will be able to get through the materials. You are responsible for everything covered in the lecture videos, lecture notes/slides, and in the assigned readings."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": " Schedule",
    "section": "",
    "text": "Please refresh often in case links/content has been updated\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nLesson\nReading\nSlides\nLabs\nHW\n\n\n\n\nWEEK 1\nTues, Aug 29\nLecture 0: Course Overview and Introduction\n\n\n\n\n\n\n\nThur, Aug 31\nLecture 1: Basics of Bayesian Inference\n\n\n\n\n\n\n\nFri, Sept 1\nLab 1: R and Monte Carlo Review\n\n\n\n\n\n\nWEEK 2\nTues, Sept 5\nLecture 2: Loss Functions & Summaries\n\n\n\n\n\n\n\nThur, Sept 7\nLecture 3: Normal Model & Predictive Distributions\n\n\n\n\n\n\n\nFri, Sept 8\nLab 2: Beta-Binomial Model and Introduction to stan\n\n\n\n\n\n\nWEEK 3\nTues, Sept 12\nLecture 4: Predictive Checks\n\n\n\n\n\n\n\nThur, Sept 14\nLecture 5: Introduction to Hierarchical Models, EB & Metropolis\n\n\n\n\n\n\n\nFri, Sept 15\nLab 3: Posterior Predictive Checks\n\n\n\n\n\n\nWEEK 4\nTues, Sept 19\nLecture 6: Metropolis Algorithm & Stochastic Sampling\n\n\n\n\n\n\n\nThur, Sept 21\nLecture 7: More MCMC - Metropolis Hastings and Adaptive Metropolis\n\n\n\n\n\n\n\nFri, Sept 22\nLab 4: Metropolis Hastings\n\n\n\n\n\n\nWEEK 5\nTues, Sept 27\nLecture 8: Metropolis-Hastings and Gibbs\n\n\n\n\n\n\n\nThur, Sept 29\nLecture 9: Data Augmentation\n\n\n\n\n\n\n\nFri, Sept 30\nLab 5: Adaptive Metropolis Hastings\n\n\n\n\n\n\nWEEK 6\nTues, Oct 4\nLecture 10: Basics of Hypothesis Testing\n\n\n\n\n\n\n\nThu, Oct 6\nLecture 11: Hypothesis Testing\n\n\n\n\n\n\n\nFri, Oct 7\nReview for Midterm I\n\n\n\n\n\n\nWEEK 7\nTue, Oct 11\nNO CLASS FALL BREAK\n\n\n\n\n\n\n\nThu, Oct 13\nMidterm I\n\n\n\n\n\n\n\nFri, Oct 14\nLab 6: Hypothesis & Multiple Testing\n\n\n\n\n\n\nWEEK 8\nTue, Oct 18\nLec 12: Multiple Testing and Hierachical Models\n\n\n\n\n\n\n\nThur, Oct 20\nLec 13: Bayesian Multiple Testing and Hierachical Models\n\n\n\n\n\n\n\nFri, Oct 21\nLab: Q & A on HW 5\n\n\n\n\n\n\nWEEK 9\nTue, Oct 25\nLec 14: Bayesian Linear Regression\n\n\n\n\n\n\n\nThur, Oct 27\nLec 15: Priors in Bayesian Linear Regression\n\n\n\n\n\n\n\nFri, Oct 28\nLab 7: Variable Selection\n\n\n\n\n\n\nWEEK 10\nTues, Nov 1\nLec 16: Bayesian Variable Selection and Model Averaging\n\n\n\n\n\n\n\nThur, Nov 3\nLec 17: Bayesian Variable Selection and Model Averaging\n\n\n\n\n\n\n\nFri, Nov 4\nLab: Q&A with HW 7\n\n\n\n\n\n\nWeek 11\nTues, Nov 8\nLec 18: Outliers\n\n\n\n\n\n\n\nThurs, Nov 10\nLec 19: Missing Data\n\n\n\n\n\n\n\nFri, Nov 11\nLab 8: Review\n\n\n\n\n\n\nWeek 12\nTues, Nov 15\nMidterm II\n\n\n\n\n\n\n\nThurs, Nov 17\nLec 20: Random Effects\n\n\n\n\n\n\n\nFri, Nov 18\nLab 9\n\n\n\n\n\n\nWeek 12\nTues, Nov 22\nLec 21: Mixed Effects Models\n\n\n\n\n\n\n\nThurs, Nov 24\nThanksgiving Break - No Class \n\n\n\n\n\n\nWeek 13\nTues, Nov 29\nHMC\n\n\n\n\n\n\n\nThur, Dec 1\nBARK: NonParametric Regression\n\n\n\n\n\n\n\nFri, Dec 2\nLab 10\n\n\n\n\n\n\nWeek 14\n\nReading Period\n\n\n\n\n\n\nFinals Period\nMon, Dec 16 2pm-5pm (usual classroom)"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": " Resources",
    "section": "",
    "text": "Supplementary Textbooks\nThese textbooks are great resources for some of the topics we will cover. You do not need to buy them, but you may be able to borrow them from Duke library should you need extra reading materials, besides the class slides and main textbooks. 1. Doing Bayesian Data Analysis in brms and the tidyverse 2. Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition 3. Albert, J. (2009), “Bayesian Computation with R (Second Edition).” 4. Bolstad, W. M. and Curran, J. M. (2016), “Introduction to Bayesian Statistics (Third Edition).”\n\n\nR and R Markdown Resources\nR Markdown can be used to create high quality reports and presentations with embedded chunks of R code. You are required to use R Markdown to type up your lab reports. R Markdown would also be my personal favorite for typing up your homework assignments for this course, but you are welcome to use any word processor of your choice for those. To learn more about R Markdown and for other resources for programming in R, see the links below.\n\nR for Data Science (by Hadley Wickham & Garrett Grolemund)\nIntroduction to R Markdown (Article by Garrett Grolemund)\nIntroduction to R Markdown (Slides by Andrew Cho)\nR Markdown Cheat Sheet\nData Visualization with ggplot2 Cheat Sheet\nOther Useful Cheat Sheets\nA very (very!) basic R Markdown template\n\n\n\nLaTeX\nYou may also use LaTeX to type up your assignments. You may find it easier to create your TeX and LaTeX documents using online editors such as Overleaf (simply create a free account and you are good to go!). However, that need not be the case. If you prefer to create them locally/offline on your personal computers, you will need to download a TeX distribution (the most popular choices are MiKTeX for Windows and MacTeX for macOS) plus an editor (I personally prefer TeXstudio but feel free to download any editor of your choice). Follow the links below for some options, and to also learn how to use LaTeX.\n\nLearn LaTeX in 30 minutes\nChoosing a LaTeX Compiler.\n\n\n\nInteresting Articles\nI will add articles I find interesting below. These are articles I find useful as supplementary readings for topics covered in class, or as good sources that cover concepts I think you should know, but which we may not have time to cover. I strongly suggest you find time to (at the very least) take a “quick peek” at each article.\n\nEfron, B., 1986. Why isn’t everyone a Bayesian?. The American Statistician, 40(1), pp. 1-5.\nGelman, A., 2008. Objections to Bayesian statistics. Bayesian Analysis, 3(3), pp. 445-449.\nDiaconis, P., 1977. Finite forms of de Finetti’s theorem on exchangeability. Synthese, 36(2), pp. 271-281.\nGelman, A., Meng, X. L. and Stern, H., 1996. Posterior predictive assessment of model fitness via realized discrepancies. Statistica sinica, pp. 733-760.\nDunson, D. B., 2018. Statistics in the big data era: Failures of the machine. Statistics & Probability Letters, 136, pp. 4-9."
  },
  {
    "objectID": "labs/lab-01-r-review-blank.html",
    "href": "labs/lab-01-r-review-blank.html",
    "title": "Lab 1: R Review and Monte Carlo",
    "section": "",
    "text": "Due: 1:00pm, Monday, Sept 5"
  },
  {
    "objectID": "labs/lab-01-r-review-blank.html#rrstudio",
    "href": "labs/lab-01-r-review-blank.html#rrstudio",
    "title": "Lab 1: R Review and Monte Carlo",
    "section": "R/RStudio",
    "text": "R/RStudio\nYou all should have R and RStudio installed on your computers by now or access to the Department Server. If you do not, first install the latest version of R here: https://cran.rstudio.com (remember to select the right installer for your operating system). Next, install the latest version of RStudio here: https://www.rstudio.com/products/rstudio/download/. Scroll down to the “Installers for Supported Platforms” section and find the right installer for your operating system."
  },
  {
    "objectID": "labs/lab-01-r-review-blank.html#r-knitr",
    "href": "labs/lab-01-r-review-blank.html#r-knitr",
    "title": "Lab 1: R Review and Monte Carlo",
    "section": "R Knitr",
    "text": "R Knitr\nYou are required to use R knitr with the Rnw format to type up this lab report. To get started see basics about knitr."
  },
  {
    "objectID": "labs/lab-01-r-review-blank.html#github-classroom-gradescope",
    "href": "labs/lab-01-r-review-blank.html#github-classroom-gradescope",
    "title": "Lab 1: R Review and Monte Carlo",
    "section": "Github Classroom & Gradescope",
    "text": "Github Classroom & Gradescope\nYou MUST submit both your final .Rnw and .pdf files to the repo on Github and upload the pdf to Gradescope here: https://www.gradescope.com/courses/431637/assignments\nMake sure to knit to pdf ; ask the TA about knitting to pdf if you cannot figure it out. Be sure to submit under the right assignment entry by the due date!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]