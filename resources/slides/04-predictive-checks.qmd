---
subtitle: "STA 702: Lecture 4"
title: "Prior/Posterior Checks"
author: "Merlise Clyde"
institute: "Duke University"
format: 
  revealjs:
    theme: [simple, custom.scss]
    slide-number: true
    incremental: true
    scrollable: false
    controls: true
    fragments: true
    preview-links: auto
    logo: ../../img/icon.png
    footer: <https://sta702-F23.github.io/website/>
    chalkboard: 
      boardmarker-width: 1
      chalk-width: 2
      chalk-effect: 0
    embed-resources: false
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"    
editor: 
  markdown: 
    wrap: 72
execute: 
  echo: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
```


 

## Uses of Posterior Predictive {.smaller}

- Plot the entire density or summarize

  
 
- Available analytically for conjugate families

  

- Monte Carlo Approximation
$$p(y_{n+1} \mid y_1, \ldots y_n) \approx \frac 1 T \sum_{t = 1}^T  p(y_{n+t} \mid \theta^{(t)})$$ 
where $\theta^{(t)} \sim \pi(\theta \mid y_1, \ldots y_n)$ for $t = 1, \ldots, T$ 

  

- T samples from the posterior distribution

  

- Empirical Estimates & Quantiles from Monte Carlo Samples


 
## Models

- So far this all assumes we have a correct sampling model and a "reasonable"  prior distrbution


- George Box:  _All models are wrong but some are useful_

  

- "Useful" $\rightarrow$ model provides a good approximation; there aren't clear aspects of the data that are ignored or misspecified

-  how can we decide if a model is misspecified and needs to change?
 
## Example {.smaller}

- Poisson model
$$Y_i  \mid \theta \stackrel{iid}{\sim }\textsf{Poisson}(\theta) \qquad i = 1, \ldots, n$$
- How might our model be misspecified?

    - Poisson assumes that $\textsf{E}(Y_i) = \textsf{Var}(Y_i) = \theta$

    - it's _very_ common for data to be **over-dispersed** $\textsf{E}(Y_i) <  \textsf{Var}(Y_i)$
    
    - ignored additional structure in the data, i.e. data are not _iid_
    
    - **zero-inflation**  many more zero values than consistent with the poisson model 



## Posterior Predictive Checks {.smaller}

- Guttman (1967), Rubin (1984) proposed the use of [Posterior Predictive Checks (PPC)]{style="color:red}] for model criticism;  further developed by Gelman et al (1996)

-  the spirit of posterior predictive checks is that "_If my model is good, then its posterior predictive distribution will generate data that look like my oberved data_"



- $y^{\text{obs}}$ is the observed  data



- $y^{\text{rep}}$ is a new dataset sampled from the posterior predictive $p(y^{\text{rep}} \mid y^{\text{obs}})$ of size $n$  (same  size as the observed)


- Use a [diagnostic statistic]{style="color:red"}  $d(y)$ to capture some feature of the data that the model may fail to capture, say variance

- compare $d(y^{\text{obs}})$ to the reference distribution of $d(y^{\text{rep}})$ 

- Use Posterior Predictive P-value as a summary
$$ p_{PPC} = P(d(y^{\text{obs}}) > d(y^{\text{rep}}) \mid d(y^{\text{obs}}))
$$

 
##  Formally {.smaller}

- choose a "diagnostic statistic" $d(\cdot)$  that captures some summary of the data, e.g. $\textsf{Var}(y)$ for over-dispersion, where large values of the statistic would be surprising if the model were correct.

  

- $d(y^{\text{obs}}) \equiv d_{\textrm{obs}}$ value of statistic in observed data

  

- $d(y^{\text{rep}}_t)  \equiv d_{\textrm{pred}}$ value of  statistic for the $t$th random dataset drawn from the posterior predictive distribution
  1) Generate $\theta_t  \stackrel{iid}{\sim}p(\theta y^{\textrm{obs}})$
  2)  Generate $y^{\textrm{rep}_t} \mid \theta_t \stackrel{iid}{\sim} p(y \mid \theta_t)$
  3) Calculate $d(y^{\text{rep}}_t)$
  

- plot posterior predictive distribution of $d(y^{\text{rep}}_t)$ and add $d_{\textrm{obs}}$ 

  

- How _extreme_ is $t_{\textrm{obs}}$ compared to the distribution of $d(y^{\text{rep}})$?

- compute p-value $p_{PPC} = \frac 1 T \sum_t I(d(y^{\text{obs}}) > d(y^{\text{rep}}_t))$

 
##  Example with Over Dispersion {.smaller}

:::: {.columns}
::: {.column width="60%"}
```{r overdispersion-ex, fig=FALSE, echo=TRUE, eval=FALSE}
n = 100; phi = 1; mu = 5
theta.t = rgamma(n,phi,phi/mu)
y = rpois(n, theta.t)
a = 1; b = 1;
t.obs = var(y)

nT = 10000
t.pred = rep(NA, nT)
for (t in 1:nT) {
  theta.post = rgamma(1, a + sum(y),
                         b + n)
  y.pred = rpois(n, theta.post)
  t.pred[t] = var(y.pred)
}

hist(t.pred, 
     xlim = range(c(t.pred, t.obs)),
     xlab="var", 
     main="Posterior Predictive Distribution")

abline(v = t.obs, col="red")

```
:::

::: {.column width="40%"}
```{r overdispersion, fig=TRUE, echo=FALSE, fig.height=5, fig.width=5, out.width="90%"}
 n = 100; phi = 1; mu = 5
theta.t = rgamma(n,phi,phi/mu)
y = rpois(n, theta.t)
a = 1; b = 1;
t.obs = var(y)

nT = 10000
t.pred = rep(NA, nT)
for (t in 1:nT) {
  theta.post = rgamma(1, a + sum(y), b + n)
  y.pred = rpois(n, theta.post)
  t.pred[t] = var(y.pred)
}

hist(t.pred, xlim = range(c(t.pred, t.obs)),
     xlab="var", main="Posterior Predictive Distribution")

abline(v = t.obs, col="red")

```

:::
::::

 
## Zero Inflated Distribution {.smaller}

:::: {.columns}

::: {.column width="50%"}

```{r zero, fig=TRUE, echo=FALSE, fig.height=5, fig.width=5}
n = 1000
phi = 1
mu = 5
theta.t = rgamma(n, phi, phi/mu)
z = rbinom(n, 1, .90)
y = rpois(n, theta.t)*z

hist(y, breaks=25, main="")
```

:::

::: {.column width="50%"}
```{r}
#| label: PPP
ppp = mean(y == 0)

```

R Code to generate zero inflated 
```{r echo=TRUE, eval=FALSE}
n = 1000
mu = 5; phi = 1
theta.t = rgamma(n,phi,phi/mu)
z = rbinom(n, 1, .90)
y = rpois(n, theta.t)*z
```

- Let the $t()$ be the proportion of zeros
$$\begin{aligned}
d(y) & = \frac{\sum_{i = 1}^{n}1(y_i = 0)}{n} \\
     & = `r round(ppp, 2)`
\end{aligned}$$   
:::
::::

 
##  Posterior Predictive Distribution


```{r zeroPP, fig=TRUE, echo=FALSE, fig.height=4, fig.width=5, out.width="90%"}

a = 1; b = 1;
t.obs = mean(y == 0)

nT = 10000
t.pred = rep(NA, nT)
for (t in 1:nT) {
  theta.post = rgamma(1, a + sum(y), b + n)
  y.pred = rpois(n, theta.post)
  t.pred[t] = mean(y.pred == 0)
}

hist(t.pred, xlim = range(c(t.pred, t.obs)),
     xlab="Proportions of Zeros", main="Posterior Predictive Distribution")

abline(v = t.obs, col="red")


```

 
## Posterior Predictive p-values (PPPs) {.smaller}

- p-value is probability of seeing something  as extreme or more so under a hypothetical  "null" model

- from a frequentist perspect, one appealing property of p-values is that they should be uniformally distributed under the "null" model

  

- PPPs advocated by Gelman & Rubin in papers and BDA are not **valid** p-values.  They are do not have a  uniform  distribution under the hypothesis that the model is correctly specified

  

- the PPPs tend to be concentrated around 0.5, tends not to reject  (conservative)

  

- theoretical reason for the incorrect distribution is due to double use of the data

  

- **DO NOT USE as a formal test!**
use as a diagnostic plot to see how model might fall flat, but be cautious!

## Example: Bivariate Normal {.smaller}

:::: {.columns}
::: {.column width="50%"}
```{r, fig=TRUE,fig.width=5, fig.height=5}
n = 1000
theta = c(c(-1,5), c(5,8), c(0,0), c(3, 2))
sigma = 1*c(.5, .5, 1, 1, 3,3, 1,1)
y = matrix(rnorm(8*n, c(theta), sigma), ncol=2, byrow=T)
plot(y, ylab=expression(y[2]), xlab=expression(y[1])); 
points(matrix(theta, ncol=2, byrow=T), col="red", pch=16)

```
:::

::: {.column width="50%"}

```{r, fig=TRUE,fig.width=5, fig.height=5}
n = nrow(y)
R = 10000
d.rep = rep(NA, R)
var.rep = d.obs = d.rep
ybar = apply(y, 2, mean)
sd.y = apply(y, 2, sd)
sse = apply(y, 2, var)*(n-1)
var.obs = mean(sweep(y,2, ybar)^2)
for (r in 1:R) {
  phi.t = rgamma(n*2, (n-1)/2, sse/2 )
  theta.t = rnorm(n*2, ybar, sqrt(1/(phi.t * n) ))
  yrep = rnorm(n*2, theta.t, sqrt(1/phi.t))
  var.rep[r] = mean((yrep - ybar)^2)
  d.rep[r] = -sum(log(dnorm(yrep, ybar, sd.y*sqrt(1 + 1/n)) ))/n
  d.obs[r] = -sum(log(dnorm(y, ybar, sd.y*sqrt(1 + 1/n)) ))/n
}
#hist(c(d.rep, d.obs))
hist(c(var.rep, var.obs), xlab="avg squared error",
  main="average squared distance to the posterior mean")
abline(v=var.obs, col="red")
ppc = mean(var.rep > var.obs)
```

- PPP = `r round(ppc, 2)`

- What's happening?
:::
::::

## Problems with PPC {.smaller}

- Bayarri & Berger (2000) provides more discussion about why PPP are not always calibrated

- Double use of the data;  $Y^{\text{rep}}$ depends on the observed diagnostic in last case

- Bayarri & Berger propose the partial predicitve p-value  and condtional predictive p-value that avoids double use of the data by "removing" the contribution of $d_{\text{obs}}$ to the posterior for $\theta$ or conditioning an a statistic, such as the MLE of $\theta$

- heuristically, need the diagnostic to be independent of posterior for $\theta$ 

- not always easy to find!

- Moran  et al (2022) propose a workaround to avoid double use of the data  by spliting the data $y_{\text{obs}}, y_{\text{new}}$, use  $y_{\text{obs}}$, to learn $\theta$ and the other to calculate $d_{\textrm{new}}$  

- can be calculated via simulation easily



## POP-PC of Moran et al

```{r, fig=TRUE,fig.width=5, fig.height=5}
n = nrow(y)
R = 10000
d.rep = rep(NA, R)
var.rep = d.obs = d.rep
obs = sample(1:n, size=n/2, replace=FALSE)
y.obs = y[obs,]
ybar = apply(y.obs, 2, mean)
sd.y = apply(y.obs, 2, sd)
df = (nrow(y.obs) - 1)
sse = apply(y.obs, 2, var)*df
var.obs = mean(sweep(y[-obs,],2, ybar)^2)
for (r in 1:R) {
  phi.t = rgamma(n, df/2, sse/2 )
  theta.t = rnorm(n, ybar, sqrt(1/(phi.t * (df + 1))))
  yrep = rnorm(n, theta.t, sqrt(1/phi.t))
  var.rep[r] = mean((yrep - ybar)^2)
  }
#hist(c(d.rep, d.obs))
hist(c(var.rep, var.obs), xlab="avg squared error",
  main="average squared distance to the posterior mean")
abline(v=var.obs, col="red")
pop.ppc = mean(var.rep > var.obs)
```


- POP-PPC = `r round(pop.ppc, 2)`
 
## Modeling Over-Dispersion {.smaller}

- Original Model  $Y_i \mid \theta \sim \textsf{Poisson}(\theta)$

  

- cause of overdispersion is variation in the rate
$$ Y_i \mid \theta_i \sim \textsf{Poisson}(\theta_i)$$

  
- model variation via prior
$$\theta_i \sim \pi_\theta()$$

  

- $\pi_\theta()$ characterizes variation in the rate parameter across inviduals

  

- Simple Two Stage Hierarchical Model

 
## Example

$$\theta_i \sim \textsf{Gamma}(\phi \mu, \phi)$$
  

- Find pmf for $Y_i \mid \mu, \phi$

  

- Find $\textsf{E}[Y_i \mid \mu, \phi]$ and $\textsf{Var}[Y_i \mid \mu, \phi]$ 

  

- Homework: 
$$\theta_i \sim \textsf{Gamma}(\phi, \phi/\mu)$$
           
  

- Can either of these model zero-inflation?




## Modeling Perspectives {.smaller}

:::: {.columns}
::: {.column width="50%"}

1) start with a simple model 
  - ask if there are surprises through Posterior Checks
  - need calibrated  diagnostic(s) with good power
  - need these to work even if starting model is relatively complex
  - other informal diagnostics (residuals)
  - remodel if needed based on departures
  - Bayesian meaning?
  
:::

::: {.column width="50%"}

2) start with a fairly complex model or models
 - shrinkage to prevend overfitting
 - formal tests for simplifying models
 - methods to combine multiple models to express uncertaity
 - properties
 
 
:::
::::
  


