---
title: "Lecture 23: Bayesian Adaptive Regression Kernels"
author: "Merlise Clyde"
subtitle: "STA702"
institute: "Duke University"
format: 
  revealjs:
    theme: [simple, custom.scss]
    slide-number: true
    incremental: true
    scrollable: false
    controls: true
    fragments: true
    preview-links: auto
    smaller: true
    logo: ../../img/icon.png
    footer: <https://sta702-F23.github.io/website/>
    chalkboard: 
      boardmarker-width: 1
      chalk-width: 2
      chalk-effect: 0
    embed-resources: false
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"    
editor: 
  markdown: 
    wrap: 72
execute: 
  echo: false
number-sections: false
filters:
  - custom-numbered-blocks  
  - parse-latex
custom-numbered-blocks:
  groups:
    thmlike:
      colors: [948bde, 584eab]
      boxstyle: foldbox.simple
      collapse: false
      listin: [mathstuff]
    todos: default  
  classes:
    Theorem:
      group: thmlike
    Corollary:
      group: thmlike
    Conjecture:
      group: thmlike
      collapse: true  
    Definition:
      group: thmlike
      colors: [d999d3, a01793]
    Feature: default
    TODO:
      label: "To do"
      colors: [e7b1b4, 8c3236]
      group: todos
      listin: [stilltodo]
    DONE:
      label: "Done"
      colors: [cce7b1, 86b754]  
      group: todos  
---


```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1,
  width=72
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

## Nonparametric Regression
{{< include macros.qmd >}}

- Consider model $Y_1, \dots, Y_n \sim \N\left(\mu(\x_i), \sigma \right)$

- Mean function represented via a Stochastic Expansion
$$\mu(\x_i) = \sum_{j \le J} b_j(\x_i, \bfomega_j) \beta_j$$

- Multivariate  Gaussian Kernel $g$ with parameters $\bfomega = (\mean, \bfLambda)$
$$
b_j(\x, \bfomega_j) =  g(
\bfLambda_j^{1/2} (\bfx - \bfchi_j)) = \exp\left\{-\frac{1}{2}(\bfx - \mean_j)^T \bfLambda_j (\bfx -
  \mean_j)\right\}
$$






- introduce a Lévy measure  $\nu( d\beta, d \bfomega)$ 

-  Poisson distribution $J \sim \Poi(\nu_+)$ where 
$\nu_+\equiv \nu(\bbR\times\bfOmega) = \iint \nu(\beta, \bfomega) d \beta \,  d\bfomega$ 
$$\beta_j,\bfomega_j \mid J \iid \pi(\beta, \bfomega)
  \propto \nu(\beta,\bfomega)$$
  
## Function Spaces

- Conditions on $\nu$ 

     - need to have that $|\beta_j|$ are absolutely summable
     - finite number of large coefficients (in absolute value)
     - allows an infinite number of small $\beta_j \in [-\epsilon, \epsilon]$
     
 - satisfied if $$\iint_{\bbR \times \bfOmega}( 1 \wedge |\beta|) \nu(\beta, \bfomega) d\beta \, d\bfomega < \infty$$     
  
- Mean function $\E[Y_i \mid \tb] = \mu(\x_i, \tb)$ falls in some class of nonlinear functions based on $g$ and prior on $\bfLambda$

     + Besov Space
     + Sobolov Space

 




## Inference via Reversible Jump MCMC


- number of support points $J$ varies from iteration to iteration
   + add a new point (birth)
   + delete an existing point (death)
   + combine two points (merge)
   + split a point into two
- update existing point(s)  

- can be much faster than shrinkage or BMA with a fixed but large $J$







## So far 

- more parsimonious than "shrinkage" priors or SVM with fixed $J$
- allows for increasing number of support points as $n$ increases (adapts to smoothness)
- no problem with non-normal data, non-negative functions or even discontinuous functions
- credible and prediction intervals; uncertainty quantification
- robust alternative to Gaussian Process Priors

-  But - hard to scale up random scales & locations as dimension of $\x$ increases
- Alternative Prior Approximation II


## Higher Dimensional $\X$

MCMC is (currently) too slow in higher dimensional space to allow 

-  $\bfchi$ to be completely arbitrary; restrict support to
  observed $\{\bfx_i\}$ like in SVM  (or observed quantiles)
  
-  use a common diagonal $\bfLambda$ for all kernels

- Kernels take form:
\begin{eqnarray*}
b_j(\bfx, \bfomega_j) & = & \prod_d \exp\{ -\frac{1}{2} \lambda_d (x_d - \chi_{dj})^2
\} \\
\mu(\bfx) & =  & \sum_j b_j(\bfx, \bfomega_j) \beta_j
\end{eqnarray*}


- accomodates nonlinear interactions among variables

- **ensemble model** like random forests, boosting, BART, SVM

## Approximate Lévy Prior II

- $\alpha$-Stable process: $\nu(d\beta, d \bfomega) = \gamma c_\alpha |\beta|^{-(\alpha + 1)} d\beta \ \pi(d \bfomega)$

- Continuous Approximation to an $\alpha$-Stable process via a Student $t(\alpha, 0, \epsilon)$:
$$\nu_\epsilon(d\beta, d \bfomega) = \gamma c_\alpha (\beta^2 + \alpha
\epsilon^2)^{-(\alpha + 1)/2} d\beta \ \pi(d \bfomega)$$


- Based on the following hierarchical prior
\begin{aligned}
  \beta_j \mid \phi_j  & \ind  \N(0,  \varphi_j^{-1}) &  &
  \phi_j      \ind  \Gam\left(\frac{\alpha}{2}, \frac{\alpha \epsilon^2}{2}\right) \\
   J & \sim  \Poi(\nu^+_\epsilon) &  &
\nu^+_\epsilon   =  \nu_\epsilon(\bbR, \bfOmega) = \gamma \frac{\alpha ^{1- \alpha/2} \Gamma(\alpha/2)}{\epsilon^\alpha  2^{1 - \alpha} \Gamma(1 - \alpha/2)}\pi(\bfOmega)
\end{aligned}

. . .

::: {.callout-tip}
## Key Idea:  need to have variance/scale of coefficients decrease as $J$ increases
:::

## Limiting Case 
\begin{eqnarray*}
    \beta_j   \mid  \varphi_j & \ind &\N(0, 1/\varphi_j) \\
      \varphi_j & \iid & \Gam(\alpha/2, 0)
  \end{eqnarray*}

. . .

Notes:

- Require $0 < \alpha < 2$  Additional restrictions  on $\omega$   
 
-  Tipping's **Relevance Vector Machine** corresponds to $\alpha =
  0$  (improper posterior!)  
- Provides an extension of **Generalized Ridge Priors**
      to infinite dimensional case  
-  Cauchy process corresponds to $\alpha = 1$       
- Infinite dimensional analog of Cauchy priors

## Simplification with $\alpha = 1$

- Poisson number of points $J \sim \Poi(\gamma/\eps)$ 

- Given $J$, $[ n_1 : n_n] \sim  \Mult(J, 1/(n+1))$ points
    supported at each kernel located at $\bfx_j$   
 
 
- Aggregating, the regression mean function can be rewritten as
$$\mu(\bfx) = \sum_{i=0}^n \tilde{\beta}_i b_j(\bfx, \bfomega_i), \quad
  \tilde{\beta}_i = \sum_{\{j \, \mid \bfchi_j = \bfx_i\}} \beta_j$$


. . .

::: {.callout-tip} 

if $\alpha = 1$, not only is the Cauchy process infinitely
divisible, the _approximated Cauchy prior distributions_ for $\beta_j$ are also infinitely divisible!

:::

$$
  \tilde{\beta}_i  \ind
  \N(0, n_i^2 \tilde{\varphi}_i^{-1}), \qquad
  \tilde{\varphi}_i \iid \Gam( 1/2, \eps^2/2)
$$
 
At most $n$ non-zero coefficients!

## Inference for Normal Model


- integrate out $\tilde{\b}$ for marginal likelihood $\cal{L}(J, \{n_i\}, \{\tilde{\varphi}_i\}, \sigma^2, \lambdab)$
$$\Y \mid \sigma^2, \{n_i\}, \{\tilde{\varphi}_i\}, \lambdab \sim \N\left(\zero_n, \sigma^2 \I_n + \mathbf{b} \, \diag\left(\frac{n_i^2}{\tilde{\varphi}_i} \right)\mathbf{b}^T\right)$$

- if $n_i = 0$  then the kernel located at $\bfx_i$ drops out so we still need birth/death steps via RJ-MCMC for $\{n_i, \tilde{\varphi}_i\}$

- for $J < n$  take advantage of  the Woodbury matrix identity for matrix inversion likelihood
$$(A + U C V)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U){-1} V A^{-1}$$
- update $\sigma^2, \lambdab$ via usual MCMC 

- for fixed $J$ and $\{n_i\}$, can update $\{\tilde{\varphi}_i\}, \sigma^2, \lambdab)$ via usual MCMC (fixed dimension)


## Feature Selection in Kernel

-  Product structure allows interactions between variables    
-  Many input variables may be irrelevant   
-  Feature selection; if $\lambda_d = 0$ variable $\x_d$ is removed
  from all kernels   
-  Allow point mass on $\lambda_d = 0$ with probability $p_\lambda
  \sim \Be(a,b)$  (in practice have used $a = b = 1$  
- can also constrain all $\lambda_d$ that are non-zero to be equal   across dimensions


. . .

Binary Regression

- add latent Gaussian variable as in Albert & Chib

##  bark package



```{r}
#| echo: true
library(bark)
set.seed(42)
n = 500
circle2 = data.frame(sim_circle(n, dim = 2))

```




```{r}
#| echo: true
#| eval: false

plot(x.1 ~ x.2, data=circle2, col=y+1)
```

## Circle Data Classification

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
#| out-width: 60%

plot(x.1 ~ x.2, data=circle2, col=y+1)
```


## BARK Classification

```{r}
#| echo: true
#| cache: false
set.seed(42)
train = sample(1:n, size = floor(n/2), rep=FALSE)
circle2.bark = bark(y ~ . , data = circle2,
                    subset = train,
                    testdata = circle2[-train,],
                    classification = TRUE,
                    printevery = 10000,
                    selection = TRUE, 
                    common_lambdas = TRUE)
```

- `classification = TRUE` for probit regression

- `selection = TRUE` allows some of the $\lambda_j$ to be 0

- `common_lambdas = TRUE` sets all (non-zero) $\lambda_j$ to a common $\lambda$

## Missclassification 

```{r}
#| echo: true
#| out-width: 60%
#| fig-width: 5
#| fig-height: 5
misscl = (circle2.bark$yhat.test.mean > 0) != circle2[-train, "y"]
plot(x.1 ~ x.2, data=circle2[-train,], pch=circle2[-train, "y"]+1, col=misscl + 1)
title(paste("Missclassification Rate", round(mean(misscl), 4)))
```


## Support Vector Machines (SVM) & BART

```{r}
#| echo: true
library(e1071)
circle2.svm = svm(y ~ x.1 + x.2, data=circle2[train,],
                  type="C")
pred.svm = predict(circle2.svm, circle2[-train,])
mean(pred.svm != circle2[-train, "y"])
```

```{r}
#| echo: true
```


```{r}
#| eval: true
#| echo: true
#| include: false
suppressMessages(library(BART))
circle.bart = pbart(x.train = circle2[train, 1:2],
                    y.train = circle2[train, "y"])
pred.bart = predict(circle.bart, circle2[-train, 1:2])
misscl.bart = mean((pred.bart$prob.test.mean > .5) != 
              circle2[-train, "y"])
```

```{r}
#| eval: false
#| echo: true
#| include: true
suppressMessages(library(BART))
circle.bart = pbart(x.train = circle2[train, 1:2],
                    y.train = circle2[train, "y"])
pred.bart = predict(circle.bart, circle2[-train, 1:2])
misscl.bart = mean((pred.bart$prob.test.mean > .5) != 
              circle2[-train, "y"])
```

```{r}
mean((pred.bart$prob.test.mean > .5) != 
       circle2[-train, "y"])
```

## Comparisons

| Data Sets   | n   | p   | BARK-D  | BARK-SE  | BARK-SD  | SVM   | BART |
|------------ |----:|---:|--------:|---------:|---------:|-------:|-------:|
|   Circle 2  | 200  |  2  | 4.91%  | 1.88%  | 1.93%  | 5.03%  | 3.97%  |
|   Circle 5  | 200  |  5  | 4.70%  | 1.47%  | 1.65%  | 10.99% | 6.51% |
|   Circle 20 | 200  | 20  | 4.84%  | 2.09%  | 3.69%  | 44.10% | 15.10% |
|   Bank      | 200  | 6   | 1.25%  | 0.55%  | 0.88%  | 1.12%  | 0.50% |
|   BC        | 569  | 30  | 4.02%  | 2.49%  | 6.09%  | 2.70%  | 3.36% |
|  Ionosphere | 351  | 33  | 8.59%  | 5.78%  |10.87%  | 5.17%  | 7.34% |


- BARK-D:  different $\lambda_d$ for each dimension
- BARK-SE: selection and equal $\lambda_d$ for non-zero $\lambda_d$
- BARK-SD: selection and different $\lambda_d$ for non-zero $\lambda_d$

## Needs & Limitations


- NP Bayes of many flavors often does better than frequentist methods
(BARK, BART, Treed GP, more) 
- Hyper-parameter specification - theory & computational
  approximation 
- asymptotic theory (rates of convergence)  
- need faster code for BARK that is easier for users (BART & TGP are
  great!)  
- Can these models be added to JAGS, STAN, etc instead of
  stand-alone R packages 
- With availability of code what are caveats for users?

 
## Summary

Lévy Random Field Priors & LARK/BARK models:
 
- Provide limit of finite dimensional priors (GRP & SVSS) to infinite
  dimensional setting 
- Adaptive bandwidth for kernel regression 
- Allow flexible generating functions 
- Provide sparser representations compared to SVM & RVM, with
    coherent Bayesian interpretation 
- Incorporation of  prior knowledge if available 
- Relax assumptions of equally spaced data and Gaussian likelihood 
- Hierarchical Extensions 
- Formulation allows one to define stochastic processes on
    arbitrary spaces (spheres, manifolds)


