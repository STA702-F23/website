---
title: "Lecture 14: Basics of Bayesian Hypothesis Testing"
author: "Merlise Clyde"
subtitle: "STA702"
institute: "Duke University"
format: 
  revealjs:
    theme: [simple, custom.scss]
    slide-number: true
    incremental: true
    scrollable: false
    controls: true
    fragments: true
    preview-links: auto
    smaller: true
    logo: ../../img/icon.png
    footer: <https://sta702-F23.github.io/website/>
    chalkboard: 
      boardmarker-width: 1
      chalk-width: 2
      chalk-effect: 0
    embed-resources: false
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"    
editor: 
  markdown: 
    wrap: 72
execute: 
  echo: false
number-sections: false
filters:
  - custom-numbered-blocks  
custom-numbered-blocks:
  groups:
    thmlike:
      colors: [948bde, 584eab]
      boxstyle: foldbox.simple
      collapse: false
      listin: [mathstuff]
    todos: default  
  classes:
    Theorem:
      group: thmlike
    Corollary:
      group: thmlike
    Conjecture:
      group: thmlike
      collapse: true  
    Definition:
      group: thmlike
      colors: [d999d3, a01793]
    Feature: default
    TODO:
      label: "To do"
      colors: [e7b1b4, 8c3236]
      group: todos
      listin: [stilltodo]
    DONE:
      label: "Done"
      colors: [cce7b1, 86b754]  
      group: todos  
---


```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
```
##  Feature Selection via Shrinkage
{{< include macros.qmd >}}

- modal estimates in regression models under certain shrinkage priors will set a subset
of coefficients to zero

- not true with posterior mean

- multi-modal posterior 

- no prior probability that coefficient is zero

- how should we approach selection/hypothesis testing?

- Bayesian Hypothesis Testing

## Basics of Bayesian Hypothesis Testing


Suppose we have univariate data $Y_i \overset{iid}{\sim} \mathcal{N}(\theta, 1)$, $\Y = (y_i, \ldots, y_n)^T$

 

- goal is to test $\mathcal{H}_0: \theta = 0; \ \ \text{vs } \mathcal{H}_1: \theta \neq 0$ 


- Additional unknowns are $\mathcal{H}_0$ and $\mathcal{H}_1$

- Put a prior on the actual hypotheses/models, that is, on $\pi(\mathcal{H}_0) = \Pr(\mathcal{H}_0 = \text{True})$ and $\pi(\mathcal{H}_1) = \Pr(\mathcal{H}_1 = \text{True})$.

- (Marginal) Likelihood of the hypotheses: 
$\cal{L}(\mathcal{H}_i) \propto p( \y \mid \mathcal{H}_i)$
      
 
. . .

$$p( \y \mid \mathcal{H}_0) = \prod_{i = 1}^n (2 \pi)^{-1/2} \exp{- \frac{1}{2} (y_i - 0)^2}$$

. . . 

$$p( \y \mid \mathcal{H}_1)  = \int_\Theta p( \y \mid \mathcal{H}_1, \theta) p(\theta \mid \mathcal{H}_1) \, d\theta$$
 
## Bayesian Approach 

- Need priors distributions on parameters under each hypothesis
    
    - in our simple normal model, the only additional unknown parameter is $\theta$
    - under $\mathcal{H}_0$, $\theta = 0$ with probability 1
    - under $\mathcal{H}_0$, $\theta \in \mathbb{R}$ we could take $\pi(\theta) = \mathcal{N}(\theta_0, 1/\tau_0^2)$.

 

-   Compute marginal likelihoods for each hypothesis, that is, $\cal{L}(\mathcal{H}_0)$ and $\cal{L}(\mathcal{H}_1)$.  

 

- Obtain posterior probabilities of $\cal{H}_0$ and $\cal{H}_1$ via Bayes Theorem.
$$
\begin{split}
\pi(\mathcal{H}_1 \mid \y) = \frac{ p( \y \mid \mathcal{H}_1) \pi(\mathcal{H}_1) }{ p( \y \mid \mathcal{H}_0) \pi(\mathcal{H}_0) + p( \y \mid \mathcal{H}_1) \pi(\mathcal{H}_1)}
\end{split}
$$
- Provides a joint posterior distribution for $\theta$ and $\mathcal{H}_i$:  $p(\theta \mid   \mathcal{H}_i,  \y)$ and $\pi(\mathcal{H}_i \mid \y)$
 
## Hypothesis Tests via  Decision Theory

- Loss function for hypothesis testing

    - $\hat{\cal{H}}$ is the chosen hypothesis
 
    - $\cal{H}_{\text{true}}$ is the true hypothesis, $\cal{H}$ for short
 
 

- Two types of errors:

    - Type I error:  $\hat{\cal{H}} = 1$  and  $\cal{H} = 0$

 

    - Type II error:  $\hat{\cal{H}} = 0$  and  $\cal{H} = 1$

 

- Loss function:
$$L(\hat{\cal{H}}, \cal{H}) =  w_1  \, 1(\hat{\cal{H}} = 1, \cal{H} = 0) + w_2 \, 1(\hat{\cal{H}} = 0, \cal{H} = 1)$$

    - $w_1$ weights how bad it is to make a Type I error

    - $w_2$ weights how bad it is to make a Type II error

 
## Loss Function Functions and Decisions

- Relative weights $w = w_2/w_1$
$$L(\hat{\cal{H}}, \cal{H}) =   \, 1(\hat{\cal{H}} = 1, \cal{H} = 0) + w \, 1(\hat{\cal{H}} = 0, \cal{H} = 1)$$
 

- Special case $w=1$
$$L(\hat{\cal{H}}, \cal{H}) =    1(\hat{\cal{H}} \neq \cal{H})$$ 
- known as 0-1 loss (most common)

 

- Bayes Risk (Posterior Expected Loss)
$$\textsf{E}_{\cal{H} \mid  \y}[L(\hat{\cal{H}}, \cal{H}) ] =
1(\hat{\cal{H}} = 1)\pi(\cal{H}_0 \mid  \y) +  1(\hat{\cal{H}} = 0) \pi(\cal{H}_1 \mid  \y)$$



 

- Minimize loss by picking hypothesis with the highest posterior probability 



 
## Bayesian hypothesis testing

- Using Bayes theorem,
$$
\begin{split}
\pi(\mathcal{H}_1 \mid \y) = \frac{ p( \y \mid \mathcal{H}_1) \pi(\mathcal{H}_1) }{ p( \y \mid \mathcal{H}_0) \pi(\mathcal{H}_0) + p( \y \mid \mathcal{H}_1) \pi(\mathcal{H}_1)},
\end{split}
$$


- If $\pi(\mathcal{H}_0) = 0.5$ and $\pi(\mathcal{H}_1) = 0.5$ _a priori_, then
$$
\begin{split}
\pi(\mathcal{H}_1 \mid \y) & = \frac{ 0.5 p( \y \mid \mathcal{H}_1) }{ 0.5 p( \y \mid \mathcal{H}_0) + 0.5 p( \y \mid \mathcal{H}_1) } \\
\\
& = \frac{ p( \y \mid \mathcal{H}_1) }{ p( \y \mid \mathcal{H}_0) + p( \y \mid \mathcal{H}_1) }= \frac{ 1 }{ \frac{p( \y \mid \mathcal{H}_0)}{p( \y \mid \mathcal{H}_1)} + 1 }\\
\end{split}
$$

 


 
## Bayes factors

- The ratio $\frac{p( \y \mid \mathcal{H}_0)}{p( \y \mid \mathcal{H}_1)}$  is a ratio of marginal likelihoods and is known as the **Bayes factor** in favor of $\mathcal{H}_0$, written as $\mathcal{BF}_{01}$. Similarly, we can compute $\mathcal{BF}_{10}$ via the inverse ratio.




- Bayes factors provide a weight of evidence in the data in favor of one model over another.
  and are used as an alternative to the frequentist p-value.

 

- **Rule of Thumb**: $\mathcal{BF}_{01} > 10$ is strong evidence for $\mathcal{H}_0$;  $\mathcal{BF}_{01} > 100$ is decisive evidence for $\mathcal{H}_0$.

 

- In the example (with equal prior probabilities),
$$
\begin{split}
\pi(\mathcal{H}_1 \mid \y) = \frac{ 1 }{ \frac{p( \y \mid \mathcal{H}_0)}{p( \y \mid \mathcal{H}_1)} + 1 } = \frac{ 1 }{ \mathcal{BF}_{01} + 1 } \\
\end{split}
$$


- the higher the value of $\mathcal{BF}_{01}$, that is, the weight of evidence in the data in favor of $\mathcal{H}_0$, the lower the marginal posterior probability that $\mathcal{H}_1$ is true.
  
 

- $\mathcal{BF}_{01} \uparrow$, $\pi(\mathcal{H}_1 \mid \y) \downarrow$.




 
## Posterior Odds and Bayes Factors


- Posterior odds $\frac{\pi(\mathcal{H}_0 \mid \y)}{\pi(\mathcal{H}_1 \mid \y)}$
$$
\begin{split}
\frac{\pi(\mathcal{H}_0 | \y)}{\pi(\mathcal{H}_1 | \y)} & = \frac{ p( \y |\mathcal{H}_0) \pi(\mathcal{H}_0) }{ p( \y | \mathcal{H}_0) \pi(\mathcal{H}_0) + p( \y | \mathcal{H}_1) \pi(\mathcal{H}_1)} \div \frac{ p( \y | \mathcal{H}_1) \pi(\mathcal{H}_1) }{ p( \y  \mathcal{H}_0) \pi(\mathcal{H}_0) + p( \y | \mathcal{H}_1) \pi(\mathcal{H}_1)}\\
\\
& = \frac{ p( \y | \mathcal{H}_0) \pi(\mathcal{H}_0) }{ p( \y | \mathcal{H}_0) \pi(\mathcal{H}_0) + p( \y | \mathcal{H}_1) \pi(\mathcal{H}_1)} \times \frac{ p( \y | \mathcal{H}_0) \pi(\mathcal{H}_0) + p( \y | \mathcal{H}_1) \pi(\mathcal{H}_1)}{ p( \y | \mathcal{H}_1) \pi(\mathcal{H}_1) }\\
\\
\therefore \underbrace{\frac{\pi(\mathcal{H}_0 \mid \y)}{\pi(\mathcal{H}_1 \mid \y)}}_{\text{posterior odds}} & = \underbrace{\frac{ \pi(\mathcal{H}_0) }{ \pi(\mathcal{H}_1) }}_{\text{prior odds}} \times \underbrace{\frac{ p( \y \mid \mathcal{H}_0) }{ p( \y \mid \mathcal{H}_1) }}_{\text{Bayes factor } \mathcal{BF}_{01}} \\
\end{split}
$$


 

- The Bayes factor can be thought of as the factor by which our prior odds change (towards the posterior odds) in the light of the data.




 
##  Likelihoods & Evidence

Maximized Likelihood. $n = 10$
```{r lik, fig.height=4.5,echo=FALSE}
n = 10; y = 1.96/sqrt(n)
th <- seq(y - 3/sqrt(n), y + 3/sqrt(n), length=1000)
plot(th,dnorm(th,y,sqrt(1/n)),
      type="l", lwd=2,
      col="blue",xlab=expression(theta),
      ylab=expression(l(theta)))
#m = integrate(f = function(x) {dbinom(y,n,x)}, low=0.00001, upper=.99999)
#abline(h=m$value, lty=4, lwd=2, col="red")
segments(0, 0, 0, dnorm(0, y, sqrt(1/n)), lty=2, lwd=2, col=2)
abline(h=dnorm(y,y, sqrt(1/n)), lty=2, lwd=2, col="black")

pval = pnorm(0,  abs(y), sqrt(1/n))*2
```

p-value = `r round(pval, 4)`
 
##  Marginal Likelihoods & Evidence

Maximized & Marginal Likelihoods
```{r marglik, fig.height=4.5, echo=F}
v0 = 1; tau0 = 1/v0
marg.H1 = dnorm(0, y, sqrt(v0 + 1/n))
theta.n = n*y/(n + tau0)
tau.n = n + tau0
plot(th,dnorm(th,y,sqrt(1/n)),
      type="l", lwd=2, ylim=c(0, dnorm(theta.n, theta.n, sqrt(1/tau.n))),
      col="blue",xlab=expression(theta),
      ylab=expression(pi(theta)))
lines(th, dnorm(th, 0, sqrt(v0)), col="red", lwd=2)
marg.H1 = dnorm(0, y, sqrt(v0 + 1/n))
segments(0, 0, 0, marg.H1, lty=3, lwd=2, col="purple")
abline(h =marg.H1, lty=2, lwd=3, col="purple")
#m = integrate(f = function(x) {dbinom(y,n,x)}, low=0.00001, upper=.99999)
#abline(h=m$value, lty=4, lwd=2, col="red")
lines(th, dnorm(th, theta.n, sqrt(1/tau.n)), lty=1,  lwd=3, col="purple")
marg.H0 = dnorm(0, y, sqrt(1/n))
segments(0, 0, 0, marg.H0, lty=3, lwd=2, col="blue")
#abline(h=dnorm(y,y, sqrt(1/n)), lty=3, lwd=2, col="black")
abline(h=marg.H0, lty=3, lwd=3, col="blue")
bf10 = marg.H1/marg.H0
bf01 = 1/bf10
post.prob.H1 = 1/(1 + bf01)
legend("topright", legend=c("p(y | H0)", "p(y | H1)", "likelihood", "prior", "posterior"), 
       col=c("blue", "purple", "blue", "red", "purple"), 
       lty=c(3,2,1,1,1))

#cand = dnorm(0,0, sqrt(v0))/dnorm(0, theta.n, sqrt(1/tau.n))
```

  $\cal{BF}_{10}$ = `r round(bf10, 2)` or   $\cal{BF}_{01}$ = `r round(1/bf10, 2)` 
  
  Posterior Probability of $\cal{H}_0$ = `r round(1 - post.prob.H1, 4)`
  
 
## Candidate's Formula (Besag 1989)


Alternative expression for BF based on Candidate's Formula or Savage-Dickey ratio 
$$\cal{BF}_{01} = \frac{p( \y \mid \cal{H}_0)}
       {p( \y \mid \cal{H}_1)} =
  \frac{\pi_\theta(0 \mid \cal{H}_1, \y)} 
       {\pi_\theta(0 \mid \cal{H}_1)}$$


. . .

$$\pi_\theta(\theta \mid \cal{H}_i, \y)  =  \frac{p(\y \mid \theta, \cal{H}_i) \pi(\theta \mid \cal{H}_i)} {p(\y \mid \cal{H}_i)}  \Rightarrow  
p(\y \mid \cal{H}_i)   = \frac{p(\y \mid \theta, \cal{H}_i) \pi(\theta \mid \cal{H}_i)} {\pi_\theta(\theta \mid \cal{H}_i, \y)}$$

. . .


$$\cal{BF}_{01}  = \frac{\frac{p(\y \mid \theta, \cal{H}_0) \pi(\theta \mid \cal{H}_0)} {\pi_\theta(\theta \mid \cal{H}_0, \y)} } { \frac{p(\y \mid \theta, \cal{H}_1) \pi(\theta \mid \cal{H}_1)} {\pi_\theta(\theta \mid \cal{H}_1, \y)}}  =   \frac{\frac{p(\y \mid \theta = 0) \delta_0(\theta)} {\delta_0(\theta)} } { \frac{p(\y \mid \theta, \cal{H}_1) \pi(\theta \mid \cal{H}_1)} {\pi_\theta(\theta \mid \cal{H}_1, \y)}} 
 =   \frac{p(\y \mid \theta = 0)}{p(\y \mid \theta, \cal{H}_1)} 
   \frac{\delta_0(\theta)} {\delta_0(\theta)}  \frac{\pi_\theta(\theta \mid \cal{H}_1, \y)}{\pi(\theta \mid \cal{H}_1)} $$ 


- Simplifies to the ratio of the posterior to prior densities  when evaluated $\theta$ at zero

## Prior

Plots were based on a $\theta \mid \cal{H}_1 \sim \textsf{N}(0, 1)$ 

 

- centered at value for $\theta$ under $\cal{H}_0$  (goes back to Jeffreys)

 

- "unit information prior"  equivalent to a prior sample size is 1


- is this a "reasonable prior"?

    - What happens if $n \to \infty$?

 

    - What happens of $\tau_0 \to 0$ ?   (less informative)

 



## Choice of Precision

:::: {.columns}

::: {.column width=50%}

```{r marglik-tau10, fig.height=6, echo=F}
v0 = 10; tau0 = 1/v0
marg.H1 = dnorm(0, y, sqrt(v0 + 1/n))
theta.n = n*y/(n + tau0)
tau.n = n + tau0
plot(th,dnorm(th,y,sqrt(1/n)),
      type="l", lwd=2, ylim=c(0, dnorm(theta.n, theta.n, sqrt(1/tau.n))),
      col="blue",xlab=expression(theta),
      ylab=expression(pi(theta)))
lines(th, dnorm(th, 0, sqrt(v0)), col="red", lwd=2)
marg.H1 = dnorm(0, y, sqrt(v0 + 1/n))
segments(0, 0, 0, marg.H1, lty=3, lwd=2, col="purple")
abline(h=marg.H1, lty=2, lwd=3, col="purple")
lines(th, dnorm(th, theta.n, sqrt(1/tau.n)), lty=1,  lwd=3, col="purple")
marg.H0 = dnorm(0, y, sqrt(1/n))
segments(0, 0, 0, marg.H0, lty=2, lwd=2, col="blue")
abline(h=marg.H0, lty=3, lwd=2, col="blue")
bf10 = marg.H1/marg.H0
bf01 = 1/bf10
post.prob.H1 = 1/(1 + bf01)
legend("topright", legend=c("p(y | H0)", "p(y | H1)", "likelihood", "prior", "posterior"), 
       col=c("blue", "purple", "blue", "red", "purple"), 
       lty=c(3,2,1,1,1), lwd=3)
```

- $\tau_0 = 1/`r v0`$


- Bayes Factor for $\cal{H}_0$ to $\cal{H}_1$ is $`r round(bf01, 2)`$



- Posterior Probability of $\cal{H}_0$ = `r round(1 - post.prob.H1, 4)`

:::

::: {.column width=50%}

```{r marglik-tau1000, fig.height=6, echo=F}
v0 = 1000; tau0 = 1/v0
marg.H1 = dnorm(0, y, sqrt(v0 + 1/n))
theta.n = n*y/(n + tau0)
tau.n = n + tau0
plot(th,dnorm(th,y,sqrt(1/n)),
      type="l", lwd=2, ylim=c(0, dnorm(theta.n, theta.n, sqrt(1/tau.n))),
      col="blue",xlab=expression(theta),
      ylab=expression(pi(theta)))
lines(th, dnorm(th, 0, sqrt(v0)), col="red", lwd=3)
marg.H1 = dnorm(0, y, sqrt(v0 + 1/n))
segments(0, 0, 0, marg.H1, lty=2, lwd=3, col="purple")
abline(h=marg.H1, lty=3, lwd=3, col="purple")
lines(th, dnorm(th, theta.n, sqrt(1/tau.n)), lty=3,  lwd=3, col="purple")
marg.H0 = dnorm(0, y, sqrt(1/n))
segments(0, 0, 0, marg.H0, lty=2, lwd=2, col="blue")
abline(h=marg.H0, lty=3, lwd=2, col="blue")
bf10 = marg.H1/marg.H0
bf01 = 1/bf10
post.prob.H1 = 1/(1 + bf01)
legend("topright", legend=c("p(y | H0)", "p(y | H1)", "likelihood", "prior", "posterior"), 
       col=c("blue", "purple", "blue", "red", "purple"), 
       lty=c(3,2,1,1,1), lwd=3)
```

- $\tau_0 = 1/`r v0`$


- Bayes Factor for $\cal{H}_0$ to $\cal{H}_1$ is $`r round(bf01, 2)`$



- Posterior Probability of $\cal{H}_0$ = `r round(1 - post.prob.H1, 4)`

:::

::::


## Vague Priors & Hypothesis Testing



- As $\tau_0 \to 0$ the $\cal{BF}_{01} \to \infty$ and  $\Pr(\cal{H}_0 \mid \y \to 1$! 



- As we use a less & less informative prior  for $\theta$ under $\cal{H}_1$ we obtain more & more evidence for $\cal{H}_0$ over $\cal{H}_1$!



- Known as **Bartlett's Paradox** - the paradox is that a seemingly non-informative prior for $\theta$ is very informative about $\cal{H}$!



- General problem with nested sequence of models.  If we choose vague priors on the additional parameter in the larger model we will be favoring the smaller models under consideration!


- Similar phenomenon with increasing sample size (**Lindley's Paradox**)

. . .

::: {.callout-warning}
**Bottom Line** Don't use vague priors!
:::


. . .

What should we use then?