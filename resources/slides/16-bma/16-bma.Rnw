\documentclass[]{beamer}
%\documentclass[handout]{beamer}
% ***************************************************************
% for handout, change only this...
%   \documentclass[twocolumn]{article}
%   \usepackage{beamerarticle}
%   \setlength{\textwidth}{7.5in}
%   \setlength{\textheight}{9.8in}
%   \setlength{\topmargin}{-1in}
%   \setlength{\oddsidemargin}{-.52in}
%   \setlength{\evensidemargin}{-.52in}

%\usepackage{beamerprosper}
%\usetheme{Warsaw}
%\usecolortheme{orchid}

\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{macros}
\title{Bayesian Variable Selection \&  Bayesian Model Averaging}
 
\author{Hoff Chapter 9, Liang et al 2008, Hoeting et al (1999), Clyde \&
 George (2004)}
\date{\today}
%\Logo(-1.9,7.3){\includegraphics[width=.5in]{../eps/duke}}
% Optional: text to put in the bottom of each slide.
% By default, the title of the talk will be placed there.
%\slideCaption{\textit{October 28, 2005 }}

\begin{document}
\SweaveOpts{concordance=TRUE}

%\SweaveOpts{prefix.string=tmpout/t, split=TRUE, ae=FALSE, height=5,width=6}
%\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}
% make the title slide
\maketitle







\begin{frame}
  \frametitle{Zellner's $g$-prior}
Zellner's g-prior(s) $\b \mid \phi \sim \N(\bv_0, g
    (\X^T\X)^{-1}/\phi)$ \pause

$$\b \mid \Y, \phi \sim \N\left( \frac{g}{1 + g} \bhat +  \frac{1}{1 + g}
\bv_0, \frac{g}{1 + g} (\X^T\X)^{-1} \phi^{-1} \right)$$ \pause

\begin{itemize}
\item Invariance: Require posterior of   $\X \b$  equal the posterior of $\X \H \alphav$
\pause   ($\a_0 = \H^{-1} \bv_0$)  (  $\bv_0 = \zero$)

\item Choice of $g$?  \pause
\item $\frac{g}{1 + g}$  weight given to the data \pause
\item Fixed $g$ effect does not vanish as $n \to \infty$
\item Use $g = n$ or place a prior diistribution on $g$
\end{itemize}


\end{frame}
\begin{frame}
  \frametitle{Shrinkage}
 Posterior mean under  $g$-prior  with $\bv_0 = 0$
$\frac{g}{1 +g} \bhat $

\centerline{\includegraphics[height=3in]{shrinkage}}
\end{frame}


\begin{frame}\frametitle{Ridge Regression}

\begin{itemize}

\item If $\X^T\X$ is nearly singular, certain  elements of $\b$ or (linear combinations of $\b$) may have huge variances under the $g$-prior (or flat prior) as the MLEs are highly unstable!


\item {\bf Ridge regression} protects against the explosion of variances and ill-conditioning with the conjugate prior:

$$\b \mid \phi \sim \textsf{N}(0, \frac{1}{\phi \lambda} \I_p)$$

\item Posterior for $\b$  (conjugate case)

$$\b \mid \phi, \lambda, \Y \sim \N \left((\lambda \I_p + \X^T\X)^{-1} \X^T \Y,  \frac{1}{\phi}(\lambda \I_p + \X^T\X)^{-1}\right)$$

\item induces shrinkage as well!
\end{itemize}
\end{frame}

\begin{frame} \frametitle{Model Choice ?}
\begin{itemize}
  \item Redundant variables lead to unstable estimates \pause
  \item Some variables may not be relevant ($\beta_j = 0$) \pause
  \item Can we infer a "good" model from the data?
  \item Expand model hierarchically to introduce another latent variable $\g$ that encodes models $\Mg$
  $\g = (\gamma_1, \gamma_2, \ldots \gamma_p)^T$ where
  \begin{align*}
  \gamma_j = 0 & \Leftrightarrow \beta_j = 0 \\
  \gamma_j = 1 &  \Leftrightarrow \beta_j \neq 0 
  \end{align*}
  \item Find Bayes factors and posterior probabilities of models $\Mg$
  \item $2^p$ models!
  
  
  
\end{itemize}


\end{frame}


\begin{frame}\frametitle{Zellner's g-prior}
  Centered model:  $$\Y = \1 \alpha + \X^c \b + \epsilon$$
  where $\X^c$ is the centered design matrix where all variables have
  had their mean subtracted \pause
\begin{itemize}
\item   $p(\alpha, \phi) \propto 1/\phi$ \pause
\item  $\b_\gamma \mid \alpha, \phi, \g \sim \N(0, g \phi^{-1}
  ({\Xg^c}^\prime \Xg^c)^{-1})$ \pause
%\item  take $g=n$
\end{itemize}
which leads to marginal likelihood of $\g$ that is proportional
to $$ p(\Y \mid \g) = C (1 + g)^{\frac{n-p-1}{2}} ( 1 + g (1 -
 R^2_\gamma))^{- \frac{(n-1)}{2}}$$
where $R^2$ is the usual coefficient of determination for model $\Mg$.
\pause

Trade-off of model complexity versus goodness of fit

\bigskip
Lastly, assign distribution to space of models
\end{frame}

\begin{frame}{Sketch}
\begin{itemize}
  \item  Integrate out $\bg$  using sums of normals \pause
  \item  Find inverse of $\I_n + g \P_{\Xg}$  (properties of projections) \pause
  \item Find determinant of $\phi (\I_n + g \P_{\Xg})$  \pause
  \item Integrate out intercept (normal)  \pause
  \item Integrate out $\phi$  (gamma)  \pause
  \item algebra to simplify quadratic forms to  $R^2_{\g}$
\end{itemize}

Or integrate $\alpha$, $\bg$ and $\phi$  (complete the square!)
\end{frame}

\begin{frame}\frametitle{Posteriors}
\begin{align*}\alpha \mid \phi, y & \sim \textsf{N}\left(\bar{y}, \frac{1}{n \phi}\right)\\
\boldsymbol{\beta}_{\gamma} \mid \gamma, \phi, g, y &\sim \textsf{N}\left( \frac{g}{1 + g} \hat{\boldsymbol{\beta}}_{\gamma}, \frac{g}{1 + g} \frac{1}{\phi} \left[{\boldsymbol{X}_{\gamma}}^T \boldsymbol{X}_{\gamma} \right]^{-1}  \right) \\
\phi \mid \gamma, y & \sim \textsf{Gamma}\left(\frac{n-1}{2}, \frac{\textsf{TotalSS} - \frac{g}{1+g} \textsf{RegSS}}{2}\right) \\
p(\gamma \mid y) & \propto p(y \mid \gamma) p(\gamma) \\
\textsf{TotalSS} \equiv \sum_i (y_i - \bar{y})^2 & \qquad
\textsf{RegSS} \equiv \hat{\boldsymbol{\beta}}_\gamma^T \boldsymbol{X}_\gamma^T \boldsymbol{X}_\gamma \hat{\beta}\gamma\\
R^2_\gamma = \frac{\textsf{RegSS}}{\textsf{TotalSS}} & = 1 - \frac{\textsf{ErrorSS}}{\textsf{TotalSS}}
\end{align*}
\end{frame}
\begin{frame}
  \frametitle{Priors on Model Space}
  $p(\Mg) \Leftrightarrow p(\g)$
  \begin{itemize}
  \item $p(\gamma_j = 1) = .5 \Rightarrow P(\Mg) = .5^p$  Uniform on space of models \pause $\pg \sim \Bin(p, .5)$
\item $\gamma_j \mid \pi \simiid \Ber(\pi)$ and $\pi \sim \Be(a,b)$ then  $\pg \sim \BB_p(a, b)$
$$
p(\pg \mid p, a, b) = \frac{ \Gamma(p + 1) \Gamma(\pg + a) \Gamma(p - \pg + b) \Gamma (a + b) }{\Gamma(\pg+1) \Gamma(p - \pg + 1) \Gamma(p + a + b) \Gamma(a) \Gamma(b)}
$$
\item $\pg \sim \BB_p(1, 1) \sim \Un(0, p)$
  \end{itemize}
\end{frame}


\begin{frame} \frametitle{Posterior Probabilities of Models}

\begin{itemize}
  \item  Calculate analytically under enumeration
  $$p(\Mg \mid \Y )= \frac{p(\Y \mid \g) p(\g)} {\sum_{\g^\prime \in \Gamma} p(\Y \mid \g^\prime) p(\g^\prime)}$$
  Express as a function of Bayes factors and prior odds!
  
  \item Use MCMC over $\Gamma$ - Gibbs, Metropolis Hastings if $p$ is large
  
 \item slow convergence/poor mixing with high correlations \pause
  \item Metropolis Hastings algorithms more flexibility \pause
        (swap pairs of variables)

  
  \item Do we need to run MCMC over $\g$, $\bg$, $\alpha$, and $\phi$?
\end{itemize}

\end{frame}



\begin{frame} \frametitle{Choice of $g$: Bartlett's Paradox}


The Bayes factor for comparing $\g$ to the null
model:
$$
 BF(\g : \g0) =    (1 + g)^{(n - 1 - \pg)/2} (1 + g(1 - R_{\g}^2))^{-(n-1)/2}
$$
\pause
\begin{itemize}
\item For fixed sample size $n$ and $R_{\g}^2$, consider taking values of  $g$ that
  go to infinity  \pause
\item Increasing vagueness in prior \pause
\item What happens to BF as $g \to \infty$? \pause
\item why is this a paradox?

\end{itemize}
\end{frame}



\begin{frame}
  \frametitle{Information Paradox}

The Bayes factor for comparing $\g$ to the null
model:
$$
 BF(\g : \g_0) =    (1 + g)^{(n - 1 - \pg)/2} (1 + g(1 - R_{\g}^2))^{-(n-1)/2}
$$
\pause
\begin{itemize}
\item Let $g$ be a fixed constant and take $n$ fixed. \pause
\item Let $F = \frac{R_{\g}^2/\pg}{(1 - R_{\g}^2)/(n - 1 - \pg)}$ \pause
\item As $R^2_{\g} \to 1$, $F \to \infty$ LR test would reject $\g_0$
  where $F$ is the usual $F$ statistic for  comparing model $\g$ to
  $\g_0$ \pause
\item BF converges to a fixed constant $(1+g)^{n - 1 -\pg/2}$  (does not go
  to infinity 
\end{itemize}

``Information Inconsistency''  see Liang et al JASA 2008


\end{frame}


\begin{frame}
  \frametitle{Mixtures of $g$ priors \& Information consistency}


\begin{itemize}
\item Need $BF \to \infty$ if $\R_{\g}^2 \to 1$
\item Put a prior on $g$
$$BF(\g : \g_0) =  \frac{ C \int (1 + g)^{(n - 1 - \pg)/2} (1 + g(1 - R_{\g}^2))^{-(n-1)/2} \pi(g) dg}{C}$$
\item interchange limit and integration as $R^2 \to 1$
want
$$ \E_g[(1 +
g)^{(n-1-\pg)/2}]$$  to diverge

\item hyper-g prior (Liang et al JASA 2008)
$$p(g) = \frac{a-2}{2}(1 + g)^{-a/2}$$ or $g/(1+g) \sim Beta(1, (a-2)/2)$

\item prior expectation converges if $a > n + 1 - \pg$

\item Consider minimal model $\pg = 1$ and $n = 3$ (can estimate intercept, one coefficient, and  $\sigma^2$, then $a > 3$ integral exists

\item For $2 < a \le 3$ integral diverges and resolves the information paradox!
\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Mixtures of $g$ priors \& Information consistency}

Need $BF \to \infty$ if $\R^2 \to 1$  $\Leftrightarrow$ $\E_g[(1 +
g)^{(n-1-\pg)/2}]$ diverges  (proof in Liang et al)
\pause
\begin{itemize}

\item hyper-g prior (Liang et al JASA 2008)
$$p(g) = \frac{a-2}{2}(1 + g)^{-a/2}$$ or $g/(1+g) \sim Beta(1, (a-2)/2)$
need $2 < a \le 3$
\pause
\item Jeffreys prior on $g$ corresponds to $a = 2$ (improper) \pause
\item Hyper-g/n  $(g/n)(1 + g/n) \sim (Beta(1, (a-2)/2)$ \pause
\item Zellner-Siow Cauchy prior $1/g \sim G(1/2, n/2)$ \pause
\item robust prior (Bayarri et al Annals of Statistics 2012 \pause
\item Intrinsic prior (Womack et al  JASA 2015)
\end{itemize}

 All have prior tails for $\b$  that behave like a Cauchy distribution
 and (the latter 4) marginal  likelihoods that can be computed using special hypergeometric
 functions   ($_2F_1$, Appell $F_1$)
\end{frame}


\begin{frame}[fragile]
\frametitle{USair Data}
<<>>=
library(BAS)
data(usair, package="HH")
poll.bma = bas.lm(log(SO2) ~ temp + log(mfgfirms) +
                             log(popn) + wind +
                             precip + raindays,
                  data=usair,
                  prior="JZS",  #Jeffrey-Zellner-Siow
                  alpha=nrow(usair), # n
                  n.models=2^6,
                  modelprior = uniform(),
                  method="deterministic")
@

% Marginal Posterior Inclusion Probabilities:
% Intercept  temp   log(mfgfirms)  log(popn)   wind  precip  raindays
%   1.0000   0.9755       0.7190     0.2757  0.7654  0.5994   0.3104

\end{frame}


\begin{frame}[fragile]\frametitle{Summary}

\begin{small}
<<summary>>=
summary(poll.bma)
@
\end{small}

\end{frame}


\begin{frame}[fragile]\frametitle{Plots}

\centering
<<coef_plot,out.width='75%',out.height='75%',fig.height=5,fig.width=8, echo=TRUE, fig = TRUE>>=
 beta = coef(poll.bma)
 par(mfrow=c(2,3));  plot(beta, subset=2:7,ask=F)
@
\end{frame}


\begin{frame}[fragile]\frametitle{Posterior Distribution  with Uniform Prior on Model Space}


<<out.width='4.5in',out.height='3in', fig.height=6,fig.width=8,fig=TRUE>>=
image(poll.bma, rotate=FALSE)
@


\end{frame}



\begin{frame}[fragile]\frametitle{Posterior Distribution  with BB(1,1) Prior on Model Space}


<<>>=
poll.bb.bma = bas.lm(log(SO2) ~ temp + log(mfgfirms) +
                                log(popn) + wind +
                                precip + raindays,
                     data=usair,
                     prior="JZS",
                     alpha=nrow(usair),
                     n.models=2^6,  #enumerate
                     modelprior=beta.binomial(1,1))
@

\end{frame}

\begin{frame}[fragile]\frametitle{BB(1,1) Prior on Model Space}


<<out.width='4.5in',out.height='3in', fig.height=6,fig.width=8, fig=TRUE>>=
image(poll.bb.bma, rotate=FALSE)
@
\end{frame}


\begin{frame}\frametitle{Summary}

\begin{itemize}
  \item Choice of prior on $\bg$ 
  \item g-priors or mixtures of $g$ (sensitivity)
  \item priors on the models (sensitivity)
  \item posterior summaries - select a model or "average" over all models
\end{itemize}


\end{frame}



\begin{frame}[fragile] \frametitle{Diabetes Example from Hoff $p=64$ }
<<>>=
set.seed(8675309)
source("yX.diabetes.train.txt")
diabetes.train = as.data.frame(diabetes.train)

source("yX.diabetes.test.txt")
diabetes.test = as.data.frame(diabetes.test)
colnames(diabetes.test)[1] = "y"

str(diabetes.train)

@




\end{frame}



\begin{frame}[fragile]\frametitle{MCMC with BAS}
<<MCMC, cache=TRUE>>=
library(BAS)
diabetes.bas = bas.lm(y ~ ., data=diabetes.train,
                      prior = "JZS",
                      method="MCMC",
                      n.models = 10000,
                      MCMC.iterations=150000,
                      thin = 10,
                      initprobs="eplogp",
                      force.heredity=FALSE)

system.time(bas.lm(y ~ ., data=diabetes.train, 
                   prior = "JZS",
                   method="MCMC", n.models = 10000,
                   MCMC.iterations=150000,
                   thin = 10,  initprobs="eplogp",
                   force.heredity=FALSE))

@

Time is in seconds

\end{frame}



\begin{frame}[fragile] \frametitle{Diagnostics}

<<diagnostics, fig.height=5, fig.width=5, out.width='.4\\linewidth', fig=TRUE>>=
diagnostics(diabetes.bas, type="pip")
@

\end{frame}


\begin{frame}[fragile]\frametitle{Prediction}

<<predictions,cache=TRUE>>=
pred.bas = predict(diabetes.bas,
                   newdata=diabetes.test,
                   estimator="BMA",
                   se=TRUE)
mean((pred.bas$fit- diabetes.test$y)^2)

ci.bas = confint(pred.bas);
coverage = mean(diabetes.test$y > ci.bas[,1] & diabetes.test$y < ci.bas[,2])
coverage
@

\end{frame}


\begin{frame}[fragile]\frametitle{95\% prediction intervals}

\begin{small}
<<fig.height=3, fig.width=5, out.width=3in, fig=TRUE>>=
plot(ci.bas); points(diabetes.test$y, col=2, pch=15)
@
\end{small}





\end{frame}

\begin{frame}\frametitle{Selection and Prediction}

\begin{itemize}
  \item  BMA  - optimal for squared error loss Bayes
  \item  HPM: Highest Posterior Probability model (not optimal for prediction) but for selection

\item MPM: Median Probabilty model (select model where PIP > 0.5)
 (optimal under certain conditions; nested models)

\item BPM: Best Probability Model - Model closest to BMA under loss
      (usually includes more predictors than HPM or MPM)
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{Selection}
<<>>=
pred.bas = predict(diabetes.bas,
                   newdata=diabetes.test,
                   estimator="BPM",
                   se=TRUE)
#MSE
mean((pred.bas$fit- diabetes.test$y)^2)
#Coverage
ci.bas = confint(pred.bas)
mean(diabetes.test$y > ci.bas[,1] &
     diabetes.test$y < ci.bas[,2])
@

\end{frame}


\begin{frame}\frametitle{Alternatives to MCMC}


\begin{itemize}
\item "Stochastic Search" (no guarantee samples represent posterior) \pause
\item Variational, EM, etc to find modal model \pause
\item in BMA all variables are included, but coefficients are
   shrunk to 0; alternative is to use shrinkage methods without point mass at zero \pause
  \item
  \item If $p > n$, can use a generalized inverse, but requires care for prior on $\g$!
\end{itemize}

\bigskip Model averaging versus Model Selection  -- what are objectives?


\end{frame}


\begin{frame}
  \frametitle{Effect Estimation}
  \begin{itemize}
  \item  Coefficients in each model are adjusted for other variables
    in the model
\item OLS:  leave out a predictor with a non-zero coefficient then
  estimates are biased!
\item Model Selection in the presence of high correlation, may leave
  out "redundant" variables;
\item improved MSE for prediction (Bias-variance tradeoff)

\item in BMA all variables are included, but coefficients are
   shrunk to 0
\item Care needed for "causal" questions   and confounder adjustment!    With confounding, should not use plain BMA.  Need to change prior to
  include potential confounders  (advanced topic)

  \end{itemize}


\end{frame}




\end{document}
