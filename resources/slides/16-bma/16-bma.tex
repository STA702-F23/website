\documentclass[]{beamer}
%\documentclass[handout]{beamer}
% ***************************************************************
% for handout, change only this...
%   \documentclass[twocolumn]{article}
%   \usepackage{beamerarticle}
%   \setlength{\textwidth}{7.5in}
%   \setlength{\textheight}{9.8in}
%   \setlength{\topmargin}{-1in}
%   \setlength{\oddsidemargin}{-.52in}
%   \setlength{\evensidemargin}{-.52in}

%\usepackage{beamerprosper}
%\usetheme{Warsaw}
%\usecolortheme{orchid}

\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{macros}
\title{Bayesian Variable Selection \&  Bayesian Model Averaging}
 
\author{Hoff Chapter 9, Liang et al 2008, Hoeting et al (1999), Clyde \&
 George (2004)}
\date{\today}
%\Logo(-1.9,7.3){\includegraphics[width=.5in]{../eps/duke}}
% Optional: text to put in the bottom of each slide.
% By default, the title of the talk will be placed there.
%\slideCaption{\textit{October 28, 2005 }}

\usepackage{Sweave}
\begin{document}
\input{16-bma-concordance}

%\SweaveOpts{prefix.string=tmpout/t, split=TRUE, ae=FALSE, height=5,width=6}
%\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}
% make the title slide
\maketitle







\begin{frame}
  \frametitle{Zellner's $g$-prior}
Zellner's g-prior(s) $\b \mid \phi \sim \N(\bv_0, g
    (\X^T\X)^{-1}/\phi)$ \pause

$$\b \mid \Y, \phi \sim \N\left( \frac{g}{1 + g} \bhat +  \frac{1}{1 + g}
\bv_0, \frac{g}{1 + g} (\X^T\X)^{-1} \phi^{-1} \right)$$ \pause

\begin{itemize}
\item Invariance: Require posterior of   $\X \b$  equal the posterior of $\X \H \alphav$
\pause   ($\a_0 = \H^{-1} \bv_0$)  (  $\bv_0 = \zero$)

\item Choice of $g$?  \pause
\item $\frac{g}{1 + g}$  weight given to the data \pause
\item Fixed $g$ effect does not vanish as $n \to \infty$
\item Use $g = n$ or place a prior diistribution on $g$
\end{itemize}


\end{frame}
\begin{frame}
  \frametitle{Shrinkage}
 Posterior mean under  $g$-prior  with $\bv_0 = 0$
$\frac{g}{1 +g} \bhat $

\centerline{\includegraphics[height=3in]{shrinkage}}
\end{frame}


\begin{frame}\frametitle{Ridge Regression}

\begin{itemize}

\item If $\X^T\X$ is nearly singular, certain  elements of $\b$ or (linear combinations of $\b$) may have huge variances under the $g$-prior (or flat prior) as the MLEs are highly unstable!


\item {\bf Ridge regression} protects against the explosion of variances and ill-conditioning with the conjugate prior:

$$\b \mid \phi \sim \textsf{N}(0, \frac{1}{\phi \lambda} \I_p)$$

\item Posterior for $\b$  (conjugate case)

$$\b \mid \phi, \lambda, \Y \sim \N \left((\lambda \I_p + \X^T\X)^{-1} \X^T \Y,  \frac{1}{\phi}(\lambda \I_p + \X^T\X)^{-1}\right)$$

\item induces shrinkage as well!
\end{itemize}
\end{frame}

\begin{frame} \frametitle{Model Choice ?}
\begin{itemize}
  \item Redundant variables lead to unstable estimates \pause
  \item Some variables may not be relevant ($\beta_j = 0$) \pause
  \item Can we infer a "good" model from the data?
  \item Expand model hierarchically to introduce another latent variable $\g$ that encodes models $\Mg$
  $\g = (\gamma_1, \gamma_2, \ldots \gamma_p)^T$ where
  \begin{align*}
  \gamma_j = 0 & \Leftrightarrow \beta_j = 0 \\
  \gamma_j = 1 &  \Leftrightarrow \beta_j \neq 0 
  \end{align*}
  \item Find Bayes factors and posterior probabilities of models $\Mg$
  \item $2^p$ models!
  
  
  
\end{itemize}


\end{frame}


\begin{frame}\frametitle{Zellner's g-prior}
  Centered model:  $$\Y = \1 \alpha + \X^c \b + \epsilon$$
  where $\X^c$ is the centered design matrix where all variables have
  had their mean subtracted \pause
\begin{itemize}
\item   $p(\alpha, \phi) \propto 1/\phi$ \pause
\item  $\b_\gamma \mid \alpha, \phi, \g \sim \N(0, g \phi^{-1}
  ({\Xg^c}^\prime \Xg^c)^{-1})$ \pause
%\item  take $g=n$
\end{itemize}
which leads to marginal likelihood of $\g$ that is proportional
to $$ p(\Y \mid \g) = C (1 + g)^{\frac{n-p-1}{2}} ( 1 + g (1 -
 R^2_\gamma))^{- \frac{(n-1)}{2}}$$
where $R^2$ is the usual coefficient of determination for model $\Mg$.
\pause

Trade-off of model complexity versus goodness of fit

\bigskip
Lastly, assign distribution to space of models
\end{frame}

\begin{frame}{Sketch}
\begin{itemize}
  \item  Integrate out $\bg$  using sums of normals \pause
  \item  Find inverse of $\I_n + g \P_{\Xg}$  (properties of projections) \pause
  \item Find determinant of $\phi (\I_n + g \P_{\Xg})$  \pause
  \item Integrate out intercept (normal)  \pause
  \item Integrate out $\phi$  (gamma)  \pause
  \item algebra to simplify quadratic forms to  $R^2_{\g}$
\end{itemize}

Or integrate $\alpha$, $\bg$ and $\phi$  (complete the square!)
\end{frame}

\begin{frame}\frametitle{Posteriors}
\begin{align*}\alpha \mid \phi, y & \sim \textsf{N}\left(\bar{y}, \frac{1}{n \phi}\right)\\
\boldsymbol{\beta}_{\gamma} \mid \gamma, \phi, g, y &\sim \textsf{N}\left( \frac{g}{1 + g} \hat{\boldsymbol{\beta}}_{\gamma}, \frac{g}{1 + g} \frac{1}{\phi} \left[{\boldsymbol{X}_{\gamma}}^T \boldsymbol{X}_{\gamma} \right]^{-1}  \right) \\
\phi \mid \gamma, y & \sim \textsf{Gamma}\left(\frac{n-1}{2}, \frac{\textsf{TotalSS} - \frac{g}{1+g} \textsf{RegSS}}{2}\right) \\
p(\gamma \mid y) & \propto p(y \mid \gamma) p(\gamma) \\
\textsf{TotalSS} \equiv \sum_i (y_i - \bar{y})^2 & \qquad
\textsf{RegSS} \equiv \hat{\boldsymbol{\beta}}_\gamma^T \boldsymbol{X}_\gamma^T \boldsymbol{X}_\gamma \hat{\beta}\gamma\\
R^2_\gamma = \frac{\textsf{RegSS}}{\textsf{TotalSS}} & = 1 - \frac{\textsf{ErrorSS}}{\textsf{TotalSS}}
\end{align*}
\end{frame}
\begin{frame}
  \frametitle{Priors on Model Space}
  $p(\Mg) \Leftrightarrow p(\g)$
  \begin{itemize}
  \item $p(\gamma_j = 1) = .5 \Rightarrow P(\Mg) = .5^p$  Uniform on space of models \pause $\pg \sim \Bin(p, .5)$
\item $\gamma_j \mid \pi \simiid \Ber(\pi)$ and $\pi \sim \Be(a,b)$ then  $\pg \sim \BB_p(a, b)$
$$
p(\pg \mid p, a, b) = \frac{ \Gamma(p + 1) \Gamma(\pg + a) \Gamma(p - \pg + b) \Gamma (a + b) }{\Gamma(\pg+1) \Gamma(p - \pg + 1) \Gamma(p + a + b) \Gamma(a) \Gamma(b)}
$$
\item $\pg \sim \BB_p(1, 1) \sim \Un(0, p)$
  \end{itemize}
\end{frame}


\begin{frame} \frametitle{Posterior Probabilities of Models}

\begin{itemize}
  \item  Calculate analytically under enumeration
  $$p(\Mg \mid \Y )= \frac{p(\Y \mid \g) p(\g)} {\sum_{\g^\prime \in \Gamma} p(\Y \mid \g^\prime) p(\g^\prime)}$$
  Express as a function of Bayes factors and prior odds!
  
  \item Use MCMC over $\Gamma$ - Gibbs, Metropolis Hastings if $p$ is large
  
 \item slow convergence/poor mixing with high correlations \pause
  \item Metropolis Hastings algorithms more flexibility \pause
        (swap pairs of variables)

  
  \item Do we need to run MCMC over $\g$, $\bg$, $\alpha$, and $\phi$?
\end{itemize}

\end{frame}



\begin{frame} \frametitle{Choice of $g$: Bartlett's Paradox}


The Bayes factor for comparing $\g$ to the null
model:
$$
 BF(\g : \g0) =    (1 + g)^{(n - 1 - \pg)/2} (1 + g(1 - R_{\g}^2))^{-(n-1)/2}
$$
\pause
\begin{itemize}
\item For fixed sample size $n$ and $R_{\g}^2$, consider taking values of  $g$ that
  go to infinity  \pause
\item Increasing vagueness in prior \pause
\item What happens to BF as $g \to \infty$? \pause
\item why is this a paradox?

\end{itemize}
\end{frame}



\begin{frame}
  \frametitle{Information Paradox}

The Bayes factor for comparing $\g$ to the null
model:
$$
 BF(\g : \g_0) =    (1 + g)^{(n - 1 - \pg)/2} (1 + g(1 - R_{\g}^2))^{-(n-1)/2}
$$
\pause
\begin{itemize}
\item Let $g$ be a fixed constant and take $n$ fixed. \pause
\item Let $F = \frac{R_{\g}^2/\pg}{(1 - R_{\g}^2)/(n - 1 - \pg)}$ \pause
\item As $R^2_{\g} \to 1$, $F \to \infty$ LR test would reject $\g_0$
  where $F$ is the usual $F$ statistic for  comparing model $\g$ to
  $\g_0$ \pause
\item BF converges to a fixed constant $(1+g)^{n - 1 -\pg/2}$  (does not go
  to infinity 
\end{itemize}

``Information Inconsistency''  see Liang et al JASA 2008


\end{frame}


\begin{frame}
  \frametitle{Mixtures of $g$ priors \& Information consistency}


\begin{itemize}
\item Need $BF \to \infty$ if $\R_{\g}^2 \to 1$
\item Put a prior on $g$
$$BF(\g : \g_0) =  \frac{ C \int (1 + g)^{(n - 1 - \pg)/2} (1 + g(1 - R_{\g}^2))^{-(n-1)/2} \pi(g) dg}{C}$$
\item interchange limit and integration as $R^2 \to 1$
want
$$ \E_g[(1 +
g)^{(n-1-\pg)/2}]$$  to diverge

\item hyper-g prior (Liang et al JASA 2008)
$$p(g) = \frac{a-2}{2}(1 + g)^{-a/2}$$ or $g/(1+g) \sim Beta(1, (a-2)/2)$

\item prior expectation converges if $a > n + 1 - \pg$

\item Consider minimal model $\pg = 1$ and $n = 3$ (can estimate intercept, one coefficient, and  $\sigma^2$, then $a > 3$ integral exists

\item For $2 < a \le 3$ integral diverges and resolves the information paradox!
\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Mixtures of $g$ priors \& Information consistency}

Need $BF \to \infty$ if $\R^2 \to 1$  $\Leftrightarrow$ $\E_g[(1 +
g)^{(n-1-\pg)/2}]$ diverges  (proof in Liang et al)
\pause
\begin{itemize}

\item hyper-g prior (Liang et al JASA 2008)
$$p(g) = \frac{a-2}{2}(1 + g)^{-a/2}$$ or $g/(1+g) \sim Beta(1, (a-2)/2)$
need $2 < a \le 3$
\pause
\item Jeffreys prior on $g$ corresponds to $a = 2$ (improper) \pause
\item Hyper-g/n  $(g/n)(1 + g/n) \sim (Beta(1, (a-2)/2)$ \pause
\item Zellner-Siow Cauchy prior $1/g \sim G(1/2, n/2)$ \pause
\item robust prior (Bayarri et al Annals of Statistics 2012 \pause
\item Intrinsic prior (Womack et al  JASA 2015)
\end{itemize}

 All have prior tails for $\b$  that behave like a Cauchy distribution
 and (the latter 4) marginal  likelihoods that can be computed using special hypergeometric
 functions   ($_2F_1$, Appell $F_1$)
\end{frame}


\begin{frame}[fragile]
\frametitle{USair Data}
\begin{Schunk}
\begin{Sinput}
> library(BAS)
> data(usair, package="HH")
> poll.bma = bas.lm(log(SO2) ~ temp + log(mfgfirms) +
+                              log(popn) + wind +
+                              precip + raindays,
+                   data=usair,
+                   prior="JZS",  #Jeffrey-Zellner-Siow
+                   alpha=nrow(usair), # n
+                   n.models=2^6,
+                   modelprior = uniform(),
+                   method="deterministic")
\end{Sinput}
\end{Schunk}

% Marginal Posterior Inclusion Probabilities:
% Intercept  temp   log(mfgfirms)  log(popn)   wind  precip  raindays
%   1.0000   0.9755       0.7190     0.2757  0.7654  0.5994   0.3104

\end{frame}


\begin{frame}[fragile]\frametitle{Summary}

\begin{small}
\begin{Schunk}
\begin{Sinput}
> summary(poll.bma)
\end{Sinput}
\begin{Soutput}
              P(B != 0 | Y) model 1   model 2   model 3   model 4   model 5
Intercept        1.00000000 1.00000 1.0000000 1.0000000 1.0000000 1.0000000
temp             0.91158530 1.00000 1.0000000 1.0000000 1.0000000 1.0000000
log(mfgfirms)    0.31718916 0.00000 0.0000000 0.0000000 1.0000000 1.0000000
log(popn)        0.09223957 0.00000 0.0000000 0.0000000 0.0000000 0.0000000
wind             0.29394451 0.00000 0.0000000 0.0000000 1.0000000 0.0000000
precip           0.28384942 0.00000 1.0000000 0.0000000 1.0000000 0.0000000
raindays         0.22903262 0.00000 0.0000000 1.0000000 0.0000000 0.0000000
BF                       NA 1.00000 0.3286643 0.2697945 0.2655873 0.2176573
PostProbs                NA 0.29410 0.0967000 0.0794000 0.0781000 0.0640000
R2                       NA 0.29860 0.3775000 0.3714000 0.5427000 0.3645000
dim                      NA 2.00000 3.0000000 3.0000000 5.0000000 3.0000000
logmarg                  NA 3.14406 2.0313422 1.8339656 1.8182487 1.6192271
\end{Soutput}
\end{Schunk}
\end{small}

\end{frame}


\begin{frame}[fragile]\frametitle{Plots}

\centering
\begin{Schunk}
\begin{Sinput}
>  beta = coef(poll.bma)
>  par(mfrow=c(2,3));  plot(beta, subset=2:7,ask=F)
\end{Sinput}
\end{Schunk}
\includegraphics{16-bma-coef_plot}
\end{frame}


\begin{frame}[fragile]\frametitle{Posterior Distribution  with Uniform Prior on Model Space}


\begin{Schunk}
\begin{Sinput}
> image(poll.bma, rotate=FALSE)
\end{Sinput}
\end{Schunk}
\includegraphics{16-bma-004}


\end{frame}



\begin{frame}[fragile]\frametitle{Posterior Distribution  with BB(1,1) Prior on Model Space}


\begin{Schunk}
\begin{Sinput}
> poll.bb.bma = bas.lm(log(SO2) ~ temp + log(mfgfirms) +
+                                 log(popn) + wind +
+                                 precip + raindays,
+                      data=usair,
+                      prior="JZS",
+                      alpha=nrow(usair),
+                      n.models=2^6,  #enumerate
+                      modelprior=beta.binomial(1,1))
\end{Sinput}
\end{Schunk}

\end{frame}

\begin{frame}[fragile]\frametitle{BB(1,1) Prior on Model Space}


\begin{Schunk}
\begin{Sinput}
> image(poll.bb.bma, rotate=FALSE)
\end{Sinput}
\end{Schunk}
\includegraphics{16-bma-006}
\end{frame}


\begin{frame}\frametitle{Summary}

\begin{itemize}
  \item Choice of prior on $\bg$ 
  \item g-priors or mixtures of $g$ (sensitivity)
  \item priors on the models (sensitivity)
  \item posterior summaries - select a model or "average" over all models
\end{itemize}


\end{frame}



\begin{frame}[fragile] \frametitle{Diabetes Example from Hoff $p=64$ }
\begin{Schunk}
\begin{Sinput}
> set.seed(8675309)
> source("yX.diabetes.train.txt")
> diabetes.train = as.data.frame(diabetes.train)
> source("yX.diabetes.test.txt")
> diabetes.test = as.data.frame(diabetes.test)
> colnames(diabetes.test)[1] = "y"
> str(diabetes.train)
\end{Sinput}
\begin{Soutput}
'data.frame':	342 obs. of  65 variables:
 $ y      : num  -0.0147 -1.0005 -0.1444 0.6987 -0.2222 ...
 $ age    : num  0.7996 -0.0395 1.7913 -1.8703 0.113 ...
 $ sex    : num  1.064 -0.937 1.064 -0.937 -0.937 ...
 $ bmi    : num  1.296 -1.081 0.933 -0.243 -0.764 ...
 $ map    : num  0.459 -0.553 -0.119 -0.77 0.459 ...
 $ tc     : num  -0.9287 -0.1774 -0.9576 0.256 0.0826 ...
 $ ldl    : num  -0.731 -0.402 -0.718 0.525 0.328 ...
 $ hdl    : num  -0.911 1.563 -0.679 -0.757 0.171 ...
 $ tch    : num  -0.0544 -0.8294 -0.0544 0.7205 -0.0544 ...
 $ ltg    : num  0.4181 -1.4349 0.0601 0.4765 -0.6718 ...
 $ glu    : num  -0.371 -1.936 -0.545 -0.197 -0.979 ...
 $ age^2  : num  -0.312 -0.867 1.925 2.176 -0.857 ...
 $ bmi^2  : num  0.4726 0.1185 -0.0877 -0.6514 -0.2873 ...
 $ map^2  : num  -0.652 -0.573 -0.815 -0.336 -0.652 ...
 $ tc^2   : num  -0.091 -0.6497 -0.0543 -0.6268 -0.6663 ...
 $ ldl^2  : num  -0.289 -0.521 -0.3 -0.45 -0.555 ...
 $ hdl^2  : num  -0.0973 0.8408 -0.3121 -0.2474 -0.5639 ...
 $ tch^2  : num  -0.639 -0.199 -0.639 -0.308 -0.639 ...
 $ ltg^2  : num  -0.605 0.78 -0.731 -0.567 -0.402 ...
 $ glu^2  : num  -0.578 1.8485 -0.4711 -0.6443 -0.0258 ...
 $ age:sex: num  0.69 -0.139 1.765 1.609 -0.284 ...
 $ age:bmi: num  0.852 -0.142 1.489 0.271 -0.271 ...
 $ age:map: num  0.0349 -0.3346 -0.5862 1.1821 -0.3025 ...
 $ age:tc : num  -0.978 -0.246 -1.927 -0.72 -0.244 ...
 $ age:ldl: num  -0.803 -0.203 -1.504 -1.2 -0.182 ...
 $ age:hdl: num  -0.7247 0.0147 -1.2661 1.6523 0.1046 ...
 $ age:tch: num  -0.254 -0.176 -0.31 -1.598 -0.216 ...
 $ age:ltg: num  0.0644 -0.2142 -0.163 -1.1657 -0.3474 ...
 $ age:glu: num  -0.636 -0.239 -1.359 0.071 -0.438 ...
 $ sex:bmi: num  1.304 0.935 0.915 0.142 0.635 ...
 $ sex:map: num  0.258 0.289 -0.381 0.5 -0.697 ...
 $ sex:tc : num  -1.02 0.131 -1.051 -0.274 -0.112 ...
 $ sex:ldl: num  -0.927 0.236 -0.913 -0.638 -0.452 ...
 $ sex:hdl: num  -0.647 -1.188 -0.377 1.189 0.238 ...
 $ sex:tch: num  -0.411 0.47 -0.411 -1.062 -0.296 ...
 $ sex:ltg: num  0.2988 1.2093 -0.0866 -0.6032 0.4857 ...
 $ sex:glu: num  -0.6171 1.6477 -0.8069 -0.0239 0.7283 ...
 $ bmi:map: num  0.189 0.191 -0.477 -0.195 -0.702 ...
 $ bmi:tc : num  -1.5061 -0.0595 -1.1853 -0.3231 -0.3239 ...
 $ bmi:ldl: num  -1.267 0.183 -0.976 -0.407 -0.536 ...
 $ bmi:hdl: num  -0.869 -1.41 -0.286 0.586 0.251 ...
 $ bmi:tch: num  -0.505 0.505 -0.484 -0.614 -0.388 ...
 $ bmi:ltg: num  0.1014 1.1613 -0.4085 -0.5893 0.0716 ...
 $ bmi:glu: num  -0.862 1.693 -0.89 -0.337 0.358 ...
 $ map:tc : num  -0.687 -0.148 -0.131 -0.451 -0.21 ...
 $ map:ldl: num  -0.5407 0.0388 -0.1034 -0.6114 -0.036 ...
 $ map:hdl: num  -0.235 -0.672 0.254 0.745 0.252 ...
 $ map:tch: num  -0.29 0.207 -0.258 -0.835 -0.29 ...
 $ map:ltg: num  -0.214 0.428 -0.427 -0.811 -0.748 ...
 $ map:glu: num  -0.541 0.659 -0.314 -0.23 -0.812 ...
 $ tc:ldl : num  -0.144 -0.551 -0.139 -0.509 -0.581 ...
 $ tc:hdl : num  0.8363 -0.3457 0.6304 -0.2579 -0.0392 ...
 $ tc:tch : num  -0.405 -0.326 -0.404 -0.295 -0.451 ...
 $ tc:ltg : num  -0.901 -0.259 -0.571 -0.392 -0.569 ...
 $ tc:glu : num  0.0202 0.0196 0.2073 -0.396 -0.4283 ...
 $ ldl:hdl: num  0.889 -0.446 0.705 -0.207 0.26 ...
 $ ldl:tch: num  -0.463 -0.243 -0.463 -0.21 -0.506 ...
 $ ldl:ltg: num  -0.6536 0.2724 -0.3783 -0.0708 -0.5638 ...
 $ ldl:glu: num  -0.0194 0.4995 0.1032 -0.4013 -0.6234 ...
 $ hdl:tch: num  0.703 -0.5 0.692 0.171 0.651 ...
 $ hdl:ltg: num  0.0179 -1.9846 0.3839 0.0399 0.3043 ...
 $ hdl:glu: num  0.654 -2.948 0.689 0.452 0.113 ...
 $ tch:ltg: num  -0.592 0.531 -0.574 -0.253 -0.537 ...
 $ tch:glu: num  -0.371 1.114 -0.362 -0.522 -0.34 ...
 $ ltg:glu: num  -0.584 2.184 -0.468 -0.526 0.183 ...
\end{Soutput}
\begin{Sinput}
> 
\end{Sinput}
\end{Schunk}




\end{frame}



\begin{frame}[fragile]\frametitle{MCMC with BAS}
\begin{Schunk}
\begin{Sinput}
> library(BAS)
> diabetes.bas = bas.lm(y ~ ., data=diabetes.train,
+                       prior = "JZS",
+                       method="MCMC",
+                       n.models = 10000,
+                       MCMC.iterations=150000,
+                       thin = 10,
+                       initprobs="eplogp",
+                       force.heredity=FALSE)
> system.time(bas.lm(y ~ ., data=diabetes.train, 
+                    prior = "JZS",
+                    method="MCMC", n.models = 10000,
+                    MCMC.iterations=150000,
+                    thin = 10,  initprobs="eplogp",
+                    force.heredity=FALSE))
\end{Sinput}
\begin{Soutput}
   user  system elapsed 
  6.464   0.261   6.729 
\end{Soutput}
\begin{Sinput}
> 
\end{Sinput}
\end{Schunk}

Time is in seconds

\end{frame}



\begin{frame}[fragile] \frametitle{Diagnostics}

\begin{Schunk}
\begin{Sinput}
> diagnostics(diabetes.bas, type="pip")
\end{Sinput}
\end{Schunk}
\includegraphics{16-bma-diagnostics}

\end{frame}


\begin{frame}[fragile]\frametitle{Prediction}

\begin{Schunk}
\begin{Sinput}
> pred.bas = predict(diabetes.bas,
+                    newdata=diabetes.test,
+                    estimator="BMA",
+                    se=TRUE)
> mean((pred.bas$fit- diabetes.test$y)^2)
\end{Sinput}
\begin{Soutput}
[1] 0.4552798
\end{Soutput}
\begin{Sinput}
> ci.bas = confint(pred.bas);
> coverage = mean(diabetes.test$y > ci.bas[,1] & diabetes.test$y < ci.bas[,2])
> coverage
\end{Sinput}
\begin{Soutput}
[1] 1
\end{Soutput}
\end{Schunk}

\end{frame}


\begin{frame}[fragile]\frametitle{95\% prediction intervals}

\begin{small}
\begin{Schunk}
\begin{Sinput}
> plot(ci.bas); points(diabetes.test$y, col=2, pch=15)
\end{Sinput}
\begin{Soutput}
NULL
\end{Soutput}
\end{Schunk}
\includegraphics{16-bma-011}
\end{small}





\end{frame}

\begin{frame}\frametitle{Selection and Prediction}

\begin{itemize}
  \item  BMA  - optimal for squared error loss Bayes
  \item  HPM: Highest Posterior Probability model (not optimal for prediction) but for selection

\item MPM: Median Probabilty model (select model where PIP > 0.5)
 (optimal under certain conditions; nested models)

\item BPM: Best Probability Model - Model closest to BMA under loss
      (usually includes more predictors than HPM or MPM)
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{Selection}
\begin{Schunk}
\begin{Sinput}
> pred.bas = predict(diabetes.bas,
+                    newdata=diabetes.test,
+                    estimator="BPM",
+                    se=TRUE)
> #MSE
> mean((pred.bas$fit- diabetes.test$y)^2)
\end{Sinput}
\begin{Soutput}
[1] 0.4740667
\end{Soutput}
\begin{Sinput}
> #Coverage
> ci.bas = confint(pred.bas)
> mean(diabetes.test$y > ci.bas[,1] &
+      diabetes.test$y < ci.bas[,2])
\end{Sinput}
\begin{Soutput}
[1] 0.98
\end{Soutput}
\end{Schunk}

\end{frame}


\begin{frame}\frametitle{Alternatives to MCMC}


\begin{itemize}
\item "Stochastic Search" (no guarantee samples represent posterior) \pause
\item Variational, EM, etc to find modal model \pause
\item in BMA all variables are included, but coefficients are
   shrunk to 0; alternative is to use shrinkage methods without point mass at zero \pause
  \item
  \item If $p > n$, can use a generalized inverse, but requires care for prior on $\g$!
\end{itemize}

\bigskip Model averaging versus Model Selection  -- what are objectives?


\end{frame}


\begin{frame}
  \frametitle{Effect Estimation}
  \begin{itemize}
  \item  Coefficients in each model are adjusted for other variables
    in the model
\item OLS:  leave out a predictor with a non-zero coefficient then
  estimates are biased!
\item Model Selection in the presence of high correlation, may leave
  out "redundant" variables;
\item improved MSE for prediction (Bias-variance tradeoff)

\item in BMA all variables are included, but coefficients are
   shrunk to 0
\item Care needed for "causal" questions   and confounder adjustment!    With confounding, should not use plain BMA.  Need to change prior to
  include potential confounders  (advanced topic)

  \end{itemize}


\end{frame}




\end{document}
